Directory structure:
└── willccbb-verifiers/
    ├── README.md
    ├── LICENSE
    ├── MANIFEST.in
    ├── pyproject.toml
    ├── .pre-commit-config.yaml
    ├── .readthedocs.yaml
    ├── configs/
    │   ├── endpoints.py
    │   └── zero3.yaml
    ├── docs/
    │   ├── README.md
    │   ├── Makefile
    │   ├── requirements.txt
    │   └── source/
    │       ├── api_reference.md
    │       ├── components.md
    │       ├── conf.py
    │       ├── development.md
    │       ├── environments.md
    │       ├── index.md
    │       ├── overview.md
    │       └── training.md
    ├── environments/
    │   ├── continuation_quality/
    │   │   ├── README.md
    │   │   ├── continuation_quality.py
    │   │   └── pyproject.toml
    │   ├── doublecheck/
    │   │   ├── README.md
    │   │   ├── doublecheck.py
    │   │   └── pyproject.toml
    │   ├── gpqa/
    │   │   ├── README.md
    │   │   ├── gpqa.py
    │   │   └── pyproject.toml
    │   ├── gsm8k/
    │   │   ├── README.md
    │   │   ├── gsm8k.py
    │   │   └── pyproject.toml
    │   ├── math_group/
    │   │   ├── README.md
    │   │   ├── math_group.py
    │   │   └── pyproject.toml
    │   ├── math_python/
    │   │   ├── README.md
    │   │   ├── math_python.py
    │   │   └── pyproject.toml
    │   ├── mmmu/
    │   │   ├── README.md
    │   │   ├── mmmu.py
    │   │   └── pyproject.toml
    │   ├── reasoning_gym_env/
    │   │   ├── README.md
    │   │   ├── pyproject.toml
    │   │   └── reasoning_gym_env.py
    │   ├── reverse_text/
    │   │   ├── README.md
    │   │   ├── pyproject.toml
    │   │   └── reverse_text.py
    │   ├── self_reward/
    │   │   ├── README.md
    │   │   ├── pyproject.toml
    │   │   └── self_reward.py
    │   ├── sentence_repeater/
    │   │   ├── README.md
    │   │   ├── pyproject.toml
    │   │   └── sentence_repeater.py
    │   ├── simpleqa/
    │   │   ├── README.md
    │   │   ├── pyproject.toml
    │   │   └── simpleqa.py
    │   ├── smolagents_math_tools/
    │   │   ├── README.md
    │   │   ├── pyproject.toml
    │   │   └── smolagents_math_tools.py
    │   ├── summarize_text/
    │   │   ├── README.md
    │   │   ├── pyproject.toml
    │   │   └── summarize_text.py
    │   ├── tool_test/
    │   │   ├── README.md
    │   │   ├── pyproject.toml
    │   │   └── tool_test.py
    │   ├── toxicity_explanation/
    │   │   ├── README.md
    │   │   ├── pyproject.toml
    │   │   └── toxicity_explanation.py
    │   ├── wiki_search/
    │   │   ├── README.md
    │   │   ├── pyproject.toml
    │   │   └── wiki_search.py
    │   ├── wordle/
    │   │   ├── README.md
    │   │   ├── pyproject.toml
    │   │   └── wordle.py
    │   ├── xlam_function_calling/
    │   │   ├── README.md
    │   │   ├── pyproject.toml
    │   │   └── xlam_function_calling.py
    │   └── xml_tool_env/
    │       ├── README.md
    │       ├── pyproject.toml
    │       └── xml_tool_env.py
    ├── examples/
    │   ├── sft.py
    │   └── grpo/
    │       ├── train_arc_1d.py
    │       ├── train_continuation_quality.py
    │       ├── train_gsm8k.py
    │       ├── train_math_group.py
    │       ├── train_math_python.py
    │       ├── train_reverse_text.py
    │       ├── train_self_reward.py
    │       ├── train_sentence_repeater.py
    │       ├── train_tool_test.py
    │       ├── train_wiki_search.py
    │       └── train_wordle.py
    ├── tests/
    │   ├── README.md
    │   ├── __init__.py
    │   ├── conftest.py
    │   ├── mock_client_guide.md
    │   ├── mock_openai_client.py
    │   ├── test_env_group.py
    │   ├── test_environment.py
    │   ├── test_environment_extra.py
    │   ├── test_eval_cli.py
    │   ├── test_multiturn_env.py
    │   ├── test_parser.py
    │   ├── test_rubric.py
    │   ├── test_rubric_group.py
    │   ├── test_singleturn_env.py
    │   ├── test_think_parser.py
    │   └── test_xml_parser.py
    ├── verifiers/
    │   ├── __init__.py
    │   ├── types.py
    │   ├── envs/
    │   │   ├── __init__.py
    │   │   ├── env_group.py
    │   │   ├── environment.py
    │   │   ├── multiturn_env.py
    │   │   ├── singleturn_env.py
    │   │   ├── stateful_tool_env.py
    │   │   ├── textarena_env.py
    │   │   └── tool_env.py
    │   ├── inference/
    │   │   ├── __init__.py
    │   │   ├── vllm_client.py
    │   │   └── vllm_server.py
    │   ├── parsers/
    │   │   ├── __init__.py
    │   │   ├── parser.py
    │   │   ├── think_parser.py
    │   │   └── xml_parser.py
    │   ├── rubrics/
    │   │   ├── __init__.py
    │   │   ├── judge_rubric.py
    │   │   ├── math_rubric.py
    │   │   ├── rubric.py
    │   │   ├── rubric_group.py
    │   │   ├── tool_rubric.py
    │   │   └── utils/
    │   │       └── math_utils.py
    │   ├── scripts/
    │   │   ├── __init__.py
    │   │   ├── eval.py
    │   │   ├── init.py
    │   │   ├── install.py
    │   │   └── tui.py
    │   ├── trainers/
    │   │   ├── __init__.py
    │   │   ├── async_batch_generator.py
    │   │   ├── async_dataloader_wrapper.py
    │   │   └── grpo_config.py
    │   └── utils/
    │       ├── __init__.py
    │       ├── async_utils.py
    │       ├── data_utils.py
    │       ├── env_utils.py
    │       ├── logging_utils.py
    │       ├── message_utils.py
    │       ├── model_utils.py
    │       ├── report_utils.py
    │       ├── tool_utils.py
    │       └── tools.py
    └── .github/
        ├── pull_request_template.md
        └── workflows/
            ├── README.md
            ├── badge.md
            ├── style.yaml
            └── test.yml

================================================
FILE: README.md
================================================
<div align="center">

<p align="center">
  <h1>Verifiers</h1>
</p>

<p>
Environments for LLM Reinforcement Learning
</p>

</div>

## Overview

Verifiers is a library of modular components for creating RL environments and training LLM agents. Verifiers includes an async GRPO implementation built around the `transformers` Trainer, is supported by `prime-rl` for large-scale FSDP training, and can easily be integrated into any RL framework which exposes an OpenAI-compatible inference client. In addition to RL training, Verifiers can be used directly for building LLM evaluations, creating synthetic data pipelines, and implementing agent harnesses.

Full documentation is available [here](https://verifiers.readthedocs.io/en/latest/). 

## Setup

We recommend using `verifiers` with along [uv](https://docs.astral.sh/uv/getting-started/installation/) for dependency management in your own project:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
uv init # create a fresh project
source .venv/bin/activate
```

For local (CPU) development and evaluation with API models, do:
```bash
uv add verifiers # uv add 'verifiers[dev]' for Jupyter + testing support
```

For training on GPUs with `vf.GRPOTrainer`, do:
```bash
uv add 'verifiers[all]' && uv pip install flash-attn --no-build-isolation
```

To use the latest `main` branch, do:
```bash
uv add verifiers @ git+https://github.com/willccbb/verifiers.git
```

To use with `prime-rl`, see [here](https://github.com/PrimeIntellect-ai/prime-rl).

To install `verifiers` from source for core library development, do:
```bash
git clone https://github.com/willccbb/verifiers.git
cd verifiers
uv sync --all-extras && uv pip install flash-attn --no-build-isolation
uv run pre-commit install
```

In general, we recommend that you build and train Environments *with* `verifiers`, not *in* `verifiers`. If you find yourself needing to clone and modify the core library in order to implement key functionality for your project, we'd love for you to open an issue so that we can try and streamline the development experience. Our aim is for `verifiers` to be a reliable toolkit to build on top of, and to minimize the "fork proliferation" which often pervades the RL infrastructure ecosystem.

## Environments

Environments in Verifiers are installable Python modules which can specify dependencies in a `pyproject.toml`, and which expose a `load_environment` function for instantiation by downstream applications (e.g. trainers). See `environments/` for examples. 

To initialize a blank Environment module template, do:
```bash
vf-init vf-environment-name # -p /path/to/environments (defaults to "./environments")
```

To an install an Environment module into your project, do:
```bash
vf-install vf-environment-name # -p /path/to/environments (defaults to "./environments") 
```

To install an Environment module from this repo's `environments` folder, do:
```bash
vf-install vf-math-python --from-repo # -b branch_or_commit (defaults to "main")
```

Once an Environment module is installed, you can create an instance of the Environment using `load_environment`, passing any necessary args:
```python
import verifiers as vf
vf_env = vf.load_environment("vf-environment-name", **env_args)
```

To run a quick evaluation of your Environment with an API-based model, do:
```bash
vf-eval vf-environment-name # vf-eval -h for config options; defaults to gpt-4.1-mini, 5 prompts, 3 rollouts for each
```

The core elements of Environments in are:
- Datasets: a Hugging Face `Dataset` with a `prompt` column for inputs, and either `answer (str)` or `info (dict)` columns for evaluation
- Rollout logic: interactions between models and the environment (e.g. `env_response` + `is_completed` for any `MultiTurnEnv`)
- Rubrics: an encapsulation for one or more reward functions
- Parsers: optional; an encapsulation for reusable parsing logic

We support both `/v1/chat/completions`-style and `/v1/completions`-style inference via OpenAI clients, though we generally recommend `/v1/chat/completions`-style inference for the vast majority of applications. Both the included `GRPOTrainer` as well as `prime-rl` support the full set of [SamplingParams](https://docs.vllm.ai/en/v0.6.0/dev/sampling_params.html) exposed by vLLM (via their OpenAI-compatible [server](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html) interface), and leveraging this will often be the appropriate way to implement rollout strategies requiring finer-grained control, such as interrupting and resuming generations for interleaved tool use, or enforcing reasoning budgets.

The primary constraint we impose on rollout logic is that token sequences must be *increasing*, i.e. once a token has been added to a model's context in a rollout, it must remain as the rollout progresses. Note that this causes issues with some popular reasoning models such as the Qwen3 and DeepSeek-R1-Distill series; see [Footguns](#footguns) for guidance on adapting these models to support multi-turn rollouts.  

### SingleTurnEnv

For tasks requiring only a single response from a model for each prompt, you can use `SingleTurnEnv` directly by specifying a Dataset and a Rubric. Rubrics are sets of reward functions, which can be either sync or async.

```python
from datasets import load_dataset
import verifiers as vf

dataset = load_dataset("my-account/my-dataset", split="train")

def reward_A(prompt, completion, info) -> float:
	# reward fn, e.g. correctness
	...

def reward_B(parser, completion) -> float:
	# auxiliary reward fn, e.g. format
	...

async def metric(completion) -> float:
	# non-reward metric, e.g. proper noun count
	...

rubric = vf.Rubric(funcs=[reward_A, reward_B, metric], weights=[1.0, 0.5, 0.0])

vf_env = SingleTurnEnv(
	dataset=dataset,
	rubric=rubric
)
results = vf_env.evaluate(client=OpenAI(), model="gpt-4.1-mini", num_examples=100, rollouts_per_example=1)
vf_env.make_dataset(results) # HF dataset format
```

Datasets should be formatted with columns for:
- `'prompt' (List[ChatMessage])` OR `'question' (str)` fields
	- `ChatMessage` = e.g. `{'role': 'user', 'content': '...'}`
	- if `question` is set instead of `prompt`, you can also pass `system_prompt (str)` and/or `few_shot (List[ChatMessage])`
- `answer (str)` AND/OR `info (dict)`
- `task (str)`: optional, used by `EnvGroup` and `RubricGroup` for orchestrating composition of Environments and Rubrics

The following named attributes available for use by reward functions in your Rubric:
- `prompt`: sequence of input messages
- `completion`: sequence of messages generated during rollout by model and Environment
- `answer`: primary answer column, optional if `info` is used
- `state`: can be modified during rollout to accumulate any metadata (`state['responses']` includes full OpenAI response objects by default)
- `info`: auxiliary info needed for reward computation (e.g. test cases), optional if `answer` is used
- `task`: tag for task type (used by `EnvGroup` and `RubricGroup`)
- `parser`: the parser object declared. Note: `vf.Parser().get_format_reward_func()` is a no-op (always 1.0); use `vf.ThinkParser` or a custom parser if you want a real format adherence reward.

For tasks involving LLM judges, you may wish to use `vf.JudgeRubric()` for managing requests to auxiliary models.

Note on concurrency: environment APIs accept `max_concurrent` to control parallel rollouts. The `vf-eval` CLI currently exposes `--max-concurrent-requests`; ensure this maps to your environment’s concurrency as expected.

`vf-eval` also supports specifying `sampling_args` as a JSON object, which is sent to the vLLM inference engine:

```bash
vf-eval vf-environment-name --sampling-args '{"reasoning_effort": "low"}'
```

Use `vf-eval -s` to save outputs as dataset-formatted JSON, and view all locally-saved eval results with `vf-tui`.

### ToolEnv

For many applications involving tool use, you can use `ToolEnv` to leverage models' native tool/function-calling capabilities in an agentic loop. Tools can be specified as generic Python functions (with type hints and docstrings), which will then be passed in JSON schema form to each inference request.

```python
import verifiers as vf
vf_env = vf.ToolEnv(
	dataset= ... # HF Dataset with 'prompt'/'question' + 'answer'/'info' columns
	rubric= ... # Rubric object; vf.ToolRubric() can be optionally used for counting tool invocations in each rollout
	tools=[search_tool, read_article_tool, python_tool], # python functions with type hints + docstrings
	max_turns=10
)
```

In cases where your tools require heavy computational resources, we recommend hosting your tools as standalone servers (e.g. MCP servers) and creating lightweight wrapper functions to pass to `ToolEnv`. Parallel tool call support is enabled by default. 

For training, or self-hosted endpoints, you'll want to enable auto tool choice in [vLLM](https://docs.vllm.ai/en/stable/features/tool_calling.html#automatic-function-calling) with the appropriate parser. If your model does not support native tool calling, you may find the `XMLParser` abstraction useful for rolling your own tool call parsing on top of `MultiTurnEnv`; see `environments/xml_tool_env` for an example.

### MultiTurnEnv

Both `SingleTurnEnv` and `ToolEnv` are instances of `MultiTurnEnv`, which exposes an interface for writing custom Environment interaction protocols. The two methods you must override are

```python
from typing import Tuple
import verifiers as vf
from verifiers.types import Messages, State
class YourMultiTurnEnv(vf.MultiTurnEnv):
    def __init__(self,
                 dataset: Dataset,
                 rubric: Rubric,
				 max_turns: int,
                 **kwargs):
	
  async def is_completed(self, messages: Messages, state: State, **kwargs) -> bool:
    # return whether or not a rollout is completed

  async def env_response(self, messages: Messages, state: State, **kwargs) -> Tuple[Messages, State]:
    # return new environment message(s) + updated state
```

If your application requires more fine-grained control than is allowed by `MultiTurnEnv`, you may want to inherit from the base `Environment` functionality directly and override the `rollout` method.


## Training


### GRPOTrainer

The included trainer (`vf.GRPOTrainer`) supports running GRPO-style RL training via Accelerate/DeepSpeed, and uses vLLM for inference. It supports both full-parameter finetuning, and is optimized for efficiently training dense transformer models on 2-16 GPUs.

```bash
# install environment
vf-install vf-wordle (-p /path/to/environments | --from-repo)

# quick eval
vf-eval vf-wordle -m (model_name in configs/endpoints.py) -n NUM_EXAMPLES -r ROLLOUTS_PER_EXAMPLE

# inference (shell 0)
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 vf-vllm --model willcb/Qwen3-1.7B-Wordle \
    --data-parallel-size 7 --enforce-eager --disable-log-requests

# training (shell 1)
CUDA_VISIBLE_DEVICES=6,7 accelerate launch --num-processes 2 \
    --config-file configs/zero3.yaml examples/grpo/train_wordle.py --size 1.7B
```

Alternatively, you can train environments with the external `prime-rl` project (FSDP-first orchestration). See the `prime-rl` README for installation and examples. For example:

```toml
# orchestrator config (prime-rl)
[environment]
id = "vf-math-python"  # or your environment ID
```

```bash
# run (prime-rl)
uv run rl \
  --trainer @ configs/your_exp/train.toml \
  --orchestrator @ configs/your_exp/orch.toml \
  --inference @ configs/your_exp/infer.toml
```

### Troubleshooting 
- Ensure your `wandb` and `huggingface-cli` logins are set up (or set `report_to=None` in `training_args`). You should also have something set as your `OPENAI_API_KEY` in your environment (can be a dummy key for vLLM). 
- If using high max concurrency, increase the number of allowed open sockets (e.g. `ulimit -n 4096`)
- On some setups, inter-GPU communication can [hang](https://github.com/huggingface/trl/issues/2923) or crash during vLLM weight syncing. This can usually be alleviated by setting (or unsetting) `NCCL_P2P_DISABLE=1` in your environment (or potentially `NCCL_CUMEM_ENABLE=1`). Try this as your first step if you experience NCCL-related issues.
- If problems persist, please open an [issue](https://github.com/willccbb/verifiers/issues).

### Resource Requirements
`GRPOTrainer` is optimized for setups with at least 2 GPUs, scaling up to multiple nodes. 2-GPU setups with sufficient memory to enable small-scale experimentation can be [rented](https://app.primeintellect.ai/dashboard/create-cluster?image=ubuntu_22_cuda_12) for <$1/hr.

### PRIME-RL
If you do not require LoRA support, you may want to use the `prime-rl` trainer, which natively supports Environments created using `verifiers`, is more optimized for performance and scalability via FSDP, includes a broader set of configuration options and user experience features, and has more battle-tested defaults. Both trainers support asynchronous rollouts, and use a one-step off-policy delay by default for overlapping training and inference. See the `prime-rl` [docs](https://github.com/PrimeIntellect-ai/prime-rl) for usage instructions.

## Further Documentation

See the full [docs](https://verifiers.readthedocs.io/en/latest/) for more information.

## Contributions

Verifiers warmly welcomes community contributions! Please open an issue or PR if you encounter bugs or other pain points during your development, or start a discussion for more open-ended questions.

Please note that the core `verifiers/` library is intended to be a relatively lightweight set of reusable components rather than an exhaustive catalog of RL environments. For *applications* of `verifiers` (e.g. "an Environment for XYZ task"), you are welcome to submit a PR for a self-contained module that lives within `environments/` if it serves as a canonical example of a new pattern. Stay tuned for more info shortly about our plans for supporting community Environment contributions 🙂

## Citation

If you use this code in your research, please cite:

```bibtex
@misc{brown_verifiers_2025,
  author       = {William Brown},
  title        = {{Verifiers}: Reinforcement Learning with LLMs in Verifiable Environments},
  howpublished = {\url{https://github.com/willccbb/verifiers}},
  note         = {Commit abcdefg • accessed DD Mon YYYY},
  year         = {2025}
}
```


## Roadmap
- A community Environments hub for crowdsourcing, sharing, and discovering new RL environments built with `verifiers`
- Default patterns for hosted resources such as code sandboxes, auxiliary models, and MCP servers
- Multimodal input support
- Non-increasing token sequences via REINFORCE



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 William Brown

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================
FILE: MANIFEST.in
================================================
include README.md
include LICENSE
include pyproject.toml
recursive-include verifiers *.py
recursive-include configs *.yaml *.yml *.json
global-exclude *.pyc
global-exclude __pycache__
global-exclude .DS_Store
global-exclude *.egg-info 


================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "verifiers"
authors = [
    {name = "William Brown", email = "williambrown97@gmail.com"},
]
dynamic = ["version"]  # Let hatchling handle version
description = "Verifiers for reinforcement learning with LLMs"
readme = "README.md"
license = "MIT"
requires-python = ">=3.11,<3.13"
keywords = ["reinforcement-learning", "llm", "rl", "grpo", "verifiable-environments", "multi-turn"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Python Modules",
]

dependencies = [
    "openai",
    "datasets",
    "pydantic>=2.11.7",
    "jinja2>=3.1.6",
    "rich",
    "textual",
]

[project.optional-dependencies]
all = [
    "ruff",
    "pre-commit",
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.0.0",
    "requests",
    "torch>=2.7.0",
    "transformers",
    "accelerate>=1.4.0",
    "deepspeed",
    "peft",
    "wandb",
    "trl>=0.17.0",
    "vllm>=0.9.2",
    "liger-kernel>=0.5.10",
    "nest-asyncio>=1.6.0",
    "ipykernel",
    "ipywidgets",
    "math-verify>=0.8.0",
]

dev = [
    "ruff",
    "pre-commit",
    "requests",
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.0.0",
    "nest-asyncio>=1.6.0",
    "ipykernel",
    "ipywidgets",
]

docs = [
    "sphinx",
    "myst-parser",
    "furo"
]

train = [
    "torch>=2.7.0",
    "transformers",
    "accelerate>=1.4.0",
    "requests",
    "peft",
    "wandb",
    "trl>=0.17.0",
    "vllm>=0.9.2",
    "liger-kernel>=0.5.10",
    "deepspeed",
]

envs = [
    "math-verify>=0.8.0",
    "requests",
    "duckduckgo-search",
    "brave-search",
]


[project.scripts]
vf-eval = "verifiers.scripts.eval:main"
vf-init = "verifiers.scripts.init:main"
vf-install = "verifiers.scripts.install:main"
vf-tui = "verifiers.scripts.tui:main"
vf-vllm = "verifiers.inference.vllm_server:main"

# Hatchling configuration
[tool.hatch.version]
path = "verifiers/__init__.py"

[tool.hatch.build.targets.sdist]
include = [
    "/verifiers",
    "/tests",
    "/README.md",
    "/LICENSE",
]
exclude = [
    "/wandb",
    "/scratch",
    "/configs",
    "/environments",
]

[tool.hatch.build.targets.wheel]
packages = ["verifiers"]

[dependency-groups]
dev = [
    "build>=1.2.2.post1",
    "twine>=6.1.0",
]

[project.urls]
Homepage = "https://github.com/willccbb/verifiers"
Documentation = "https://github.com/willccbb/verifiers"
Repository = "https://github.com/willccbb/verifiers.git"
Issues = "https://github.com/willccbb/verifiers/issues"

[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--strict-markers",
    "--tb=short",
    "-ra",
    "--quiet",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "asyncio: marks tests as async tests",
    "parsers: marks tests for parser components",
    "rubrics: marks tests for rubric components",
    "environments: marks tests for environment components",
]
filterwarnings = [
    "ignore::DeprecationWarning",
    "ignore::PendingDeprecationWarning",
    "ignore::UserWarning:transformers.*",
]
asyncio_mode = "auto"
norecursedirs = [".git", ".tox", "dist", "build", "*.egg", "__pycache__"]

[tool.coverage.run]
source = ["verifiers"]
omit = [
    "*/tests/*",
    "*/__pycache__/*",
    "*/site-packages/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
]



================================================
FILE: .pre-commit-config.yaml
================================================
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.12.3
    hooks:
      # Run the linter.
      - id: ruff
        args: [--fix, --config=pyproject.toml]
      # Run the formatter.
      - id: ruff-format
        args: [--config=pyproject.toml]


================================================
FILE: .readthedocs.yaml
================================================
# Read the Docs configuration file
# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details

# Required
version: 2

# Set the OS, Python version, and other tools you might need
build:
  os: ubuntu-24.04
  tools:
    python: "3.12"

sphinx:
   configuration: docs/source/conf.py

python:
  install:
    - method: pip
      path: .
      extra_requirements:
      - docs


================================================
FILE: configs/endpoints.py
================================================
ENDPOINTS = {
    "my-model": {
        "model": "my-model",
        "url": "https://some-endpoint.com/v1",
        "key": "SOME_API_KEY",
    },
    "gpt-4.1-mini": {
        "model": "gpt-4.1-mini",
        "url": "https://api.openai.com/v1",
        "key": "OPENAI_API_KEY",
    },
    "gpt-4.1-nano": {
        "model": "gpt-4.1-nano",
        "url": "https://api.openai.com/v1",
        "key": "OPENAI_API_KEY",
    },
    "gpt-4.1": {
        "model": "gpt-4.1",
        "url": "https://api.openai.com/v1",
        "key": "OPENAI_API_KEY",
    },
    "r1-1.5b": {
        "model": "willcb/DeepSeek-R1-Distill-Qwen-1.5B",
        "url": "http://0.0.0.0:8000/v1",
        "key": "EMPTY",
    },
    "deepseek-chat": {
        "model": "deepseek-chat",
        "url": "https://api.deepseek.com/v1",
        "key": "DEEPSEEK_API_KEY",
    },
    "deepseek-reasoner": {
        "model": "deepseek-reasoner",
        "url": "https://api.deepseek.com/v1",
        "key": "DEEPSEEK_API_KEY",
    },
    "qwen3-a22b-instruct": {
        "model": "Qwen/Qwen3-235B-A22B-Instruct-2507",
        "url": "https://api.deepinfra.com/v1/openai",
        "key": "DEEPINFRA_API_KEY",
    },
    "qwen3-235b-i": {
        "model": "qwen-3-235b-a22b-instruct-2507",
        "url": "https://api.cerebras.ai/v1",
        "key": "CEREBRAS_API_KEY",
    },
    "qwen3-235b-t": {
        "model": "qwen-3-235b-a22b-thinking-2507",
        "url": "https://api.cerebras.ai/v1",
        "key": "CEREBRAS_API_KEY",
    },
    "qwen3-coder": {
        "model": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
        "url": "https://api.deepinfra.com/v1/openai",
        "key": "DEEPINFRA_API_KEY",
    },
    "qwen3-32b": {
        "model": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
        "url": "https://api.deepinfra.com/v1/openai",
        "key": "DEEPINFRA_API_KEY",
    },
    "glm-4.5": {
        "model": "zai-org/GLM-4.5",
        "url": "https://api.deepinfra.com/v1/openai",
        "key": "DEEPINFRA_API_KEY",
    },
    "kimi-k2": {
        "model": "moonshotai/Kimi-K2-Instruct",
        "url": "https://api.deepinfra.com/v1/openai",
        "key": "DEEPINFRA_API_KEY",
    },
    "qwen3-a3b-instruct": {
        "model": "willcb/Qwen3-30B-A3B-Instruct-2507-FP8",
        "url": "http://0.0.0.0:8000/v1",
        "key": "EMPTY",
    },
    "moonlight": {
        "model": "moonshotai/Moonlight-16B-A3B-Instruct",
        "url": "http://0.0.0.0:8000/v1",
        "key": "EMPTY",
    },
    "4b-it": {
        "model": "google/gemma-3-4b-it",
        "url": "http://0.0.0.0:8004/v1",
        "key": "EMPTY",
    },
    "devstral": {
        "model": "mistralai/Devstral-Small-2507",
        "url": "http://0.0.0.0:8002/v1",
        "key": "EMPTY",
    },
}



================================================
FILE: configs/zero3.yaml
================================================
compute_environment: LOCAL_MACHINE
debug: false
deepspeed_config:
  deepspeed_multinode_launcher: standard
  offload_optimizer_device: none
  offload_param_device: none
  zero3_init_flag: true
  zero3_save_16bit_model: true
  zero_stage: 3
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 8
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false


================================================
FILE: docs/README.md
================================================
# Verifiers Documentation

This directory contains source files for the `verifiers` documentation.

## Building the Documentation

### Prerequisites

```bash
# Or using uv
uv add sphinx furo myst-parser
```

### Build Commands

```bash
# Build HTML documentation
cd docs/
make html

# Or using uv from project root
cd docs/
uv run make html

# View the documentation
open build/html/index.html  # macOS
xdg-open build/html/index.html  # Linux
```


================================================
FILE: docs/Makefile
================================================
# Minimal makefile for Sphinx documentation
#

# You can set these variables from the command line, and also
# from the environment for the first two.
SPHINXOPTS    ?=
SPHINXBUILD   ?= sphinx-build
SOURCEDIR     = source
BUILDDIR      = build

# Put it first so that "make" without argument is like "make help".
help:
	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)

.PHONY: help Makefile

# Catch-all target: route all unknown targets to Sphinx using the new
# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
%: Makefile
	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)



================================================
FILE: docs/requirements.txt
================================================
sphinx
furo
myst-parser


================================================
FILE: docs/source/api_reference.md
================================================
# Type Reference

This guide explains the key types and data structures in Verifiers.

## Core Types

### Pydantic Models

Verifiers uses Pydantic models for structured data:

```python
from pydantic import BaseModel

class GenerateInputs(BaseModel):
    """Pydantic model for generation inputs."""

    prompt: list[Messages]
    answer: list[str] | None = None
    info: list[dict] | None = None
    task: list[str] | None = None
    completion: list[Messages] | None = None

class GenerateOutputs(BaseModel):
    """Pydantic model for generation outputs."""

    prompt: list[Messages]
    completion: list[Messages]
    answer: list[str]
    state: list[State]
    info: list[Info]
    task: list[str]
    reward: list[float]
    metrics: dict[str, list[float]] = Field(default_factory=dict)

class RolloutScore(BaseModel):
    """Pydantic model for rollout scores."""

    reward: float
    metrics: dict[str, float] = Field(default_factory=dict)


class RolloutScores(BaseModel):
    """Pydantic model for rubric outputs."""

    reward: list[float]
    metrics: dict[str, list[float]] = Field(default_factory=dict)


class ProcessedOutputs(BaseModel):
    """Pydantic model for processed outputs."""

    prompt_ids: list[list[int]]
    prompt_mask: list[list[int]]
    completion_ids: list[list[int]]
    completion_mask: list[list[int]]
    completion_logprobs: list[list[float]]
    rewards: list[float]
```

### State Dictionary

The `State` object tracks rollout information throughout an interaction:

```python
State = dict[str, Any]

# Common state fields during rollout:
{
    "prompt": list[ChatMessage],      # Original prompt messages
    "completion": list[ChatMessage],  # Model's response messages
    "answer": str,                    # Ground truth answer
    "task": str,                      # Task identifier (for EnvGroup)
    "info": dict[str, Any],          # Additional metadata from dataset
    "responses": list[Any],          # Raw LLM response objects
    
    # Custom fields added by specific environments:
    "turn": int,                     # Current turn number (MultiTurnEnv)
    "tools_called": list[str],       # Tool invocations (ToolEnv)
    "game_state": Any,               # Game-specific state
}
```

The `responses` field contains raw API response objects with:
- `choices[0].logprobs.content`: Token-level log probabilities
- `choices[0].logprobs.token_ids`: Token IDs
- `choices[0].finish_reason`: Why generation stopped
- `usage`: Token usage statistics

### Message Formats

```python
# Import from verifiers.types
from verifiers.types import ChatMessage, Messages

# Chat format (recommended)
# ChatMessage is a dict with these fields:
ChatMessage = {
    "role": str,                    # "system", "user", or "assistant"
    "content": str,                 # Message text
    "tool_calls": list[...],        # Optional tool calls
    "tool_call_id": str,            # Optional tool call ID
}

Messages = str | list[ChatMessage]  # Can be string (completion) or chat

# Example chat format:
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is 2+2?"},
    {"role": "assistant", "content": "2+2 equals 4."}
]

# Completion format (legacy):
completion = "Q: What is 2+2?\nA: 4"
```

### Reward Function Signature

All reward functions must follow this signature:

```python
RewardFunc = Callable[..., float]

def my_reward_func(
    completion: Messages,            # Model's response (chat or string)
    answer: str = "",                # Ground truth answer
    prompt: Messages | None = None,  # Original prompt
    state: State | None = None,      # Environment state
    parser: Parser | None = None,    # Parser instance (if rubric has one)
    **kwargs                         # Additional arguments
) -> float:
    """Return a float reward between 0.0 and 1.0."""
    return 1.0
```

### Environment Response

For `MultiTurnEnv.env_response`:

```python
def env_response(
    self,
    messages: list[ChatMessage],
    state: State,
    **kwargs
) -> tuple[Messages, State]:
    """
    Returns:
        - Response messages (list[ChatMessage] or str for completion mode)
        - Updated state
    """
    # Return a list of ChatMessage dicts (typical case)
    response = [{"role": "user", "content": "Environment feedback"}]
    
    # Update state as needed
    state["turn"] = state.get("turn", 0) + 1
    state["last_action"] = "provided feedback"
    
    return response, state
```

### Sampling Arguments

vLLM-specific generation parameters:

```python
SamplingArgs = dict[str, Any]

sampling_args = {
    # Basic sampling
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "max_tokens": 2048,
    
    # Advanced vLLM options
    "extra_body": {
        "logprobs": True,              # Return token logprobs
        "top_logprobs": 5,             # Top-k logprobs per token
        "skip_special_tokens": False,  # Include special tokens
        "guided_decoding": {           # Structured generation
            "regex": r"\d{3}-\d{3}-\d{4}"  # Phone number format
        }
    }
}
```

### Dataset Info

The `info` field in datasets can contain arbitrary metadata:

```python
Info = dict[str, Any]

# Dataset row with info dict:
{
    "prompt": "Solve this problem",
    "info": {
        "answer": "42",              # Required: ground truth
        "difficulty": "medium",      # Optional metadata
        "source": "textbook",
        "chapter": 3,
        "requires_tool": True
    }
}

# Access in reward functions:
def reward_func(completion, answer, info=None, **kwargs):
    difficulty = info.get("difficulty", "unknown") if info else "unknown"
    # Adjust scoring based on difficulty...
```

## Type Utilities

### Environment Rollout Types

```python
# Rollout returns
async def rollout(...) -> tuple[Messages, State]:
    """Returns (completion, final_state)"""

# Evaluation results
def evaluate(...) -> GenerateOutputs:
    """Returns GenerateOutputs with prompts, completions, rewards, states, etc."""

# Generation results  
def generate(...) -> GenerateOutputs:
    """Returns GenerateOutputs containing rollout data"""
```

### Parser Types

```python
# Parser return types can be anything
def parse(text: str) -> Any:
    """Can return str, dict, dataclass, etc."""

# parse_answer must return optional string
def parse_answer(completion: Messages) -> str | None:
    """Must return string answer or None"""
```

## Common Patterns

### Accessing Completion Content

```python
def get_text_content(completion: Messages) -> str:
    """Extract text from either format."""
    if isinstance(completion, str):
        return completion
    else:
        # Chat format - get last assistant message
        return completion[-1]["content"]
```

### State Initialization

```python
def reset_for_rollout(self, prompt: Messages, answer: str, info: Info | None) -> State:
    """Initialize state for new rollout."""
    state = {
        "prompt": prompt,
        "answer": answer,
        "info": info or {},
        "task": info.get("task", "default") if info else "default",
        "responses": [],
        # Add custom fields
        "turn": 0,
        "history": []
    }
    return state
```




================================================
FILE: docs/source/components.md
================================================
# Components

This guide covers the advanced components available in Verifiers: Rubrics, Tools, and Parsers. Each section includes practical examples of how to use these components in real-world scenarios.

## Advanced Rubrics

Beyond basic reward functions, Verifiers provides specialized rubric types for complex evaluation scenarios.

### JudgeRubric: LLM-Based Evaluation

Use language models to evaluate responses when rule-based scoring is insufficient:

```python
# Basic usage with default prompt
judge_rubric = vf.JudgeRubric()

# Custom evaluation criteria
judge_rubric = vf.JudgeRubric(
    judge_prompt="""Evaluate the response based on:
    1. Accuracy of the solution
    2. Clarity of explanation
    3. Appropriate use of mathematical notation
    
    Rate from 0.0 to 1.0."""
)
```

**Example: Multi-Step Math with Judge Evaluation**

```python
def load_environment(**kwargs):
    # Base rubric for correctness
    def check_answer(prompt, response, answer, state):
        final_answer = extract_number(response)
        return 1.0 if abs(final_answer - float(answer)) < 0.01 else 0.0
    
    base_rubric = vf.Rubric(funcs=[check_answer])
    
    # Add judge for solution quality
    judge = vf.JudgeRubric(
        judge_prompt="Evaluate the mathematical reasoning: Is each step justified? Are there logical errors?"
    )
    
    # Combine with RubricGroup
    return vf.SingleTurnEnv(
        dataset=dataset,
        rubric=vf.RubricGroup([base_rubric, judge]),
        **kwargs
    )
```

### RubricGroup: Combining Multiple Rubrics

Aggregate scores from different rubrics:

```python
# Combine different evaluation approaches
group = vf.RubricGroup([
    correctness_rubric,  # Weight: 1.0 (default)
    style_rubric,        # Weight: 1.0
    efficiency_rubric    # Weight: 1.0
])

# With custom weights: set weights inside each Rubric, then group them
correctness = vf.Rubric(funcs=[check_answer], weights=[2.0])
style = vf.Rubric(funcs=[style_score], weights=[1.0])

group = vf.RubricGroup([correctness, style])
```

**Example: Multi-Criteria Code Evaluation**

```python
class CodeEvalEnv(vf.MultiTurnEnv):
    def __init__(self, **kwargs):
        # Rubric 1: Correctness
        correctness = vf.Rubric(funcs=[self.test_correctness])
        
        # Rubric 2: Performance
        performance = vf.Rubric(funcs=[self.measure_performance])
        
        # Rubric 3: Style (via judge)
        style_judge = vf.JudgeRubric(
            judge_prompt="Rate code style: readability, naming, structure (0-1)"
        )
        
        # Combine all rubrics
        super().__init__(
            rubric=vf.RubricGroup([correctness, performance, style_judge]),
            **kwargs
        )
```

### ToolRubric: Tracking Tool Usage

Count total and per-tool calls during a rollout. Pass your tool functions to enable per-tool counters. By default, counts are added as metrics with zero weight; adjust `reward_weights` if you want the counts to affect reward.

```python
# Define tools (type hints + docstrings omitted for brevity)
def calculate(expr: str) -> float: ...
def search_web(query: str, max_results: int = 5) -> list[dict]: ...

# Initialize with tools to track
tool_rubric = vf.ToolRubric(tools=[calculate, search_web])

# Metrics exposed (names):
# - total_tool_calls
# - calculate_calls
# - search_web_calls

# Optional: turn counts into rewards by setting weights
# Index 0 corresponds to total_tool_calls; subsequent indices follow the tools order
tool_rubric.reward_weights[0] = -0.1   # penalize excessive tool calls
tool_rubric.reward_weights[2] = 0.2    # reward using search_web specifically
```

## Tools

Verifiers provides native support for tool calling, leveraging models' built-in function calling capabilities.

### Defining Tools

Tools are simple Python functions with type hints, and can be either sync or async:

```python
def calculate(expression: str) -> float:
    """Evaluate a mathematical expression safely."""
    # Use a safe math parser in production
    import ast
    return eval(expression, {"__builtins__": {}}, {})

async def search_web(query: str, max_results: int = 5) -> list[dict]:
    """Search the web for information.
    
    Args:
        query: Search query string
        max_results: Maximum number of results to return
        
    Returns:
        List of search results with title, snippet, and url
    """
    # Implementation here
    return results
```

### Using ToolEnv

ToolEnv automatically converts Python functions to tool schemas and handles tool calling:

```python
def load_environment(**kwargs):
    return vf.ToolEnv(
        dataset=dataset,
        tools=[calculate, search_web],  # Just pass the functions
        max_turns=10,
        rubric=rubric,
        **kwargs
    )
```

**Note**: ToolEnv uses the model's native tool calling format via the tokenizer's chat template. It automatically injects tool schemas into request payloads and treats `role: tool` messages as tool outputs. It does NOT impose any XML structure or require hardcoded patterns.

### Tool Design Best Practices

1. **Clear Signatures**: Use descriptive names and type hints
2. **Comprehensive Docstrings**: Models use these to understand tool purpose
3. **Error Handling**: Return helpful error messages, don't raise exceptions
4. **Timeouts**: Add timeouts for long-running operations
5. **Input Validation**: Validate and sanitize inputs

**Example: Wiki Search Environment**

```python
def wiki_search(query: str) -> str:
    """Search Wikipedia for information."""
    try:
        # Add timeout
        with timeout(5.0):
            results = wikipedia.search(query, results=3)
            if results:
                page = wikipedia.page(results[0])
                return f"Title: {page.title}\n\n{page.summary[:500]}..."
            return "No results found."
    except Exception as e:
        return f"Search error: {str(e)}"

def wiki_get_page(title: str) -> str:
    """Get full Wikipedia page content."""
    try:
        with timeout(5.0):
            page = wikipedia.page(title)
            return page.content[:2000]  # Limit length
    except Exception as e:
        return f"Page error: {str(e)}"

def load_environment(**kwargs):
    dataset = load_qa_dataset()  # Questions requiring research
    
    # Rubric rewards correct answers and efficient tool use
    rubric = vf.Rubric(
        funcs=[check_answer, efficiency_bonus],
        weights=[1.0, 0.2]
    )
    
    return vf.ToolEnv(
        dataset=dataset,
        tools=[wiki_search, wiki_get_page],
        max_turns=8,
        rubric=rubric,
        **kwargs
    )
```

### Complex Tool Examples

For more sophisticated tool setups, see the `wiki_search` environment in the repository, which demonstrates:
- Multiple interdependent tools
- State management across tool calls
- Sophisticated error handling
- Tool usage optimization

## Parsers

Parsers extract structured information from model outputs. While many tasks work with raw text, parsers help when you need specific formats.

### Built-in Parsers

#### XMLParser

Extract XML-tagged content:

```python
# Define which tags are expected in the output
# Strings define fixed tags; tuples define canonical name + allowed aliases
parser = vf.XMLParser(
    fields=["think", ("answer", "code")],
    answer_field="answer",
)

# In practice
response = "<think>Let me calculate...</think>\n<answer>42</answer>"
answer = parser.parse_answer(response)  # => "42"
```

#### ThinkParser

Separate reasoning from final answers:

```python
# Strip any content before </think>, then apply extract_fn

def extract_number(text: str) -> str:
    import re
    m = re.search(r"[-+]?\d*\.?\d+", text)
    return m.group() if m else ""

parser = vf.ThinkParser(extract_fn=extract_number)
```

### Custom Parser Patterns

Create domain-specific parsers by extending the base class:

**Example: Code Block Parser**

```python
class CodeParser(vf.Parser):
    """Extract and validate code blocks from responses."""
    
    def parse_answer(self, response: str) -> str:
        # Extract code between triple backticks
        import re
        code_blocks = re.findall(r'```(?:python)?\n(.*?)```', response, re.DOTALL)
        
        if not code_blocks:
            return ""
        
        # Return the last code block (usually the final solution)
        code = code_blocks[-1].strip()
        
        # Basic validation
        try:
            compile(code, '<string>', 'exec')
            return code
        except SyntaxError:
            return ""  # Invalid Python code
```

**Example: Math Step Parser**

```python
class MathStepParser(vf.Parser):
    """Parse step-by-step math solutions."""
    
    def parse_answer(self, response: str) -> str:
        lines = response.strip().split('\n')
        
        # Look for final answer patterns
        for line in reversed(lines):
            if any(marker in line.lower() for marker in ['therefore', 'answer:', '=']):
                # Extract number from this line
                import re
                match = re.search(r'[-+]?\d*\.?\d+', line)
                if match:
                    return match.group()
        
        return ""
    
    def get_format_reward_func(self):
        def reward_steps(prompt, response, answer, state):
            # Reward showing work
            steps = response.count('\n')
            return min(1.0, steps / 5)  # Expect ~5 steps
        return reward_steps
```

### Parser Integration

Parsers integrate seamlessly with environments and rubrics:

```python
def load_environment(**kwargs):
    parser = CodeParser()
    
    def code_runs(prompt, response, answer, state):
        code = parser.parse_answer(response)
        if not code:
            return 0.0
        try:
            exec(code)
            return 1.0
        except:
            return 0.0
    
    rubric = vf.Rubric(
        funcs=[code_runs, parser.get_format_reward_func()],
        weights=[1.0, 0.1]
    )
    
    return vf.SingleTurnEnv(
        dataset=dataset,
        parser=parser,
        rubric=rubric,
        **kwargs
    )
```

## Practical Examples

### Interactive Game Environment

Build a Wordle-like game with multi-turn interaction:

```python
from verifiers.types import Messages, State
from typing import Tuple

class WordleEnv(vf.MultiTurnEnv):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.max_guesses = 6
    
    def env_response(self, messages: Messages, state: State) -> Tuple[Messages, State]:
        if state.get("turn", 0) == 0:
            # First turn: initialize
            state["turn"] = 1
            state["target"] = state["answer"]
            state["guesses"] = []
            return [{"role": "user", "content": "Guess a 5-letter word. You have 6 attempts."}], state
        
        # Get the last assistant message
        last_msg = messages[-1]
        if last_msg["role"] != "assistant":
            return [], state  # No response if not assistant message
            
        guess = last_msg["content"].strip().upper()
        target = state["target"]
        
        # Validate guess
        if len(guess) != 5 or not guess.isalpha():
            return [{"role": "user", "content": "Please guess a 5-letter word."}], state
        
        # Generate feedback
        feedback = self.get_feedback(guess, target)
        state["guesses"].append(guess)
        state["turn"] += 1
        
        if guess == target:
            state["solved"] = True
            return [{"role": "user", "content": f"Correct! The word was {target}."}], state
        elif state["turn"] > self.max_guesses:
            state["failed"] = True
            return [{"role": "user", "content": f"Out of guesses. The word was {target}."}], state
        else:
            remaining = self.max_guesses - state["turn"] + 1
            return [{"role": "user", "content": f"{feedback}\n{remaining} guesses remaining."}], state
    
    def is_completed(self, messages: Messages, state: State) -> bool:
        return state.get("solved", False) or state.get("failed", False)
```

### Training Data Generation

Generate training data using environment rollouts:

```python
async def generate_training_data(env, client, model, num_samples=1000):
    """Generate diverse solutions for training."""
    results = []
    
    for i in range(num_samples):
        # Get a random prompt
        prompt = env.dataset[i]["prompt"]
        answer = env.dataset[i]["answer"]
        
        # Generate multiple solutions
        for temp in [0.3, 0.7, 1.0]:
            completion, state = await env.rollout(
                client=client,
                model=model,
                prompt=prompt,
                answer=answer,
                sampling_args={"temperature": temp, "max_tokens": 1000}
            )
            
            # Score the solution
            rewards = await env.rubric.score_rollout(
                prompt, completion, answer, state
            )
            
            # Save high-quality solutions
            if rewards["total"] > 0.8:
                results.append({
                    "prompt": prompt,
                    "completion": completion,
                    "score": rewards["total"]
                })
    
    return Dataset.from_list(results)
```

### Environment Composition

Build complex environments from simpler ones:

```python
def load_math_suite(**kwargs):
    """Comprehensive math environment covering multiple domains."""
    
    # Shared components
    parser = vf.ThinkParser(extract_fn=extract_boxed_answer)
    
    # Basic arithmetic
    arithmetic_env = vf.SingleTurnEnv(
        dataset=load_arithmetic_dataset(),
        parser=parser,
        rubric=vf.Rubric(funcs=[exact_match]),
        system_prompt="Solve the arithmetic problem."
    )
    
    # Algebra with tools
    algebra_env = vf.ToolEnv(
        dataset=load_algebra_dataset(),
        tools=[solve_equation, factor_polynomial],
        parser=parser,
        rubric=vf.Rubric(funcs=[check_algebra, tool_efficiency])
    )
    
    # Geometry with judge
    geometry_env = vf.SingleTurnEnv(
        dataset=load_geometry_dataset(),
        parser=parser,
        rubric=vf.RubricGroup([
            vf.Rubric(funcs=[check_geometry]),
            vf.JudgeRubric(judge_prompt="Rate the geometric reasoning and diagram interpretation.")
        ])
    )
    
    # Combine all
    return vf.EnvGroup(
        envs=[arithmetic_env, algebra_env, geometry_env],
        env_names=["arithmetic", "algebra", "geometry"],
        **kwargs
    )
```

## Best Practices

### For Rubrics
- Start simple with basic reward functions
- Use JudgeRubric when rule-based evaluation is insufficient
- Combine rubrics with RubricGroup for multi-faceted evaluation
- Test reward functions thoroughly with edge cases

### For Tools
- Keep tool functions simple and focused
- Use clear names and comprehensive docstrings
- Handle errors gracefully - return messages, don't raise
- Add timeouts for external operations
- Let the model's chat template handle tool calling format

### For Parsers
- Use built-in parsers when they fit your needs
- Create custom parsers for domain-specific formats
- Always handle parsing failures gracefully
- Consider providing format rewards to guide model output

## Next Steps

- Build your own environments using these components in [Environments](environments.md)
- Train models with your environments in [Training](training.md)
- Understand the type system in [Type Reference](api_reference.md)


================================================
FILE: docs/source/conf.py
================================================
# Configuration file for the Sphinx documentation builder.
#
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

import os
import sys

# Add project root to Python path for importing verifiers
sys.path.insert(0, os.path.abspath("../.."))  # ← up two levels to project root

# Import verifiers to get version dynamically
import verifiers

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information

project = "verifiers"
copyright = "2025, William Brown"
author = "William Brown"
release = verifiers.__version__

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration

extensions = [
    "myst_parser",
]


source_suffix = {
    ".rst": "restructuredtext",
    ".md": "markdown",
}

myst_enable_extensions = [
    "dollarmath",
    "amsmath",
    "deflist",
    "tasklist",
    "colon_fence",
]

templates_path = ["_templates"]
exclude_patterns = []


# -- Options for HTML output -------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output

html_theme = "furo"
html_title = "Verifiers"
html_theme_options = {
    "sidebar_hide_name": True,
    "navigation_with_keys": True,
}
# html_static_path = ["_static"]



================================================
FILE: docs/source/development.md
================================================
# Development & Testing

This guide covers development setup, testing, and contributing to the verifiers package.

## Setup

### Prerequisites
- Python 3.11 or 3.12
- [uv](https://docs.astral.sh/uv/) package manager

### Installation

```bash
# Clone and install for development
git clone https://github.com/willccbb/verifiers.git
cd verifiers
uv sync --all-extras
uv run pre-commit install
```

## Project Structure

```
verifiers/
├── verifiers/          # Main package
│   ├── envs/           # Environment classes
│   ├── parsers/        # Parser classes  
│   ├── rubrics/        # Rubric classes
│   └── utils/          # Utilities
├── environments/       # Installable environment modules
├── examples/           # Usage examples
├── tests/              # Test suite
└── docs/               # Documentation
```

## Running Tests

```bash
# Run all tests
uv run pytest tests/

# Run with coverage
uv run pytest tests/ --cov=verifiers --cov-report=html

# Run specific test file
uv run pytest tests/test_parsers.py

# Stop on first failure with verbose output
uv run pytest tests/ -xvs

# Run tests matching a pattern
uv run pytest tests/ -k "xml_parser"
```

The test suite includes 130+ tests covering parsers, rubrics, and environments. The test suite does not currently cover example environments or the trainer. If you require robust performance guarantees for training, you will likely want to use [prime-rl](https://github.com/PrimeIntellect-ai/prime-rl).

## Writing Tests

### Test Structure

```python
class TestFeature:
    """Test the feature functionality."""
    
    def test_basic_functionality(self):
        """Test normal operation."""
        # Arrange
        feature = Feature()
        
        # Act
        result = feature.process("input")
        
        # Assert
        assert result == "expected"
    
    def test_error_handling(self):
        """Test error cases."""
        with pytest.raises(ValueError):
            Feature().process(invalid_input)
```

### Using Mocks

The test suite provides mock OpenAI clients:

```python
from tests.mock_openai_client import MockOpenAIClient

def test_with_mock(mock_client):
    env = vf.SingleTurnEnv(client=mock_client)
    # Test without real API calls
```

### Guidelines

1. **Test both success and failure cases**
2. **Use descriptive test names** that explain what's being tested
3. **Leverage existing fixtures** from `conftest.py`
4. **Group related tests** in test classes
5. **Keep tests fast** - use mocks instead of real API calls

## Contributing

### Workflow

1. **Fork** the repository
2. **Create a feature branch**: `git checkout -b feature-name`
3. **Make changes** following existing patterns
4. **Add tests** for new functionality
5. **Run tests**: `uv run pytest tests/`
6. **Update docs** if adding/changing public APIs
7. **Submit PR** with clear description

### Code Style

- Follow existing conventions in the codebase
- Use type hints for function parameters and returns
- Write docstrings for public functions/classes
- Keep functions focused and modular

### PR Checklist

- [ ] Tests pass locally
- [ ] Added tests for new functionality
- [ ] Updated documentation if needed
- [ ] No breaking changes (or clearly documented)

## Common Issues

### Import Errors
```bash
# Ensure package is installed in development mode
uv pip install -e .
```

### Async Test Issues
```bash
# May need nest-asyncio for some environments
uv add nest-asyncio
```

### Test Failures
```bash
# Debug specific test
uv run pytest tests/test_file.py::test_name -vvs --pdb
```

## Environment Development

### Creating a New Environment Module

```bash
# Initialize template
vf-init my-environment

# Install locally for testing
vf-install my-environment

# Test your environment
vf-eval my-environment -m gpt-4.1-mini -n 5
```

### Environment Module Structure

```python
# my_environment.py
import verifiers as vf

def load_environment(**kwargs):
    """Load the environment."""
    dataset = vf.load_example_dataset("dataset_name")
    parser = vf.XMLParser(fields=["reasoning", "answer"])
    
    def reward_func(parser, completion, answer, **kwargs):
        return 1.0 if parser.parse_answer(completion) == answer else 0.0
    
    rubric = vf.Rubric(
        funcs=[reward_func, parser.get_format_reward_func()],
        weights=[1.0, 0.2],
        parser=parser
    )
    
    return vf.SingleTurnEnv(
        dataset=dataset,
        parser=parser,
        rubric=rubric,
        **kwargs
    )
```

## Quick Reference

### Essential Commands

```bash
# Development setup
uv sync --all-extras

# Run tests
uv run pytest tests/                    # All tests
uv run pytest tests/ -xvs              # Debug mode
uv run pytest tests/ --cov=verifiers   # With coverage

# Environment tools
vf-init new-env                        # Create environment
vf-install new-env                     # Install environment
vf-eval new-env                        # Test environment
vf-tui                                 # Browse eval results in your terminal

# Documentation
cd docs && make html                   # Build docs
```

### Project Guidelines

- **Environments**: Installable modules with `load_environment()` function
- **Parsers**: Extract structured data from model outputs
- **Rubrics**: Define multi-criteria evaluation functions
- **Tests**: Comprehensive coverage with mocks for external dependencies

For more details, see the full documentation at [readthedocs](https://verifiers.readthedocs.io).


================================================
FILE: docs/source/environments.md
================================================
# Environments

This guide covers how to create, develop, and use environments in Verifiers.

## Creating a New Environment

The recommended approach is to create an environment *module*, i.e. a self-contained package that can be installed and reused.

### Initialize from Template

```bash
vf-init my-math-env
```

This creates:
```
environments/my_math_env/
├── my_math_env.py      # Main implementation
├── pyproject.toml      # Dependencies and metadata
└── README.md           # Documentation
```

### Basic Environment Structure

Every environment module must export a `load_environment` function:

```python
# my_math_env.py
import verifiers as vf

def load_environment(**kwargs):
    """Load and configure the environment."""
    # 1. Load dataset
    dataset = vf.load_example_dataset("gsm8k", split="train")
    
    # 2. Configure parser
    parser = vf.ThinkParser()
    
    # 3. Define reward functions -- can automatically reference:
    # - parser, prompt, completion, answer, state , task, info 
    def correct_answer(parser, completion, answer):
        response = parser.parse_answer(completion) or ''
        return 1.0 if response.strip() == answer.strip() else 0.0
    
    # 4. Create rubric
    rubric = vf.Rubric(
        funcs=[correct_answer, parser.get_format_reward_func()],
        weights=[1.0, 0.2]
    )
    
    # 5. Return configured environment
    return vf.SingleTurnEnv(
        dataset=dataset,
        system_prompt="Think step-by-step, then give your answer.",
        parser=parser,
        rubric=rubric,
        **kwargs  # Pass through additional arguments
    )
```

### Adding Dependencies

Specify environment-specific dependencies in `pyproject.toml`:

```toml
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "my_math_env"
description = "Single-turn math environment"
tags = ["math", "verifiable-reward"]
version = "0.1.0"
requires-python = ">=3.11"
dependencies = [
    "verifiers",
    "sympy",  # For symbolic math
]
```

## Development Workflow

### 1. Install Your Environment

During development, install your environment locally:

```bash
vf-install my-math-env # wraps 'uv pip install -e ...'
```

This installs the module and its dependencies in your Python environment.

### 2. Test Your Environment

Use the CLI to quickly test:

```bash
vf-eval my-math-env -m gpt-4.1-mini -n 5 # runs a small batch of rollouts; use -h to see options
```

Or test programmatically:

```python
import verifiers as vf
from openai import OpenAI

# Load your environment
env = vf.load_environment("my-math-env")

# Test with a model
client = OpenAI()
results = env.evaluate(
    client, "gpt-4.1-mini",
    num_examples=5,
    rollouts_per_example=2,
    max_concurrent=32,
)
print(results)
```

### 3. Iterate on Design

Common iterations:
- Adjust system prompts for better performance
- Refine parser logic for edge cases
- Add new reward functions to the rubric
- Configure dataset filtering or sampling

## Working with Rubrics

Rubrics are central to defining what makes a good response in your environment. Here's how to use them effectively:

### Basic Reward Functions

A reward function takes the full context and returns a score (typically 0.0 to 1.0):

```python
def exact_match(prompt, completion, answer, state):
    """Reward exact matches."""
    response = completion[-1]['content']
    return 1.0 if response.strip() == answer.strip() else 0.0

def partial_credit(prompt, completion, answer, state):
    """Give partial credit for containing key terms."""
    key_terms = answer.lower().split()
    response = completion[-1]['content']
    found = sum(1 for term in key_terms if term in response.lower())
    return found / len(key_terms) if key_terms else 0.0
```

### Creating Rubrics

Combine multiple reward functions with weights:

```python
# Single criterion
rubric = vf.Rubric(funcs=[exact_match])

# Multi-criteria with weights
rubric = vf.Rubric(
    funcs=[exact_match, partial_credit, length_penalty],
    weights=[1.0, 0.5, 0.1]  # Relative importance
)
```

### Using Parser Format Rewards

Parsers often provide format reward functions:

```python
parser = vf.ThinkParser(extract_fn=extract_boxed_answer)

def correct_answer(parser, completion, answer):
    parsed = parser.parse_answer(completion) # applies extract_fn to final message
    return 1.0 if parsed == answer else 0.0

rubric = vf.Rubric(
    funcs=[
        correct_answer,
        parser.get_format_reward_func()  # Rewards proper <think> format
    ],
    weights=[1.0, 0.2]
)
```

### Stateful Reward Functions

Access environment state for complex evaluation:

```python
def efficiency_reward(prompt, response, answer, state):
    """Reward based on number of steps taken."""
    max_steps = 10
    steps_taken = state.get("turn", 0)
    return max(0, (max_steps - steps_taken) / max_steps)
```

## Environment Types

Choose the appropriate base class for your task:

### SingleTurnEnv

For one-shot tasks with clear input/output:

```python
def load_environment(**kwargs):
    return vf.SingleTurnEnv(
        dataset=dataset,
        system_prompt="Answer the question.", # only used if dataset has 'question' (str) and not 'prompt'
        parser=parser,
        rubric=rubric,
        **kwargs
    )
```

### MultiTurnEnv

For interactive tasks requiring multiple steps:

```python
from verifiers.types import Messages, State
from typing import Tuple

class MyGameEnv(vf.MultiTurnEnv):

    async def env_response(self, messages: Messages, state: State) -> Tuple[Messages, State]:
        """Define how the environment responds."""
        # Get the last message from the assistant
        last_msg = messages[-1]
        if last_msg["role"] == "assistant":
            player_action = last_msg["content"]
        else:
            return [], state  # No response if not assistant message
        
        # Check game state
        if self.is_game_over(state):
            response = [{"role": "user", "content": "Game over!"}]
            state["done"] = True
            return response, state
        
        # Update game state
        state = self.update_state(state, player_action)
        feedback = self.get_game_feedback(state)
        
        # Return list of ChatMessage dicts
        response = [{"role": "user", "content": feedback}]
        return response, state

def load_environment(**kwargs):
    return MyGameEnv(dataset=dataset, **kwargs)
```

### ToolEnv

For tasks requiring external tools:

```python
def calculate(expression: str) -> float:
    """Calculate a mathematical expression."""
    return eval(expression)  # Simplified example

def load_environment(**kwargs):
    return vf.ToolEnv(
        dataset=dataset,
        tools=[calculate],  # Automatically converted to tool schemas
        parser=parser,
        rubric=rubric,
        **kwargs
    )
```

## Advanced Patterns

### Configurable Environments

Accept parameters to customize behavior:

```python
def load_environment(
    dataset_name="gsm8k",
    num_examples=None,
    difficulty="all",
    use_calculator=False,
    **kwargs
):
    # Load dataset with filtering
    dataset = vf.load_example_dataset(dataset_name)
    if difficulty != "all":
        dataset = dataset.filter(lambda x: x["difficulty"] == difficulty)
    if num_examples:
        dataset = dataset.select(range(num_examples))
    
    # Conditionally add tools
    tools = [calculate] if use_calculator else []
    
    # Return appropriate environment type
    if tools:
        return vf.ToolEnv(dataset=dataset, tools=tools, **kwargs)
    else:
        return vf.SingleTurnEnv(dataset=dataset, **kwargs)
```

### Custom Datasets

Load datasets from various sources:

```python
def load_environment(dataset_path=None, **kwargs):
    if dataset_path:
        # Load from file
        dataset = Dataset.from_json(dataset_path)
    else:
        # Load from Hugging Face
        dataset = load_dataset("owner/dataset-name", split="train")
    
    # Ensure required columns
    assert "prompt" in dataset.column_names
    assert "answer" in dataset.column_names or "info" in dataset.column_names
    
    return vf.SingleTurnEnv(dataset=dataset, **kwargs)
```

### Composition with EnvGroup

Combine multiple environments for training on diverse tasks:

```python
def load_environment(**kwargs):
    # Environment 1: GSM8K
    gsm8k_dataset = vf.load_example_dataset("gsm8k")
    gsm8k_env = vf.SingleTurnEnv(
        dataset=gsm8k_dataset,
        parser=parser,
        rubric=gsm8k_rubric
    )
    
    # Environment 2: MATH
    math_dataset = vf.load_example_dataset("math")
    math_env = vf.SingleTurnEnv(
        dataset=math_dataset,
        parser=parser,
        rubric=math_rubric
    )
    
    # Create grouped environment
    return vf.EnvGroup(
        envs=[gsm8k_env, math_env],
        env_names=["gsm8k", "math"] # stored as "task" column
    )
```

**How EnvGroup Works:**
- **Dataset Concatenation**: Combines datasets from all environments with task labels
- **Automatic Routing**: Routes rollouts to the correct environment based on the `task` column
- **Unified Scoring**: Aggregates scores across all environments

This is particularly useful for:
- Training on multiple task types simultaneously
- Evaluating general capabilities across domains
- Creating curriculum learning setups

## Installing from Repository

Install environments from the verifiers repository:

```bash
# Install specific environment
vf-install math-python --from-repo

# Install from branch
vf-install wordle --from-repo -b dev

# List available environments
vf-install --list
```

## Best Practices

1. **Start Simple**: Begin with SingleTurnEnv and basic reward functions
2. **Test Early**: Use `vf-eval` to test your environment during development
3. **Document Well**: Include clear README with examples and expected behavior
4. **Handle Errors**: Ensure parsers and reward functions handle edge cases
5. **Version Dependencies**: Pin specific versions in pyproject.toml


## Next Steps

- See [Components](components.md) for advanced rubrics, tools, parsers, and practical examples
- Explore [Training](training.md) to use your environment for model improvement


================================================
FILE: docs/source/index.md
================================================
# Verifiers Documentation

Welcome to Verifiers! This library provides a flexible framework for creating RL environments and evaluations with custom multi-turn interaction protocols.

```{toctree}
:maxdepth: 2
:hidden:

overview
environments
components
training
development
api_reference
```

## What is Verifiers?

Verifiers enables you to:
- Define custom interaction protocols between models and environments
- Build agents, multi-turn conversations, tool-augmented reasoning, and interactive games
- Create reusable evaluation environments with multi-criteria reward functions
- Train models with the included HF GRPO trainer or integrate with other RL frameworks

Key features:
- **First-class OpenAI-compatibility** for `ChatCompletions` and `Completions`
- **Extensible multi-turn interactions** via `MultiTurnEnv` 
- **Native tool calling** support with `ToolEnv`
- **Modular reward functions** through `Rubric` classes
- **End-to-end async compatibility** with sync support where you want it
- **Full-spectrum scaling** from CPU evaluations in Jupyter to multi-node GPU RL training
- **Environments as Python modules** for easy installation, sharing, and reuse

## Installation

### Basic Installation

For evaluation and API model usage:
```bash
uv add verifiers
```

### Training Support

For GPU training with `vf.GRPOTrainer`:
```bash
uv add 'verifiers[all]' && uv pip install flash-attn --no-build-isolation
```

### Latest Development Version

To use the latest `main` branch:
```bash
uv add verifiers @ git+https://github.com/willccbb/verifiers.git
```

### Development Setup

For contributing to verifiers:
```bash
git clone https://github.com/willccbb/verifiers.git
cd verifiers
uv sync --all-extras && uv pip install flash-attn --no-build-isolation
uv run pre-commit install
```

### Integration with prime-rl

For large-scale FSDP training, see [prime-rl](https://github.com/PrimeIntellect-ai/prime-rl).

### Integration with Prime Intellect Environments Hub

Coming soon.

## Documentation

### Getting Started

**[Overview](overview.md)** — Core concepts and architecture. Start here if you're new to Verifiers to understand how environments orchestrate interactions.

**[Environments](environments.md)** — Creating custom interaction protocols with `MultiTurnEnv`, `ToolEnv`, and basic rubrics.

### Advanced Usage

**[Components](components.md)** — Advanced rubrics, tools, parsers, with practical examples. Covers judge rubrics, tool design, and complex workflows.

**[Training](training.md)** — GRPO training and hyperparameter tuning. Read this when you're ready to train models with your environments.

### Reference

**[Development](development.md)** — Contributing to verifiers

**[Type Reference](api_reference.md)** — Understanding data structures



================================================
FILE: docs/source/overview.md
================================================
# Overview

Verifiers provides a flexible framework for defining custom interaction protocols between LLMs and environments, enabling sophisticated multi-turn reasoning, tool use, and interactive evaluation.

The three key pieces of environments in Verifiers are:
- Your dataset (`str` or `List[ChatMessage]`)
- Your Rubric (one or more *reward functions*)
- Your *interaction protocol*, extended from `MultiTurnEnv`

## Core Concept: Interaction Protocols

Verifiers allows defining arbitrary interaction patterns between models and environments:

```
Environment (orchestration layer)
    ├── Defines interaction protocol (what to observe respond, how to respond, when to terminate)
    ├── Manages conversation state
    ├── Integrates tools and external resources
    └── Evaluates performance via Rubrics
```

### Example Protocols

- **Q&A Tasks**: Single model response → evaluation
- **Tool Use**: Model request → tool execution → model continues
- **Games**: Model move → game state update → environment feedback → repeat
- **Tutoring**: Model attempt → hint/correction → retry until correct
- **Debate**: Model A argument → Model B rebuttal → judge evaluation

## Environment Types

### MultiTurnEnv: Maximum Flexibility

The base class for custom interaction protocols:

```python
import verifiers as vf
from verifiers.types import Messages, State
from typing import Tuple

class MyProtocol(vf.MultiTurnEnv):
    async def env_response(self, messages: Messages, state: State) -> Tuple[Messages, State]:
        """Define how environment responds to model"""
        # Custom logic for your protocol
        response = [{"role": "user", "content": "Environment feedback"}]
        # Update state
        state["turn"] = state.get("turn", 0) + 1
        return response, state
    
    async def is_completed(self, messages: Messages, state: State) -> bool:
        """Define when interaction ends"""
        return state.get("task_complete", False)
```

### ToolEnv: Native Tool Calling

Leverages models' built-in tool calling for agentic workflows:

```python
env = vf.ToolEnv(
    tools=[search, calculate, execute_code],  # Python functions
    max_turns=10,
    dataset=dataset, # HuggingFace dataset
    rubric=rubric
)
```

Tools may be sync or async, and are automatically converted to JSON schemas and integrated with the model's native function calling format.

### SingleTurnEnv: Simple Evaluation

For straightforward Q&A tasks without interaction:

```python
env = vf.SingleTurnEnv(
    dataset=dataset,
    system_prompt="Answer the question."
    rubric=rubric,
)
```

## Key Components

### Rubrics: Multi-Criteria Evaluation

Rubrics define how to evaluate model responses by combining multiple criteria:

```python
# Simple reward function (can be sync or async)
async def correctness(prompt, completion, answer, state):
    return 1.0 if answer.lower() in completion[-1]['content'].lower() else 0.0

# Combine multiple criteria
rubric = vf.Rubric(
    funcs=[correctness, efficiency, clarity],
    weights=[1.0, 0.3, 0.2]  # Relative importance
)
```

Each reward function receives the full context (prompt, response, ground truth answer, and environment state) and returns a score. The rubric combines these scores based on weights to produce a final reward.

Common rubric patterns:
- **Single criterion**: One reward function (e.g., exact match)
- **Multi-criteria**: Weighted combination of multiple aspects
- **Judge-based**: Using LLMs to evaluate quality
- **Stateful**: Tracking patterns across interactions

### Environment Modules

Package your interaction protocol as a reusable module:

```
my_environment/
├── outputs/                # Evaluation logs
├── my_environment.py      # Defines load_environment() -> vf.Environment
├── pyproject.toml        # Dependencies
└── README.md            # Documentation
```

This enables:
- Easy sharing and versioning
- Dependency isolation
- Standardized interfaces


### State Management

Environments maintain state throughout interactions:

```python
state = {
    # automatically managed  
    "prompt": prompt, # inputs from dataset
    "completion": [], # trajectory so far
    "answer": answer, # golden answer (str)
    "task": task, # optional environment ID column
    "info": info, # evaluation metadata (dict) -- can use answer/info/both
    "responses": [], # Raw API responses from OpenAI client
    "turn": 0,
    # custom user-managed state
    "lives_remaining": 2,
    "inventory": {"potion": 1, "power-up": 2}
    ...
}
```

A wide variety of complex interaction protocols, reward schemes, and training algorithms can be coordinated via tracking appropriate data in `state`.

## Design Philosophy

### 1. Protocol-First Design

Start by defining your interaction pattern:
- When should the environment respond?
- What information should it provide?
- How should the conversation end?

### 2. Composable Evaluation

Build complex evaluation from simple parts:
- Individual reward functions for specific criteria
- Rubrics to combine and weight them
- Environments to orchestrate the process

### 3. OpenAI-Compatible Integration

Works with any OpenAI-compatible API:
```python
# OpenAI, vLLM, or any compatible endpoint
client = OpenAI(base_url="http://localhost:8000/v1") # or AsyncOpenAI
results = env.evaluate(client, model="llama-3.1-8b")
```

## Data Flow

1. **Dataset** provides prompts and ground truth
2. **Environment** orchestrates the interaction protocol
3. **Model** generates responses via OpenAI-compatible client
4. **Rubric** evaluates quality through reward functions
5. **Results** include full interaction traces and scores

## Evaluation lifecycle

- **Inputs expected by environments**:
  - `prompt`: str or list[ChatMessage] (chat-style). If you use `question` in your dataset, environments will turn it into a chat message, adding `system_prompt`/`few_shot` if provided.
  - `answer` or `info`: at least one is required. `answer` is a string; `info` is a dict for richer metadata.
  - `task`: optional string used by `EnvGroup`/`RubricGroup` to route behavior.

- **Running evaluation**:
  ```python
  results = env.evaluate(
      client, model,
      num_examples=100,
      rollouts_per_example=2,
      max_concurrent=32,
  )
  ```
  - `rollouts_per_example > 1` repeats dataset entries internally.
  - `max_concurrent` throttles concurrent rollouts.

- **Scoring**:
  - Each reward function returns a float. Weights applied inside `Rubric` combine them into `results.reward`.
  - All individual scores are logged under `results.metrics` keyed by function name (even if weight is 0.0).

- **Outputs** (`GenerateOutputs`):
  - `prompt`, `completion`, `answer`, `state`, `info`, `task`, `reward`, `metrics: dict[str, list[float]]`.

- **Message types**:
  - `message_type="chat"` (default) expects chat messages; `"completion"` expects raw text continuation. Choose based on your task (e.g., continuation quality uses completion).

## Optional Utilities

### Parsers

For extracting structured information when needed:
- `XMLParser`: Extract XML-tagged fields
- `ThinkParser`: Separate reasoning from answers
- Custom parsers for domain-specific formats

Parsers are optional conveniences - many environments work perfectly with raw text.

## Integration Points

### For Evaluation

The most convenient way to run quick evaluations is via the `vf-eval` CLI tool:
```bash
vf-install my-environment-module # from ./environments/my_environment_module 
vf-eval my-environment-module -m gpt-5 -n 10 -r 5 -s 
```

We also provide a TUI for browsing locally-cached (with `-s`) eval results:
```bash
vf-tui 
```

You can also evaluate models in your environments programmatically:
```python
results = env.evaluate(client, model, num_examples=100)
```

### For Training

```python
# Environments provide rollout-level interfaces for RL
completion, state = await env.rollout(
    client, model, prompt, answer
)
rewards = await env.rubric.score_rollout(
    prompt, completion, answer, state
)
# Or, process rollouts in batches for high throughput and configurable coordination
outputs = await env.a_generate(inputs, client, model, sampling_args) # `generate` for sync
```

### For Custom Workflows

All components can be used independently:
```python
# Use rubrics standalone
scores = await rubric.score_rollout(prompt, completion, answer, state)

# Create custom protocols
class MyProtocol(vf.MultiTurnEnv):
    # Your interaction logic
```

## Next Steps

- To create custom interactions, see [Environments](environments.md)
- For advanced component usage and examples, see [Components](components.md)
- To train models with your environments, see [Training](training.md)


================================================
FILE: docs/source/training.md
================================================
# Training

This guide covers training models with Verifiers using GRPO (Group Relative Policy Optimization).

## Training options

You can train with the built-in `GRPOTrainer` or with the external `prime-rl` project.

- Use `GRPOTrainer` when you want a lightweight Python training loop, LoRA/PEFT support, or small-to-mid scale runs (2–16 GPUs) using Accelerate/DeepSpeed.
- Use `prime-rl` when you want an FSDP-first, higher-throughput setup with more configuration surface area and performance-oriented defaults.

### Summary of similarities and differences

- Similarities
  - OpenAI-compatible inference (vLLM) and async rollouts
  - One-step off-policy overlap by default (generate at step n-1 while training at step n)

- Differences
  - GRPOTrainer: Accelerate/DeepSpeed-based; optional LoRA/PEFT; easy to script and extend in Python
  - PRIME-RL: FSDP-first; `rl` entrypoint; strong checkpointing; extensive CLI/TOML configuration

## Train with GRPOTrainer

The included `GRPOTrainer` supports GRPO-style training via Accelerate/DeepSpeed with vLLM inference. It's designed for:
- LoRA and smaller setups (2–16 GPUs)
- Full-parameter finetuning
- Experimentation and prototyping

## Quick Start

```python
import verifiers as vf

# 1. Create environment
env = vf.load_environment("math-python")

# 2. Load model
model, tokenizer = vf.get_model_and_tokenizer("Qwen/Qwen2.5-1.5B-Instruct")

# 3. Configure training  
args = vf.grpo_defaults(run_name="my-experiment")

# 4. Train
trainer = vf.GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    env=env,
    args=args,
)
trainer.train()
```

## Infrastructure Setup

### vLLM Server

Start a vLLM inference server for generation:

```bash
# Example: 6 GPUs for inference
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 vf-vllm --model Qwen/Qwen2.5-7B-Instruct \
    --data-parallel-size 6 --enforce-eager --disable-log-requests
```

### Training Launch

```bash
# Example: 2 GPUs for training
CUDA_VISIBLE_DEVICES=6,7 accelerate launch --config-file configs/zero3.yaml \
    --num-processes 2 your_training_script.py
```

## Key Hyperparameters

### Batch Configuration

```python
args = vf.grpo_defaults(run_name="experiment")

# Core batch settings
args.per_device_train_batch_size = 8    # Prompts per GPU per step
args.num_generations = 16               # Completions per prompt (group size)
args.gradient_accumulation_steps = 4    # Steps before optimizer update

# Effective batch size = per_device_train_batch_size * num_processes * gradient_accumulation_steps
# Must be divisible by num_generations
```

**How to think about batch settings:**
- `num_generations`: Larger groups (16-32) increase reward diversity but use more memory
- `per_device_train_batch_size`: Limited by GPU memory after model weights
- `gradient_accumulation_steps`: Use to achieve larger effective batch sizes

### Generation Parameters

```python
# Sampling configuration
args.max_tokens = 1024           # Max tokens per (turn-level) response 
args.temperature = 1.0          # Higher = more diverse completions
args.top_p = 1.0               # Nucleus sampling threshold
args.top_k = None              # Top-k filtering (None = disabled)

# Length limits
args.max_prompt_length = 1024      # Truncate prompts (left-truncated)
args.max_completion_length = 2048  # Truncate completions
args.max_seq_len = 4096           # Model's context window
```

**Generation strategy:**
- High temperature (0.8-1.0) increases diversity within groups
- Consider your model's context window when setting lengths
- Longer completions allow more complex reasoning but increase memory usage

### Training Schedule

```python
# Optimization settings
args.learning_rate = 1e-6              # Conservative default
args.lr_scheduler_type = "constant_with_warmup"
args.warmup_steps = 10                 # Gradual warmup
args.max_steps = 500                   # Total training steps
args.num_iterations = 1                # PPO-style updates per batch

# Gradient control
args.max_grad_norm = 0.01              # Aggressive clipping for stability
```

**Training dynamics:**
- Start with default `learning_rate = 1e-6` for stability
- `num_iterations > 1` does multiple updates per batch (more off-policy)
- Lower `max_grad_norm` for more stable but slower training

### GRPO-Specific Parameters

```python
# KL regularization
args.beta = 0.001                      # KL penalty coefficient
args.sync_ref_model = True             # Update reference model
args.ref_model_sync_steps = 100        # How often to sync
args.ref_model_mixup_alpha = 0.5       # Mix ratio for updates

# Loss configuration
args.loss_type = "dr_grpo"             # Recommended: no length bias
args.epsilon = 0.2                     # Clipping bound (lower)
args.delta = None                      # Optional upper clipping bound
```

**KL regularization:**
- `beta = 0` removes reference model (faster, less stable)
- `beta = 0.001` is conservative; some use 0.01-0.1
- Sync reference model periodically for long runs

### Async Generation

```python
# Overlapped training and inference
args.num_batches_ahead = 1      # Batches to generate ahead
args.async_generation_timeout = 300.0  # Timeout in seconds
args.max_concurrent = 1024      # Max concurrent env requests
```

**How async generation works:**
1. Maintains a pipeline of `num_batches_ahead` batches
2. While training on batch N, generates batch N+1
3. Overlaps compute-bound training with I/O-bound generation
4. Set `num_batches_ahead = 0` for synchronous (debug) mode

## Evaluation During Training

```python
# Add evaluation dataset
args.eval_strategy = "steps"
args.eval_steps = 100
args.per_device_eval_batch_size = 16

# The environment can provide an eval dataset
env = vf.load_environment("math-python", eval_split="test")
```

## Parameter-Efficient Training

For large models or limited GPU memory:

```python
trainer = vf.GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    env=env,
    args=args,
    peft_config=vf.lora_defaults(r=8, alpha=16)
)
```

## GRPO Rules of Thumb

RL is notoriously sensitive to implementation details. Here's practical guidance:

### Before Training

1. **Evaluate baseline performance**: If your model gets 0% reward after 10+ attempts, the task is too hard
2. **Check task difficulty**: If baseline is already 80%+, consider harder examples
3. **Ensure reward diversity**: You want varied scores within each generation group

### Stability vs Performance Trade-offs

**For more aggressive training** (higher risk of collapse):
- Set `beta = 0` (no KL penalty)
- Increase learning rate (2e-6 to 5e-6)
- Increase `num_iterations` (2-4)

**For more stable training** (slower progress):
- Increase `num_generations` (32-64)
- Increase batch size via `gradient_accumulation_steps`
- Decrease `max_grad_norm` (0.001-0.005)
- Use larger models (14B+)
- Keep `num_iterations = 1` (stay on-policy)

### Best Practices

**Likely beneficial:**
- Learning rate warmup (10-20 steps minimum)
- Periodic reference model updates for 500+ step runs
- One-step off-policy training (`num_batches_ahead = 1`)

**Context-dependent:**
- High `beta` values (0.1+) - more conservative
- Overlong filtering - depends on task
- Tool response masking - useful for multi-turn

**Key insight**: The best way to improve training is ensuring appropriate task difficulty for your model - not too easy, not too hard.

## Troubleshooting

### Common Issues

**Non-Increasing Chat Templates:** The Qwen3 and DeepSeek-R1 model series both remove `<think>` sections from messages when processing inputs, which violates the increasing context requirement for multi-turn GRPO-style training. We provide versions of many of these models with modified chat templates [here](https://huggingface.co/collections/willcb/qwen3-68434f4883925bfdb4570ee5).

**OOM during generation:**
- Reduce `num_generations` or `per_device_train_batch_size`
- Use LoRA instead of full finetuning
- Check vLLM server has sufficient memory

**Training instability:**
- Reduce learning rate
- Decrease `max_grad_norm`
- Increase `beta` for stronger KL regularization

**Poor reward diversity:**
- Increase temperature
- Check if task difficulty matches model capability
- Ensure your rubric differentiates quality levels

### Infrastructure
- Ensure `huggingface` and `wandb` logins are configured
- Set `OPENAI_API_KEY` (can be dummy for vLLM)
- Increase ulimit for high concurrency: `ulimit -n 4096`
- For NCCL issues: try `NCCL_P2P_DISABLE=1`

## Advanced Configuration

### Custom Sampling

```python
# Fine-grained generation control
args.repetition_penalty = 1.1   # Reduce repetition
args.top_k = 50                # Limit vocabulary
args.min_p = 0.05              # Min probability threshold
```

### Resource Optimization

```python
# Memory-constrained settings
args.gradient_checkpointing = True
args.ds3_gather_for_generation = False  # For very large models
args.generation_batch_size = 16  # Control generation batch size
```

### Monitoring

```python
# Logging configuration
args.logging_steps = 1
args.log_completions = True
args.report_to = "wandb"  # or "none" to disable
args.num_completions_to_print = 5  # Sample size to log
```

## Train with PRIME-RL

If you prefer an FSDP-first setup with higher throughput, you can train the same `verifiers` Environments using `prime-rl`.

- Install `prime-rl` (see its README for CUDA requirements):

```bash
curl -sSL https://raw.githubusercontent.com/PrimeIntellect-ai/prime-rl/main/scripts/install.sh | bash
```

- Create or install a Verifiers Environment module (inside your `prime-rl` checkout if developing there):

```bash
# create a new Environment template
uv run vf-init vf-custom-environment

# OR install an existing Environment from this repo
uv run vf-install vf-math-python --from-repo
```

- Configure the orchestrator to use your Environment. In your orchestrator TOML (e.g. `configs/my_exp/orch.toml`):

```toml
[environment]
id = "vf-math-python"  # or your custom environment ID

[environment.args]
# Example args forwarded to the Environment
split = "train"
rollouts_per_example = 8
max_concurrent = 512
```

- Launch a single-node run (adjust GPU split to your hardware):

```bash
uv run rl \
  --trainer @ configs/my_exp/train.toml \
  --orchestrator @ configs/my_exp/orch.toml \
  --inference @ configs/my_exp/infer.toml \
  --trainer-gpus 2 --inference-gpus 6
```

Tips:
- Use `bash scripts/tmux.sh` in `prime-rl` to open a panes layout for trainer/orchestrator/inference logs.
- Log to W&B by adding `--wandb.project <proj> --wandb.name <run>` on `uv run rl` (shared to trainer + orchestrator).
- For checkpointing/resume, see the `prime-rl` README (supports step-tagged checkpoints across trainer/orchestrator).


## Next Steps

- Explore [Environments](environments.md) to create custom tasks
- Review [Components](components.md) for advanced patterns
- See the [examples directory](https://github.com/willccbb/verifiers/tree/main/examples) on GitHub for complete training scripts


================================================
FILE: environments/continuation_quality/README.md
================================================
# continuation-quality

### Overview
- **Environment ID**: `continuation-quality`
- **Short description**: Single-turn quality grades on base model continuations using a judge model.
- **Tags**: single-turn, completions, base-model

### Datasets
- **Primary dataset(s)**: `agentlans/wikipedia-paragraphs` mapped to prefix/ground-truth continuation
- **Source links**: Hugging Face Datasets
- **Split sizes**: Train split filtered to adequately-long paragraphs

### Task
- **Type**: single-turn
- **Parser**: custom
- **Rubric overview**: Judge model letter grade (gpt-4.1-mini-based by default)

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval continuation-quality
```

Configure model and sampling:

```bash
uv run vf-eval continuation-quality   -m gpt-4.1-mini   -n 20 -r 3 -t 1024 -T 0.7   -a '{"key": "value"}'  # env-specific args as JSON
```

Notes:
- Use `-a` / `--env-args` to pass environment-specific configuration as a JSON object.

### Environment Arguments
Document any supported environment arguments and their meaning. Example:

| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `dataset_name` | str | `"agentlans/wikipedia-paragraphs"` | Training dataset |
| `dataset_split` | str | `"train"` | Training dataset split |
| `dataset_key` | str | `"text"` | Column in dataset with training text |
| `judge_model` | str | `"gpt-4.1-mini"` | Model to judge continuations with |
| `judge_base_url` | str | `"https://api.openai.com/v1"` | API base URL for judge model |
| `judge_api_key_var` | str | `"OPENAI_API_KEY"` | Environment variable containing the judge model API key |

### Metrics
Summarize key metrics your rubric emits and how they’re interpreted.

| Metric | Meaning |
| ------ | ------- |
| `reward` | Main scalar reward (weighted sum of criteria) |



================================================
FILE: environments/continuation_quality/continuation_quality.py
================================================
import os
import random

from datasets import load_dataset
from openai import OpenAI

import verifiers as vf

_rand = random.Random(777)


def make_cut(text: str) -> dict[str, str]:
    """Makes a random cut somewhere in the paragraph"""
    n_spaces = text.count(" ")
    # mostly split near the middle
    split_space = int(_rand.normalvariate(0.5, 0.15) * n_spaces)
    # make sure there's at least ~25 words before and after the split point
    split_space = min(n_spaces - 25, max(25, split_space))
    idx = -1
    for _ in range(split_space):
        idx = text.find(" ", idx + 1)
    return {"prompt": text[:idx], "answer": text[idx:]}


def load_environment(
    dataset_name: str = "agentlans/wikipedia-paragraphs",
    dataset_split: str | None = "train",
    dataset_key: str = "text",
    judge_model: str = "gpt-4.1-mini",
    judge_base_url: str = "https://api.openai.com/v1",
    judge_api_key_var: str = "OPENAI_API_KEY",
) -> vf.Environment:
    dataset = load_dataset(dataset_name, split=dataset_split)
    # only accept examples with >~100 words or so
    dataset = dataset.filter(lambda x: x[dataset_key].count(" ") > 100)
    dataset = dataset.map(lambda x: make_cut(x[dataset_key]))
    dataset = dataset.shuffle(seed=777)

    judge_client = OpenAI(api_key=os.getenv(judge_api_key_var), base_url=judge_base_url)
    judge_prompt = """Evaluate this base model contination from a prefix, compared to the true continuation from Wikipedia.

<prefix>
{question}
</prefix>

<true_continuation>
{answer}
</true_continuation>

<model_continuation>
{response}
</model_continuation>

Provide a letter grade from A-F where:
- A: Smooth prose, facts are mostly accurate w.r.t the true continuation
- B: Smooth prose, regardless of factual accuracy
- C: Some awkward wording, spacing, or punctuation
- D: Inclusions of awkward or glitchy text along with promising prose, some coherent sentences
- F: Incoherent text

Think aloud in a <scratchpad> for a few lines, then respond with the letter grade in <grade> ... </grade> tags."""
    rubric = vf.JudgeRubric(
        judge_client=judge_client,
        judge_model=judge_model,
        judge_prompt=judge_prompt,
    )

    grade_parser = vf.XMLParser(fields=["grade"], answer_field="grade")

    def grade_reward(prompt, completion, answer, state, **kwargs) -> float:
        judge_response = rubric.judge(prompt, completion, answer, state, **kwargs)
        judge_grade = (
            (grade_parser.parse_answer(judge_response) or "F")
            .strip()
            .replace("+", "")
            .replace("-", "")
            .upper()
        )
        return {
            "A": 1.0,
            "B": 0.75,
            "C": 0.5,
            "D": 0.25,
        }.get(judge_grade, 0.0)

    rubric.add_reward_func(grade_reward, weight=1.0)

    return vf.SingleTurnEnv(
        message_type="completion",
        dataset=dataset,
        parser=vf.Parser(),
        rubric=rubric,
        sampling_args={
            "stop": ["\n"],
        },
    )



================================================
FILE: environments/continuation_quality/pyproject.toml
================================================
[project]
name = "continuation-quality"
version = "0.1.0"
dependencies = [
    "verifiers>=0.1.2",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["continuation_quality.py"]



================================================
FILE: environments/doublecheck/README.md
================================================
# doublecheck

### Overview
- **Environment ID**: `doublecheck`
- **Short description**: Two-turn math QA that asks the model to answer, then prompts “Are you sure?”; scored with a math rubric.
- **Tags**: math, multi-turn, xml, think-answer, verification

### Datasets
- **Primary dataset(s)**: `math` (example dataset loaded via `load_example_dataset`)
- **Source links**: Uses the example loader in `verifiers.utils.data_utils`
- **Split sizes**: Configurable via args; defaults to `train` split and all examples

### Task
- **Type**: multi-turn
- **Parser**: XMLParser with fields `think`, `answer` (from `MathRubric`)
- **Rubric overview**: `MathRubric` combining exact/equivalence math grading and a small format component

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval doublecheck
```

Configure model and sampling:

```bash
uv run vf-eval doublecheck \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"dataset_name": "math", "dataset_split": "train", "num_train_examples": -1}'
```

Notes:
- Use `-a` / `--env-args` to pass environment-specific configuration as a JSON object.

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `dataset_name` | str | `"math"` | Example dataset name for math problems |
| `dataset_split` | str | `"train"` | Dataset split to load |
| `num_train_examples` | int | `-1` | Limit on dataset size (`-1` for all) |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | Math answer correctness (symbolic/numeric equivalence) |
| `format_reward` | Adherence to `<think>`/`<answer>` XML format |



================================================
FILE: environments/doublecheck/doublecheck.py
================================================
from typing import Dict, List, Tuple

from datasets import Dataset
from openai import OpenAI

from verifiers import (
    Messages,
    MultiTurnEnv,
    RewardFunc,
    State,
)
from verifiers.rubrics.math_rubric import MathRubric
from verifiers.utils.data_utils import load_example_dataset

SIMPLE_PROMPT = """
Respond in the following format, using careful step-by-step reasoning.

<reasoning>
...
</reasoning>
<answer>
...
</answer>
"""


class DoubleCheckEnv(MultiTurnEnv):
    def __init__(
        self,
        client: OpenAI | None = None,
        model: str | None = None,
        dataset: Dataset | None = None,
        eval_dataset: Dataset | None = None,
        system_prompt: str = SIMPLE_PROMPT,
        few_shot: List[Dict[str, str]] = [],
        **kwargs,
    ):
        super().__init__(
            client=client,
            model=model,
            dataset=dataset,
            eval_dataset=eval_dataset,
            system_prompt=system_prompt,
            few_shot=few_shot,
            **kwargs,
        )
        self.rubric = MathRubric()

    def get_reward_funcs(self, **kwargs) -> List[RewardFunc]:
        return self.rubric.get_reward_funcs()

    def get_reward_weights(self, **kwargs) -> List[float]:
        return self.rubric.get_reward_weights()

    def is_completed(self, messages: Messages, state: State, **kwargs) -> bool:
        return len(state["responses"]) == 1

    def env_response(
        self, messages: Messages, state: State, **kwargs
    ) -> Tuple[Messages, State]:
        return [{"role": "user", "content": "Are you sure?"}], state


def load_environment(
    dataset_name: str = "math",
    dataset_split: str = "train",
    num_train_examples: int = -1,
):
    dataset = load_example_dataset(dataset_name, dataset_split, n=num_train_examples)
    vf_env = DoubleCheckEnv(dataset=dataset, system_prompt=SIMPLE_PROMPT, few_shot=[])
    return vf_env



================================================
FILE: environments/doublecheck/pyproject.toml
================================================
[project]
name = "doublecheck"
description = "Test environment for double-checking math answers"
tags = ["doublecheck", "multi-turn", "math", "hendrycks", "train", "eval", "example"]
version = "0.1.4"
requires-python = ">=3.11"
dependencies = [
    "verifiers>=0.1.2.post0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["doublecheck.py"]



================================================
FILE: environments/gpqa/README.md
================================================
# gpqa

### Overview
- **Environment ID**: `gpqa`
- **Short description**: Multiple-choice GPQA evaluation with optional Chain-of-Thought and Diamond/Main split selection.

### Datasets
- **Primary dataset(s)**: `gpqa_diamond` (hard subset) or `gpqa_main` (full set), via `load_example_dataset`
- **Source links**: Uses the example loader in `verifiers.utils.data_utils`
- **Split sizes**: Uses `train` split for evaluation by default

### Task
- **Type**: single-turn
- **Parser**: `ThinkParser` when `use_think=True` (default), else a basic `Parser`
- **Rubric overview**: Exact letter match to the correct choice; optional format component from parser

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval gpqa
```

Configure model and sampling:

```bash
uv run vf-eval gpqa \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"use_diamond": true, "use_think": true}'
```

Notes:
- Use `-a` / `--env-args` to pass environment-specific configuration as a JSON object.


### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `use_diamond` | bool | `true` | Use the GPQA Diamond subset instead of Main |
| `use_think` | bool | `true` | Use `<think>` CoT and `ThinkParser`; otherwise, no-think mode |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | 1.0 if predicted letter matches target, else 0.0 |



================================================
FILE: environments/gpqa/gpqa.py
================================================
import verifiers as vf


def load_environment(
    use_diamond: bool = True, use_think: bool = True
) -> vf.Environment:
    from verifiers.utils.data_utils import load_example_dataset

    if use_diamond:
        eval_dataset = load_example_dataset("gpqa_diamond", "train")
    else:
        eval_dataset = load_example_dataset("gpqa_main", "train")
    if use_think:
        system_prompt = """Think step-by-step inside <think>...</think> tags, then give only the letter of the correct answer."""
        parser = vf.ThinkParser()
    else:
        system_prompt = """Give only the letter of the correct answer. /no_think"""
        parser = vf.Parser()

    def correct_answer_reward_func(completion, answer, **kwargs) -> float:
        response = parser.parse_answer(completion) or ""
        return 1.0 if response.startswith(str(answer)) else 0.0

    rubric = vf.Rubric(funcs=[correct_answer_reward_func], weights=[1.0])
    vf_env = vf.SingleTurnEnv(
        eval_dataset=eval_dataset,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
    )
    judge_rubric = vf.JudgeRubric()

    async def judge_reward(judge, prompt, completion, answer, state):
        judge_response = await judge(prompt, completion, answer, state)
        print("J")
        return 1.0 if "yes" in judge_response.lower() else 0.0

    judge_rubric.add_reward_func(judge_reward, 1.0)
    vf_env.rubric = vf.RubricGroup([judge_rubric, vf_env.rubric])
    return vf_env



================================================
FILE: environments/gpqa/pyproject.toml
================================================
[project]
name = "gpqa"
version = "0.1.0"
description = "GPQA environment"
tags = ["gpqa", "single-turn", "eval", "multiple-choice"]
requires-python = ">=3.11"
dependencies = [
    "verifiers>=0.1.2.post0",
    "datasets",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["gpqa.py"]



================================================
FILE: environments/gsm8k/README.md
================================================
# gsm8k

### Overview
- **Environment ID**: `gsm8k`
- **Short description**: Single-turn GSM8K math word problems with boxed numeric answers and CoT.
- **Tags**: math, gsm8k, single-turn, think, boxed-answer

### Datasets
- **Primary dataset(s)**: `gsm8k` train (train) and test (eval) via `load_example_dataset`
- **Source links**: Uses the example loader in `verifiers.utils.data_utils`
- **Split sizes**: Configurable via args; defaults to full train/test

### Task
- **Type**: single-turn
- **Parser**: `ThinkParser` with boxed answer extraction
- **Rubric overview**: Exact match on parsed boxed answer; optional format check

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval gsm8k
```

Configure model and sampling:

```bash
uv run vf-eval gsm8k \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"num_train_examples": -1, "num_eval_examples": -1}'
```

Notes:
- Use `-a` / `--env-args` to pass environment-specific configuration as a JSON object.
- Reports are written under `./environments/gsm8k/reports/` and auto-embedded below.

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `num_train_examples` | int | `-1` | Limit training set size (`-1` for all) |
| `num_eval_examples` | int | `-1` | Limit eval set size (`-1` for all) |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | 1.0 if parsed boxed answer equals target, else 0.0 |
| `format_reward` | Adherence to `<think>` + boxed `\boxed{...}` format |

## Evaluation Reports

<!-- Do not edit below this line. Content is auto-generated. -->
<!-- vf:begin:reports -->
<p>No reports found. Run <code>uv run vf-eval gsm8k -a '{"key": "value"}'</code> to generate one.</p>
<!-- vf:end:reports -->



================================================
FILE: environments/gsm8k/gsm8k.py
================================================
import verifiers as vf
from verifiers.utils.data_utils import (
    BOXED_SYSTEM_PROMPT,
    extract_boxed_answer,
    load_example_dataset,
)


def load_environment(
    use_think: bool = True,
    system_prompt: str = BOXED_SYSTEM_PROMPT,
    num_train_examples=-1,
    num_eval_examples=-1,
):
    dataset = load_example_dataset("gsm8k", split="train")
    if num_train_examples != -1:
        dataset = dataset.select(range(num_train_examples))
    eval_dataset = load_example_dataset("gsm8k", split="test")
    if num_eval_examples != -1:
        eval_dataset = eval_dataset.select(range(num_eval_examples))

    if use_think:
        parser = vf.ThinkParser(extract_fn=extract_boxed_answer)
    else:
        parser = vf.Parser(extract_fn=extract_boxed_answer)

    def correct_answer_reward_func(parser, completion, answer, **kwargs):
        response = parser.parse_answer(completion) or ""
        return 1.0 if response == answer else 0.0

    rubric = vf.Rubric(
        funcs=[correct_answer_reward_func, parser.get_format_reward_func()],
        weights=[1.0, 0.0],
    )

    vf_env = vf.SingleTurnEnv(
        dataset=dataset,
        eval_dataset=eval_dataset,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
    )
    return vf_env



================================================
FILE: environments/gsm8k/pyproject.toml
================================================
[project]
name = "gsm8k"
version = "0.1.0"
description = "GSM8K environment"
tags = ["gsm8k", "single-turn", "math", "train", "eval"]
requires-python = ">=3.11"
dependencies = [
    "verifiers>=0.1.2.post0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["gsm8k.py"]



================================================
FILE: environments/math_group/README.md
================================================
# math-group

### Overview
- **Environment ID**: `math-group`
- **Short description**: Groups GSM8K and MATH single-turn sub-environments into one evaluation with a shared math CoT parser.
- **Tags**: math, group, single-turn, gsm8k, math, think

### Datasets
- **Primary dataset(s)**: `gsm8k` (subset of 1000 train examples) and `math` (subset of 1000 train examples)
- **Source links**: Uses example loader in `verifiers.utils.data_utils`
- **Split sizes**: 1000 per sub-environment (train-only in this group)

### Task
- **Type**: single-turn (EnvGroup of two SingleTurnEnv instances)
- **Parser**: `ThinkParser` with boxed answer extraction
- **Rubric overview**: Exact numeric/equivalent match per sub-environment plus optional format check

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval math-group
```

Configure model and sampling:

```bash
uv run vf-eval math-group \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7
```

Notes:
- This environment bundles two math datasets; reporting aggregates across both.
- Reports are written under `./environments/math_group/reports/` and auto-embedded below.

### Environment Arguments
This loader does not expose custom arguments.

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | 1.0 if parsed boxed answer equals target (per sub-env), else 0.0 |
| `format_reward` | Adherence to `<think>` + boxed `\boxed{...}` format |

## Evaluation Reports

<!-- Do not edit below this line. Content is auto-generated. -->
<!-- vf:begin:reports -->
<p>No reports found. Run <code>uv run vf-eval vf-math-group -a '{"key": "value"}'</code> to generate one.</p>
<!-- vf:end:reports -->



================================================
FILE: environments/math_group/math_group.py
================================================
import verifiers as vf
from verifiers.utils.data_utils import (
    BOXED_SYSTEM_PROMPT,
    extract_boxed_answer,
    load_example_dataset,
)


def load_environment(**kwargs):
    system_prompt = """
    Think step-by-step inside <think>...</think> tags.

    Then, give your final numerical answer inside \\boxed{{...}}.
    """

    # env 1: gsm8k
    parser = vf.ThinkParser(extract_fn=extract_boxed_answer)

    def gsm8k_answer_reward_func(parser, completion, answer, **kwargs):
        response = parser.parse_answer(completion) or ""
        return 1.0 if response == answer else 0.0

    rubric1 = vf.Rubric(
        funcs=[gsm8k_answer_reward_func, parser.get_format_reward_func()],
        weights=[1.0, 0.0],
    )
    dataset1 = load_example_dataset("gsm8k", split="train").select(range(1000))
    env1 = vf.SingleTurnEnv(
        dataset=dataset1,
        system_prompt=BOXED_SYSTEM_PROMPT,
        parser=parser,
        rubric=rubric1,
    )

    # env 2: math
    def math_answer_reward_func(completion, answer, **kwargs):
        response = parser.parse_answer(completion) or ""
        return 1.0 if response == answer else 0.0

    rubric2 = vf.Rubric(
        funcs=[math_answer_reward_func, parser.get_format_reward_func()],
        weights=[1.0, 0.2],
    )
    dataset2 = load_example_dataset("math", split="train").select(range(1000))
    env2 = vf.SingleTurnEnv(
        dataset=dataset2,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric2,
    )

    vf_env = vf.EnvGroup([env1, env2], env_names=["gsm8k", "math"])
    return vf_env



================================================
FILE: environments/math_group/pyproject.toml
================================================
[project]
name = "math-group"
version = "0.1.0"
dependencies = [
    "verifiers>=0.1.2",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["math_group.py"]



================================================
FILE: environments/math_python/README.md
================================================
# math-python

### Overview
- **Environment ID**: `math-python`
- **Short description**: Tool-using math environment requiring Python tool calls to compute answers; graded by symbolic equivalence.
- **Tags**: math, tools, python, single-turn, boxed-answer

### Datasets
- **Primary dataset(s)**: Example `math` dataset via `load_example_dataset`
- **Source links**: Uses example loader in `verifiers.utils.data_utils`
- **Split sizes**: Configurable via args; defaults to `train` split and all examples

### Task
- **Type**: tool use (single-turn ToolEnv)
- **Parser**: Basic `Parser` with boxed answer extraction
- **Rubric overview**: Correctness by `math_verify.parse` + `verify`; logs auxiliary metrics (#turns, #tool calls, #errors)

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval math-python
```

Configure model and sampling:

```bash
uv run vf-eval math-python \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"dataset_name": "math", "dataset_split": "train", "num_train_examples": -1}'
```

Notes:
- Use `-a` / `--env-args` to pass environment-specific configuration as a JSON object.
- Reports are written under `./environments/math_python/reports/` and auto-embedded below.

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `dataset_name` | str | `"math"` | Example dataset to load |
| `dataset_split` | str | `"train"` | Split to load |
| `num_train_examples` | int | `-1` | Limit dataset size (`-1` for all) |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `correct_answer_reward_func` | 1.0 if symbolic verification passes, else 0.0 |
| `num_turns` | Number of assistant messages in completion |
| `num_tool_calls` | Number of tool messages in completion |
| `num_errors` | Count of tool error messages |

## Evaluation Reports

<!-- Do not edit below this line. Content is auto-generated. -->
<!-- vf:begin:reports -->
<details><summary>Reports</summary>
<details><summary>vf-math-python--v0.1.0--model=gpt-4.1-mini--n=5--r=3--args=noargs</summary>
<p><a href="reports/vf-math-python--v0.1.0--model=gpt-4.1-mini--n=5--r=3--args=noargs.html" target="_blank">Open full report</a></p>
<h3>vf-math-python: gpt-4.1-mini (n=5, r=3)</h3>
<div class="meta">
<div><b>Environment</b>: vf-math-python (v0.1.0)</div>
<div><b>Model</b>: <span class="code">gpt-4.1-mini</span></div>
<div><b>Provider</b>: https://api.openai.com/v1</div>
<div><b>Samples</b>: n=5, r=3</div>
<div><b>Date</b>: 2025-08-08</div>
<div><b>Time</b>: 17:25:56</div>
<div><b>Sampling</b>: max_tokens=1024, temperature=0.7</div>
</div>

<h2>Reward</h2>
<table>
<tr><th>mean</th><th>std</th><th>n</th><th>p5</th><th>p25</th><th>p50</th><th>p75</th><th>p95</th></tr>
<tr>
<td>0.9933</td>
<td>0.0249</td>
<td>15</td>
<td>0.97</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
</table>


<h2>Metrics</h2>
<table>
<tr>
<th>metric</th><th>mean</th><th>std</th><th>n</th><th>p5</th><th>p25</th><th>p50</th><th>p75</th><th>p95</th>
</tr>

<tr>
<td>correct_answer_reward_func</td>
<td>1.0</td>
<td>0.0</td>
<td>15</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>

<tr>
<td>num_turns</td>
<td>1.6</td>
<td>0.4899</td>
<td>15</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
<td>2.0</td>
<td>2.0</td>
</tr>

<tr>
<td>num_tool_calls</td>
<td>0.7333</td>
<td>0.6799</td>
<td>15</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>

<tr>
<td>num_errors</td>
<td>0.0667</td>
<td>0.2494</td>
<td>15</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.3</td>
</tr>

</table>


<h2>Examples <span class="muted">(showing up to 15 of 15)</span></h2>
<div class="examples">
<table>
<tr><th>#</th><th>reward</th><th>correct_answer_reward_func</th><th>completion</th></tr>

<tr>
<td>0</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>The values of \(a\) and \(b\) that make the piecewise function continuous are \(a = -3\) and \(b = 3\). The sum \(a + b\) is:

\[
a + b = -3 + 3 = 0
\]

So, \(a + b = 0\).</pre></td>
</tr>

<tr>
<td>1</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>Let&#39;s denote:
- \( r \) = the original number of rows,
- \( m \) = the original number of members in each row,
- \( N \) = the total number of band members.

From the problem, we have the following conditions:

1. The total number of band members is less than 100:
   \[
   N &lt; 100
   \]

2. When ar…</pre></td>
</tr>

<tr>
<td>2</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>The polynomial given is:

\[ 4 + 5x^3 + 100 + 2\pi x^4 + \sqrt{10} x^4 + 9 \]

Let&#39;s identify the degree by analyzing the powers of \(x\) in each term.

- \(4\) is a constant term (degree 0).
- \(5x^3\) is a term with degree 3.
- \(100\) is a constant term (degree 0).
- \(2\pi x^4\) is a term with …</pre></td>
</tr>

<tr>
<td>3</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>The value of \(\left\lceil3\left(6-\frac12\right)\right\rceil\) is 17.</pre></td>
</tr>

<tr>
<td>4</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>Sam did not work for 6 days.</pre></td>
</tr>

<tr>
<td>5</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>The values of \( a \) and \( b \) that make the function continuous are:
\[
a = -3, \quad b = 3.
\]

Therefore,
\[
a + b = -3 + 3 = 0.
\]

The value of \( a + b \) is \( \boxed{0} \).</pre></td>
</tr>

<tr>
<td>6</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>Let&#39;s denote:

- \( m \) = number of members in each row in the original formation,
- \( r \) = number of rows in the original formation,
- \( N \) = total number of band members.

From the problem:

1. \( N &lt; 100 \).
2. The original formation is \( m \times r \) but with 2 members left over, so:
 …</pre></td>
</tr>

<tr>
<td>7</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>The polynomial given is:

\[4 + 5x^3 + 100 + 2\pi x^4 + \sqrt{10} x^4 + 9\]

Let&#39;s identify the degree of each term:
- \(4\) is a constant term (degree 0).
- \(5x^3\) has degree 3.
- \(100\) is a constant term (degree 0).
- \(2\pi x^4\) has degree 4.
- \(\sqrt{10} x^4\) has degree 4.
- \(9\) is a c…</pre></td>
</tr>

<tr>
<td>8</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>The value of \(\left\lceil3\left(6-\frac12\right)\right\rceil\) is 17.</pre></td>
</tr>

<tr>
<td>9</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>Sam did not work for 6 days.</pre></td>
</tr>

<tr>
<td>10</td>
<td>0.9</td>
<td>1.0</td>
<td><pre>From the first calculation, we have the values:
\[
a = -3, \quad b = 3
\]

Now, let&#39;s find \( a + b \):
\[
a + b = -3 + 3 = 0
\]

So, the value of \( a + b \) is 0.</pre></td>
</tr>

<tr>
<td>11</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>Let the number of members in each of the \( r \) rows be \( m \). Then the total number of band members \( N \) satisfies:
\[
N = mr + 2
\]
since there are 2 members left over when arranged in \( r \) rows of \( m \) members.

If the director increases the number of members per row by 1 (i.e., \( m…</pre></td>
</tr>

<tr>
<td>12</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>The polynomial is:

\[4 + 5x^3 + 100 + 2\pi x^4 + \sqrt{10} x^4 + 9\]

Let&#39;s identify the degree by the highest power of \(x\) with a nonzero coefficient.

The terms and their degrees are:
- \(4\) (degree 0)
- \(5x^3\) (degree 3)
- \(100\) (degree 0)
- \(2\pi x^4\) (degree 4)
- \(\sqrt{10} x^4\) (d…</pre></td>
</tr>

<tr>
<td>13</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>The value of \(\left\lceil3\left(6-\frac{1}{2}\right)\right\rceil\) is \(\boxed{17}\).</pre></td>
</tr>

<tr>
<td>14</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>Sam did not work for 6 days.</pre></td>
</tr>

</table>
</div>
</details>
<details><summary>vf-math-python--v0.1.0--model=gpt-4.1-mini--n=3--r=2--args=noargs</summary>
<p><a href="reports/vf-math-python--v0.1.0--model=gpt-4.1-mini--n=3--r=2--args=noargs.html" target="_blank">Open full report</a></p>
<h3>vf-math-python: gpt-4.1-mini (n=3, r=2)</h3>
<div class="meta">
<div><b>Environment</b>: vf-math-python (v0.1.0)</div>
<div><b>Model</b>: <span class="code">gpt-4.1-mini</span></div>
<div><b>Provider</b>: https://api.openai.com/v1</div>
<div><b>Samples</b>: n=3, r=2</div>
<div><b>Date</b>: 2025-08-08</div>
<div><b>Time</b>: 16:57:08</div>
<div><b>Sampling</b>: max_tokens=1024, temperature=0.7</div>
</div>

<h2>Reward</h2>
<table>
<tr><th>mean</th><th>std</th><th>n</th><th>p5</th><th>p25</th><th>p50</th><th>p75</th><th>p95</th></tr>
<tr>
<td>0.8167</td>
<td>0.367</td>
<td>6</td>
<td>0.225</td>
<td>0.925</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
</table>


<h2>Metrics</h2>
<table>
<tr>
<th>metric</th><th>mean</th><th>std</th><th>n</th><th>p5</th><th>p25</th><th>p50</th><th>p75</th><th>p95</th>
</tr>

<tr>
<td>correct_answer_reward_func</td>
<td>0.8333</td>
<td>0.3727</td>
<td>6</td>
<td>0.25</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>

<tr>
<td>num_turns</td>
<td>1.5</td>
<td>0.7638</td>
<td>6</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.75</td>
<td>2.75</td>
</tr>

<tr>
<td>num_tool_calls</td>
<td>1.0</td>
<td>1.5275</td>
<td>6</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.5</td>
<td>3.5</td>
</tr>

<tr>
<td>num_errors</td>
<td>0.1667</td>
<td>0.3727</td>
<td>6</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.75</td>
</tr>

</table>


<h2>Examples <span class="muted">(showing up to 6 of 6)</span></h2>
<div class="examples">
<table>
<tr><th>#</th><th>reward</th><th>correct_answer_reward_func</th><th>completion</th></tr>

<tr>
<td>0</td>
<td>0.9</td>
<td>1.0</td>
<td><pre>The solution for the system of equations is:
\[
a = -3, \quad b = 3.
\]

Now, let&#39;s find the sum \( a + b \):
\[
a + b = -3 + 3 = 0.
\]

Therefore, \( a + b = 0 \).</pre></td>
</tr>

<tr>
<td>1</td>
<td>0.0</td>
<td>0.0</td>
<td><pre>Let&#39;s define the variables:
- \( m \) = number of band members in each row initially,
- \( r \) = number of rows initially,
- \( N \) = total number of band members in the band.

From the problem:
1. The band has less than 100 members, so \( N &lt; 100 \).
2. Initially, the band is arranged in a recta…</pre></td>
</tr>

<tr>
<td>2</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>The polynomial given is:

\[
4 + 5x^3 + 100 + 2\pi x^4 + \sqrt{10} x^4 + 9
\]

To find the degree of the polynomial, we need to identify the term with the highest power of \(x\).

The terms are:
- \(4\) (constant term, degree 0)
- \(5x^3\) (degree 3)
- \(100\) (constant term, degree 0)
- \(2\pi x^4…</pre></td>
</tr>

<tr>
<td>3</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>The solutions to the continuity equations are \(a = -3\) and \(b = 3\). Therefore, the sum \(a + b = -3 + 3 = 0\).

So, \(a + b = 0\).</pre></td>
</tr>

<tr>
<td>4</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>Let&#39;s denote:
- \( m \) = number of band members in each row in the original formation,
- \( r \) = number of rows in the original formation,
- \( N \) = total number of band members in the band.

From the problem:
1. The band has less than 100 members, so \( N &lt; 100 \).
2. When arranged in the ori…</pre></td>
</tr>

<tr>
<td>5</td>
<td>1.0</td>
<td>1.0</td>
<td><pre>The polynomial is \( 4 + 5x^3 + 100 + 2\pi x^4 + \sqrt{10} x^4 + 9 \).

To find the degree, we look for the highest power of \( x \) with a nonzero coefficient.

- \(4\), \(100\), and \(9\) are constants (degree 0).
- \(5x^3\) has degree 3.
- \(2\pi x^4\) has degree 4.
- \(\sqrt{10} x^4\) has degre…</pre></td>
</tr>

</table>
</div>
</details>
</details>
<!-- vf:end:reports -->



================================================
FILE: environments/math_python/math_python.py
================================================
from math_verify import parse, verify

import verifiers as vf
from verifiers.utils.data_utils import extract_boxed_answer, load_example_dataset
from verifiers.utils.tools import python


def load_environment(
    dataset_name: str = "math",
    dataset_split: str = "train",
    num_train_examples: int = -1,
    **kwargs,
):
    dataset = load_example_dataset(dataset_name, dataset_split, n=num_train_examples)
    system_prompt = "Use python for all calculations (variables do not persist). Give your answer inside \\boxed{}."

    parser = vf.Parser(extract_fn=extract_boxed_answer)

    def correct_answer_reward_func(parser, completion, answer) -> float:
        completion_answer = parser.parse_answer(completion)
        parsed_completion_answer = parse(completion_answer, parsing_timeout=0)
        parsed_ground_truth_answer = parse(answer, parsing_timeout=0)
        if verify(
            parsed_completion_answer, parsed_ground_truth_answer, timeout_seconds=0
        ):
            return 1.0
        else:
            return 0.0

    def num_turns(parser, completion) -> float:
        num_assistant_messages = len(parser.get_assistant_messages(completion))
        return float(num_assistant_messages)

    def num_tool_calls(parser, completion) -> float:
        num_tool_calls = len(parser.get_tool_messages(completion))
        return float(num_tool_calls)

    def num_errors(parser, completion) -> float:
        num_errors = sum(
            [
                1.0
                for msg in parser.get_tool_messages(completion)
                if "error" in msg["content"].lower()
            ]
        )
        return float(num_errors)

    rubric = vf.Rubric(
        funcs=[correct_answer_reward_func, num_turns, num_tool_calls, num_errors],
        weights=[1.0, 0.0, 0.0, -0.1],
        parser=parser,
    )

    vf_env = vf.ToolEnv(
        dataset=dataset,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
        tools=[python],
        max_turns=3,
        **kwargs,
    )

    return vf_env



================================================
FILE: environments/math_python/pyproject.toml
================================================
[project]
name = "math-python"
description = "Solve math problems using Python"
tags = ["math-python", "multi-turn", "code-execution", "tool-use", "train", "eval"]
version = "0.1.0"
requires-python = ">=3.11"
dependencies = [
    "verifiers>=0.1.2.post0",
    "math-verify>=0.8.0"
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["math_python.py"]



================================================
FILE: environments/mmmu/README.md
================================================
# mmmu

> Replace the placeholders below, then remove this callout. Keep the Evaluation Reports section at the bottom intact so reports can auto-render.

### Overview
- **Environment ID**: `mmmu`
- **Short description**: <one-sentence description>
- **Tags**: <comma-separated tags>

### Datasets
- **Primary dataset(s)**: <name(s) and brief description>
- **Source links**: <links>
- **Split sizes**: <train/eval counts>

### Task
- **Type**: <single-turn | multi-turn | tool use>
- **Parser**: <e.g., ThinkParser, XMLParser, custom>
- **Rubric overview**: <briefly list reward functions and key metrics>

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval mmmu
```

Configure model and sampling:

```bash
uv run vf-eval mmmu   -m gpt-4.1-mini   -n 20 -r 3 -t 1024 -T 0.7   -a '{"key": "value"}'  # env-specific args as JSON
```

Notes:
- Use `-a` / `--env-args` to pass environment-specific configuration as a JSON object.

### Environment Arguments
Document any supported environment arguments and their meaning. Example:

| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `foo` | str | `"bar"` | What this controls |
| `max_examples` | int | `-1` | Limit on dataset size (use -1 for all) |

### Metrics
Summarize key metrics your rubric emits and how they’re interpreted.

| Metric | Meaning |
| ------ | ------- |
| `reward` | Main scalar reward (weighted sum of criteria) |
| `accuracy` | Exact match on target answer |




================================================
FILE: environments/mmmu/mmmu.py
================================================
import ast
import base64
from io import BytesIO

from datasets import Dataset, concatenate_datasets, load_dataset

import verifiers as vf
from verifiers.utils.data_utils import extract_boxed_answer

ALL_SUBSETS = [
    "Accounting",
    "Agriculture",
    "Architecture_and_Engineering",
    "Art",
    "Art_Theory",
    "Basic_Medical_Science",
    "Biology",
    "Chemistry",
    "Clinical_Medicine",
    "Computer_Science",
    "Design",
    "Diagnostics_and_Laboratory_Medicine",
    "Economics",
    "Electronics",
    "Energy_and_Power",
    "Finance",
    "Geography",
    "History",
    "Literature",
    "Management",
    "Marketing",
    "Materials",
    "Math",
    "Mechanical_Engineering",
    "Music",
    "Pharmacy",
    "Physics",
    "Psychology",
    "Public_Health",
    "Sociology",
]

SPLITS = ["dev", "validation", "test"]

MC_MAP = {
    0: "A",
    1: "B",
    2: "C",
    3: "D",
}


def format_prompt(example: dict):
    pil_image = example["image_1"]
    buffer = BytesIO()
    pil_image.save(buffer, format="PNG")
    b64_img = base64.b64encode(buffer.getvalue()).decode("utf-8")
    txt = example["question"] + "\n\n"
    options = ast.literal_eval(example["options"])
    assert len(options) == 4
    for i, o in enumerate(options):
        txt += f"{MC_MAP[i]}. {o}\n"
    txt += "\nThink step-by-step and give the letter of your final answer inside \\boxed{}."

    prompt = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": str(txt)},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{b64_img}"},
                },
            ],
        }
    ]
    return prompt


def load_environment(
    subset: str | None = "Art", split: str = "dev", **kwargs
) -> vf.Environment:
    """
    Loads a custom environment.
    """
    tasks = []
    if subset is None:
        # use all subsets
        tasks = ALL_SUBSETS
    elif subset not in ALL_SUBSETS:
        raise ValueError(f"Invalid subset: {subset}")
    else:
        tasks.append(subset)

    task_datasets = []
    for task in tasks:
        # load MMMU dataset
        task_dataset = load_dataset("MMMU/MMMU", task, split=split).map(
            lambda x: {"prompt": format_prompt(x), "answer": x["answer"]}
        )
        assert isinstance(task_dataset, Dataset)
        cols = task_dataset.column_names
        cols_to_remove = [col for col in cols if col not in ["prompt", "answer"]]
        task_dataset = task_dataset.remove_columns(cols_to_remove)
        task_datasets.append(task_dataset)

    dataset = concatenate_datasets(task_datasets)

    parser = vf.Parser(extract_fn=extract_boxed_answer)

    def correct_answer(parser, completion, answer) -> float:
        parsed_answer = parser.parse_answer(completion) or ""
        return 1.0 if parsed_answer == answer else 0.0

    rubric = vf.Rubric(funcs=[correct_answer], parser=parser)

    vf_env = vf.SingleTurnEnv(
        dataset=dataset,
        parser=parser,
        rubric=rubric,
    )
    return vf_env



================================================
FILE: environments/mmmu/pyproject.toml
================================================
[project]
name = "mmmu"
description = "Multimodal benchmark environment"
tags = ["multimodal", "eval"]
version = "0.1.0"
requires-python = ">=3.11"
dependencies = [
    "verifiers>=0.1.3",
    "pillow>=11.3.0",
    "datasets"
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["mmmu.py"]



================================================
FILE: environments/reasoning_gym_env/README.md
================================================
# reasoning-gym-env

### Overview
- **Environment ID**: `reasoning-gym-env`
- **Short description**: Single-turn evaluation over `reasoning_gym` procedural tasks with XML formatting.
- **Tags**: reasoning, procedural, single-turn, xml, synthetic

### Datasets
- **Primary dataset(s)**: Generated via `reasoning_gym` (e.g., `arc_1d`, or composite configs)
- **Source links**: `reasoning_gym` library
- **Split sizes**: Configurable counts for train/eval via loader args

### Task
- **Type**: single-turn
- **Parser**: `XMLParser(["think","answer"])`
- **Rubric overview**: Score computed via `reasoning_gym` task-specific scorer; optional format component

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval reasoning-gym-env
```

Configure model and sampling:

```bash
uv run vf-eval reasoning-gym-env \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"gym": "arc_1d", "num_train_examples": 2000, "num_eval_examples": 2000}'
```

Notes:
- Use `gym` to select a single dataset name, a list of names, or a composite specification.
- Reports are written under `./environments/reasoning_gym_env/reports/` and auto-embedded below.

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `gym` | str | `"arc_1d"` | Single task name, list of names, or composite config |
| `num_train_examples` | int | `2000` | Number of training examples |
| `num_eval_examples` | int | `2000` | Number of evaluation examples |
| `seed` | int | `0` | Random seed for dataset generation |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | Task-specific score from `reasoning_gym` for parsed answer |
| `format_reward` | Adherence to `<think>`/`<answer>` XML format |



================================================
FILE: environments/reasoning_gym_env/pyproject.toml
================================================
[project]
name = "reasoning-gym-env"
description = "ReasoningGym suite of programmatically-generated reasoning tasks"
tags = ["reasoning-gym", "logic", "puzzles", "math", "train"]
version = "0.1.1"
requires-python = ">=3.11"
dependencies = [
    "verifiers>=0.1.2.post0",
    "reasoning-gym",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["reasoning_gym_env.py"]



================================================
FILE: environments/reasoning_gym_env/reasoning_gym_env.py
================================================
from typing import List, Tuple

import reasoning_gym as rg
from datasets import Dataset
from reasoning_gym.composite import DatasetSpec
from reasoning_gym.dataset import ProceduralDataset

from verifiers.envs.singleturn_env import SingleTurnEnv
from verifiers.parsers.xml_parser import XMLParser
from verifiers.rubrics.rubric import Rubric


class ReasoningGymEnv(SingleTurnEnv):
    def __init__(
        self,
        gym: str | List[str | dict],
        num_train_examples: int = 1000,
        num_eval_examples: int = 100,
        seed: int = 0,
        **kwargs,
    ):
        self.gym = gym
        self.num_train_examples = num_train_examples
        self.num_eval_examples = num_eval_examples
        self.seed = seed
        total_examples = num_train_examples + num_eval_examples
        self.rg_dataset = self.build_rg_dataset(gym, total_examples, seed=seed)
        dataset, eval_dataset = self.rg_to_hf(self.rg_dataset)
        parser = XMLParser(fields=["think", "answer"])
        rubric = Rubric(parser=parser)

        def check_answer_reward_func(completion, answer, **kwargs) -> float:
            entry = self.rg_dataset[answer]
            response = str(parser.parse_answer(completion)).strip()
            reward = self.rg_dataset.score_answer(answer=response, entry=entry)
            return reward

        rubric.add_reward_func(check_answer_reward_func)
        rubric.add_reward_func(parser.get_format_reward_func(), weight=0.2)
        system_prompt = rg.utils.SYSTEM_PROMPTS["DeepSeekZero"]  # type: ignore
        super().__init__(
            dataset=dataset,
            eval_dataset=eval_dataset,
            system_prompt=system_prompt,
            parser=parser,
            rubric=rubric,
            message_type="chat",
            **kwargs,
        )
        self.parser = parser
        self.rubric = rubric

    def build_rg_dataset(
        self, gym: str | List[str | dict], total_examples: int = 1000, seed: int = 0
    ) -> ProceduralDataset:
        if isinstance(gym, str):
            return rg.create_dataset(gym, size=total_examples, seed=seed)
        dataset_specs = []
        for dataset_config in gym:
            if isinstance(dataset_config, str):
                dataset_specs.append(
                    DatasetSpec(name=dataset_config, weight=1.0, config={})
                )
            elif isinstance(dataset_config, dict):
                dataset_specs.append(DatasetSpec(**dataset_config))
            else:
                raise ValueError(f"Invalid dataset config: {dataset_config}")
        return rg.create_dataset(
            "composite", datasets=dataset_specs, size=total_examples, seed=seed
        )

    def rg_to_hf(self, rg_dataset: ProceduralDataset) -> Tuple[Dataset, Dataset]:
        train_dataset_rows = []
        eval_dataset_rows = []
        for i, x in enumerate(rg_dataset):
            row = {
                "question": x["question"],
                "answer": i,
                "task": x["metadata"]["source_dataset"],
            }
            if i < self.num_train_examples:
                train_dataset_rows.append(row)
            else:
                eval_dataset_rows.append(row)
        dataset = Dataset.from_list(train_dataset_rows)
        eval_dataset = Dataset.from_list(eval_dataset_rows)
        return dataset, eval_dataset


def load_environment(
    gym: str | List[str | dict] = "arc_1d",
    num_train_examples: int = 2000,
    num_eval_examples: int = 2000,
    **kwargs,
):
    vf_env = ReasoningGymEnv(
        gym=gym,
        num_train_examples=num_train_examples,
        num_eval_examples=num_eval_examples,
        **kwargs,
    )
    return vf_env



================================================
FILE: environments/reverse_text/README.md
================================================
# reverse-text

### Overview
- **Environment ID**: `reverse-text`
- **Short description**: Reverse a given paragraph; evaluated by LCS similarity to the exact reversal.
- **Tags**: text, transformation, single-turn, xml

### Datasets
- **Primary dataset(s)**: `agentlans/wikipedia-paragraphs` mapped to question/answer pairs
- **Source links**: Hugging Face Datasets
- **Split sizes**: Train/eval split controlled by `num_train_examples` and `num_eval_examples`

### Task
- **Type**: single-turn
- **Parser**: `XMLParser(["think","answer"])`
- **Rubric overview**: LCS similarity between parsed answer and ground-truth reversed text; optional format check

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval reverse-text
```

Configure model and sampling:

```bash
uv run vf-eval reverse-text \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"num_train_examples": 2000, "num_eval_examples": 200}'
```

Notes:
- Use `-a` / `--env-args` to pass environment-specific configuration as a JSON object.

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `num_train_examples` | int | `2000` | Number of training examples |
| `num_eval_examples` | int | `200` | Number of evaluation examples |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | LCS similarity between reversed text and parsed answer |
| `format_reward` | Adherence to `<think>`/`<answer>` XML format |



================================================
FILE: environments/reverse_text/pyproject.toml
================================================
[project]
name = "reverse-text"
version = "0.1.0"
description = "Reverse a given text"
tags = ["reverse-text", "single-turn", "train", "eval"]
requires-python = ">=3.11"
dependencies = [
    "verifiers>=0.1.2.post0",
    "datasets",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["reverse_text.py"]



================================================
FILE: environments/reverse_text/reverse_text.py
================================================
from datasets import load_dataset

import verifiers as vf


def load_environment(num_train_examples=2000, num_eval_examples=200, **kwargs):
    dataset = load_dataset("agentlans/wikipedia-paragraphs", split="train").map(
        lambda x: {"question": x["text"], "answer": x["text"][::-1]}
    )
    train_dataset = dataset.select(range(num_train_examples))  # type: ignore
    eval_dataset = dataset.select(  # type: ignore
        range(num_train_examples, num_train_examples + num_eval_examples)
    )

    parser = vf.XMLParser(["think", "answer"], answer_field="answer")
    system_prompt = f"""Reverse the given text.

    Respond in the following format:
    {parser.get_format_str()}"""

    def lcs_reward_func(completion, answer, **kwargs) -> float:
        """
        LCS ratio of the reversed prompt and the parsed completion.
        """

        def lcs_ratio(x: str, y: str) -> float:
            """
            Return the longest common subsequence ratio of x and y.
            """
            from difflib import SequenceMatcher

            return SequenceMatcher(None, x, y).ratio()

        response = parser.parse_answer(completion) or ""
        return lcs_ratio(response, answer)

    rubric = vf.Rubric(
        funcs=[
            lcs_reward_func,
            parser.get_format_reward_func(),
        ],
        weights=[1.0, 0.2],
    )

    vf_env = vf.SingleTurnEnv(
        dataset=train_dataset,  # type: ignore
        eval_dataset=eval_dataset,  # type: ignore
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
    )
    return vf_env



================================================
FILE: environments/self_reward/README.md
================================================
# self-reward

### Overview
- **Environment ID**: `self-reward`
- **Short description**: Single-turn evaluation where a judge model scores responses based on a simple scoring prompt.
- **Tags**: judge, single-turn, self-reward, openai-compatible

### Datasets
- **Primary dataset(s)**: Any HF dataset with `question`/`answer` columns (specified by `dataset_name`)
- **Source links**: Hugging Face Datasets
- **Split sizes**: Uses the dataset’s `train` file by default

### Task
- **Type**: single-turn
- **Parser**: default `Parser`
- **Rubric overview**: `JudgeRubric` uses a judge client/model/prompt to produce a 0–1 score

### Quickstart
Run an evaluation with default settings (example):

```bash
uv run vf-eval self-reward -a '{"dataset_name": "your/dataset", "model_name": "Qwen/Qwen3-0.6B"}'
```

Configure model and sampling:

```bash
uv run vf-eval self-reward \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"dataset_name": "your/dataset", "model_name": "Qwen/Qwen3-0.6B", "base_url": "http://0.0.0.0:8000/v1", "api_key_var": "JUDGE_API_KEY"}'
```

Notes:
- Use `-a` / `--env-args` to pass environment-specific configuration as a JSON object.
- Reports are written under `./environments/self_reward/reports/` and auto-embedded below.

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `dataset_name` | str | — | HF dataset name or path containing `question`/`answer` |
| `model_name` | str | — | Judge model name (OpenAI-compatible) |
| `base_url` | str | `"http://0.0.0.0:8000/v1"` | Judge API base URL |
| `api_key_var` | str | `"JUDGE_API_KEY"` | Env var containing judge API key |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | Judge-produced score, normalized to 0–1 |

## Evaluation Reports

<!-- Do not edit below this line. Content is auto-generated. -->
<!-- vf:begin:reports -->
<p>No reports found. Run <code>uv run vf-eval self-reward -a '{"key": "value"}'</code> to generate one.</p>
<!-- vf:end:reports -->



================================================
FILE: environments/self_reward/pyproject.toml
================================================
[project]
name = "self-reward"
version = "0.1.1"
description = "Environment models self-rewarding their own responses"
tags = ["self-reward", "single-turn", "llm-judge", "knowledge", "train"]
dependencies = [
    "verifiers>=0.1.2.post1",
    "datasets",
    "openai",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["self_reward.py"]



================================================
FILE: environments/self_reward/self_reward.py
================================================
import os

from datasets import load_dataset
from openai import OpenAI

import verifiers as vf


def load_environment(
    dataset_name: str,
    model_name: str,
    base_url: str = "http://0.0.0.0:8000/v1",
    api_key_var: str = "JUDGE_API_KEY",
):
    judge_prompt = "Q: {question}\nA: {answer}\nGiven: {response}\nRespond with a score between 0.0 and 1.0."
    rubric = vf.JudgeRubric(
        judge_client=OpenAI(base_url=base_url, api_key=os.getenv(api_key_var, "EMPTY")),
        judge_model=model_name,
        judge_prompt=judge_prompt,
    )
    vf_env = vf.SingleTurnEnv(
        dataset=load_dataset(
            dataset_name, data_files="train"
        ),  # HF dataset with "question" and "answer" columns
        system_prompt="You are a helpful assistant.",
        rubric=rubric,
    )

    return vf_env



================================================
FILE: environments/sentence_repeater/README.md
================================================
# sentence-repeater

### Overview
- **Environment ID**: `sentence-repeater`
- **Short description**: Multi-turn QA over a paragraph to retrieve specific sentences in random order.
- **Tags**: retrieval, multi-turn, text, similarity

### Datasets
- **Primary dataset(s)**: `agentlans/wikipedia-paragraphs` processed into 5 Q/A turns per paragraph
- **Source links**: Hugging Face Datasets
- **Split sizes**: Train split filtered to paragraphs with 5 adequate-length sentences

### Task
- **Type**: multi-turn
- **Parser**: default `Parser`
- **Rubric overview**: Sequence similarity against the 5 target answers across turns

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval sentence-repeater
```

Configure model and sampling:

```bash
uv run vf-eval sentence-repeater \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7
```


### Environment Arguments
This loader does not expose custom arguments.

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | Mean SequenceMatcher ratio across the 5 repeated sentences |



================================================
FILE: environments/sentence_repeater/pyproject.toml
================================================
[project]
name = "sentence-repeater"
description = "Interactively repeat sentences from a paragraph in a random order"
version = "0.1.0"
tags = ["sentence-repeater", "multi-turn", "train", "eval"]
requires-python = ">=3.11"
dependencies = [
    "verifiers>=0.1.2.post0",
    "datasets",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["sentence_repeater.py"]



================================================
FILE: environments/sentence_repeater/sentence_repeater.py
================================================
import random
from copy import deepcopy
from difflib import SequenceMatcher
from typing import List, Tuple

from datasets import Dataset, load_dataset

import verifiers as vf
from verifiers.types import (
    Messages,
    State,
)


def get_sentences(paragraph: str) -> List[str]:
    """Get first sentences of paragraph (up to 5)."""
    sentences = paragraph.replace('." ', '". ').split(". ")
    return sentences[:5]


def filter_sentences(sentences: List[str]) -> bool:
    """Filter paragraphs by number of sentences and sentence length."""
    if len(sentences) < 5:
        return False
    for sentence in sentences:
        if len(sentence) < 10 or len(sentence) > 500:
            return False
    return True


index = [("first", 0), ("second", 1), ("third", 2), ("fourth", 3), ("fifth", 4)]


def get_sentence_questions(x):
    sentences = x["sentences"]
    shuffled_index = deepcopy(index)
    random.shuffle(shuffled_index)
    questions_answers = [
        (
            f"What's the {shuffled_index[0][0]} sentence of the paragraph?",
            f'The {shuffled_index[0][0]} sentence of the paragraph is: "{sentences[shuffled_index[0][1]]}"',
        ),
        (
            f"OK how about the {shuffled_index[1][0]} sentence?",
            f'The {shuffled_index[1][0]} sentence of the paragraph is: "{sentences[shuffled_index[1][1]]}"',
        ),
        (
            f"And the {shuffled_index[2][0]} sentence?",
            f'The {shuffled_index[2][0]} sentence of the paragraph is: "{sentences[shuffled_index[2][1]]}"',
        ),
        (
            f"What's the {shuffled_index[3][0]} sentence?",
            f"The {shuffled_index[3][0]} sentence of the paragraph is: {sentences[shuffled_index[3][1]]}",
        ),
        (
            f"OK last one. What's the {shuffled_index[4][0]} sentence?",
            f"The {shuffled_index[4][0]} sentence of the paragraph is: {sentences[shuffled_index[4][1]]}",
        ),
    ]
    x["task"] = "sentence-repeater"
    x["info"] = {}
    x["info"]["questions"] = [q for q, _ in questions_answers]
    x["info"]["answers"] = [a for _, a in questions_answers]
    paragraph = ". ".join(sentences)
    x["prompt"] = [
        {
            "role": "user",
            "content": f'Answer questions about the following paragraph:\n\n"{paragraph}"\n\n{questions_answers[0][0]}',
        }
    ]
    return x


class SentenceRepeaterEnv(vf.MultiTurnEnv):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def is_completed(self, messages: Messages, state: State, **kwargs) -> bool:
        return state["turn"] >= len(state["info"]["questions"])

    def env_response(
        self, messages: Messages, state: State, **kwargs
    ) -> Tuple[Messages, State]:
        return [
            {
                "role": "user",
                "content": state["info"]["questions"][state["turn"]],
            }
        ], state


def load_environment(**kwargs) -> vf.Environment:
    dataset: Dataset = load_dataset("agentlans/wikipedia-paragraphs", split="train")  # type: ignore
    dataset = dataset.map(lambda x: {"sentences": get_sentences(x["text"])})
    dataset = dataset.filter(lambda x: len(x["sentences"]) == 5)
    dataset = dataset.map(get_sentence_questions)
    parser = vf.Parser()

    def compare_answers_reward_func(parser, completion, info, **kwargs) -> float:
        assert isinstance(completion, list)
        model_answers = [
            m["content"] for m in parser.get_assistant_messages(completion)
        ]
        answers = info["answers"]
        reward = 0
        if len(model_answers) != len(answers):
            return 0

        def similarity(a, b):
            return SequenceMatcher(None, a, b).ratio()

        for model_answer, answer in zip(model_answers[: len(answers)], answers):
            reward += similarity(model_answer, answer) / len(answers)
        return reward

    rubric = vf.Rubric(parser=parser, funcs=[compare_answers_reward_func])

    vf_env = SentenceRepeaterEnv(
        dataset=dataset, parser=parser, rubric=rubric, **kwargs
    )

    return vf_env



================================================
FILE: environments/simpleqa/README.md
================================================
# simpleqa

### Overview
- **Environment ID**: `simpleqa`
- **Short description**: Single-turn factual QA judged by a configurable LLM rubric with CORRECT/INCORRECT/NOT_ATTEMPTED labels.
- **Tags**: qa, judge, single-turn, openai-compatible

### Datasets
- **Primary dataset(s)**: `basicv8vc/SimpleQA` (test split)
- **Source links**: Hugging Face Datasets
- **Split sizes**: Uses the `test` split for evaluation

### Task
- **Type**: single-turn
- **Parser**: default `Parser` (judge-based scoring)
- **Rubric overview**: `JudgeRubric` asks a judge model to label A/B/C, then maps to binary reward

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval simpleqa
```

Configure model and sampling (judge config shown):

```bash
uv run vf-eval simpleqa \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"judge_model": "gpt-4.1-mini", "judge_base_url": "https://api.openai.com/v1", "judge_api_key_var": "OPENAI_API_KEY"}'
```

Notes:
- Use `-a` / `--env-args` to configure the judge model/provider.
- Reports are written under `./environments/simpleqa/reports/` and auto-embedded below.

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `judge_model` | str | `"gpt-4.1-mini"` | Judge model name |
| `judge_base_url` | str | — | Judge provider base URL |
| `judge_api_key_var` | str | — | Env var containing judge API key |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | 1.0 if judge returns A (correct), else 0.0 |

## Evaluation Reports

<!-- Do not edit below this line. Content is auto-generated. -->
<!-- vf:begin:reports -->
<p>No reports found. Run <code>uv run vf-eval vf-simpleqa -a '{"key": "value"}'</code> to generate one.</p>
<!-- vf:end:reports -->



================================================
FILE: environments/simpleqa/pyproject.toml
================================================
[project]
name = "simpleqa"
version = "0.1.1"
description = "SimpleQA eval"
tags = ["simpleqa", "single-turn", "llm-judge", "knowledge"]
dependencies = [
    "verifiers>=0.1.2.post0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["simpleqa.py"]



================================================
FILE: environments/simpleqa/simpleqa.py
================================================
import os
import re

from datasets import load_dataset
from openai import OpenAI

import verifiers as vf


def load_environment(
    judge_model: str = "gpt-4.1-mini",
    judge_base_url: str | None = None,
    judge_api_key_var: str | None = None,
) -> vf.Environment:
    """
    Adapted from: https://github.com/openai/simple-evals/blob/main/simpleqa_eval.py
    """
    eval_dataset = load_dataset("basicv8vc/SimpleQA", split="test").map(
        lambda x: {
            "question": x["problem"],
            "answer": x["answer"],
            "task": "simpleqa",
        }
    )

    JUDGE_TEMPLATE = """\
Your job is to look at a question, a gold target, and a predicted answer, and then assign a grade of either ["CORRECT", "INCORRECT", "NOT_ATTEMPTED"].
First, I will give examples of each grade, and then you will grade a new example.


The following are examples of CORRECT predicted answers.
```
Question: What are the names of Barack Obama's children?
Gold target: Malia Obama and Sasha Obama
Predicted answer 1: sasha and malia obama
Predicted answer 2: most people would say Malia and Sasha, but I'm not sure and would have to double check
Predicted answer 3: Barack Obama has two daughters. Their names are Malia Ann and Natasha Marian, but they are commonly referred to as Malia Obama and Sasha Obama. Malia was born on July 4, 1998, and Sasha was born on June 10, 2001.
```
These predicted answers are all CORRECT because:
    - They fully contain the important information in the gold target.
    - They do not contain any information that contradicts the gold target.
    - Only semantic meaning matters; capitalization, punctuation, grammar, and order don't matter.
    - Hedging and guessing are permissible, provided that the gold target is fully included and the response contains no incorrect information or contradictions.


The following are examples of INCORRECT predicted answers.
```
Question: What are the names of Barack Obama's children?
Gold target: Malia and Sasha
Predicted answer 1: Malia.
Predicted answer 2: Malia, Sasha, and Susan.
Predicted answer 3: Barack Obama does not have any children.
Predicted answer 4: I think it's either Malia and Sasha. Or it could be Malia and Jackie. Or it could be Joey and Malia.
Predicted answer 4: While I don't know their exact names, I can tell you that Barack Obama has three children.
Predicted answer 5: It's possible you may mean Betsy and Olivia. However, you should clarify further details with updated references if necessary. Is that the correct answer?
Predicted answer 6: It may be the case that Obama's child is named James. However, it's recommended to confirm the most accurate and updated information since this could change over time. This model may not always reflect the most current information.
```
These predicted answers are all INCORRECT because:
    - A factual statement in the answer contradicts the gold target. Incorrect statements that have some hedging (e.g., "it is possible that", "although i'm not sure, i think") are also considered incorrect.


The following are examples of NOT_ATTEMPTED predicted answers.
```
Question: What are the names of Barack Obama's children?
Gold target: Malia and Sasha
Predicted answer 1: I don't know.
Predicted answer 2: I need more context about which Obama you are talking about.
Predicted answer 3: Without researching the web, I cannot answer this question. However, I can tell you that Barack Obama has two children.
Predicted answer 4: Barack Obama has two children. I know that one of them is Malia, but I'm not sure about the other one.
```
These predicted answers are all NOT_ATTEMPTED because:
    - The important information in the gold target is not included in the answer.
    - No statements in the answer contradict the gold target.


Also note the following things:
- For grading questions where the gold target is a number, the predicted answer needs to be correct to the last significant figure in the gold answer. For example, consider a question "How many citations does the Transformer Paper have?" with gold target "120k".
    - Predicted answers "120k", "124k", and 115k" are all CORRECT.
    - Predicted answers "100k" and "113k" are INCORRECT.
    - Predicted answers "around 100k" and "more than 50k" are considered NOT_ATTEMPTED because they neither confirm nor contradict the gold target.
- The gold target may contain more information than the question. In such cases, the predicted answer only needs to contain the information that is in the question.
    - For example, consider the question "What episode did Derek and Meredith get legally married in Grey's Anatomy?" with gold target "Season 7, Episode 20: White Wedding". Either "Season 7, Episode 20" or "White Wedding" would be considered a CORRECT answer.
- Do not punish predicted answers if they omit information that would be clearly inferred from the question.
    - For example, consider the question "What city is OpenAI headquartered in?" and the gold target "San Francisco, California". The predicted answer "San Francisco" would be considered CORRECT, even though it does not include "California".
    - Consider the question "What award did A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity win at NAACL '24?", the gold target is "Outstanding Paper Award". The predicted answer "Outstanding Paper" would be considered CORRECT, because "award" is presumed in the question.
    - For the question "What is the height of Jason Wei in meters?", the gold target is "1.73 m". The predicted answer "1.75" would be considered CORRECT, because meters is specified in the question.
    - For the question "What is the name of Barack Obama's wife?", the gold target is "Michelle Obama". The predicted answer "Michelle" would be considered CORRECT, because the last name can be presumed.
- Do not punish for typos in people's name if it's clearly the same name.
    - For example, if the gold target is "Hyung Won Chung", you can consider the following predicted answers as correct: "Hyoong Won Choong", "Hyungwon Chung", or "Hyun Won Chung".


Here is a new example. Simply reply with either CORRECT, INCORRECT, NOT ATTEMPTED. Don't apologize or correct yourself if there was a mistake; we are just trying to grade the answer.
```
Question: {question}
Gold target: {answer}
Predicted answer: {response}
```

Grade the predicted answer of this new question as one of:
A: CORRECT
B: INCORRECT
C: NOT_ATTEMPTED

Just return the letters "A", "B", or "C", with no text around it.
""".strip()  # noqa: E501
    api_key = os.getenv(judge_api_key_var) if judge_api_key_var else None
    judge_client = OpenAI(base_url=judge_base_url, api_key=api_key)

    rubric = vf.JudgeRubric(
        judge_client=judge_client,
        judge_model=judge_model,
        judge_prompt=JUDGE_TEMPLATE,
    )

    def correct_answer_reward_func(
        prompt, completion, answer, state, **kwargs
    ) -> float:
        judge_response = rubric.judge(prompt, completion, answer, state, **kwargs)
        match = re.search(r"(A|B|C)", judge_response)
        result = match.group(0) if match else "C"
        return 1.0 if result == "A" else 0.0

    def incorrect_answer_reward_func(
        prompt, completion, answer, state, **kwargs
    ) -> float:
        judge_response = rubric.judge(prompt, completion, answer, state, **kwargs)
        match = re.search(r"(A|B|C)", judge_response)
        result = match.group(0) if match else "C"
        return 1.0 if result == "B" else 0.0

    def not_attempted_answer_reward_func(
        prompt, completion, answer, state, **kwargs
    ) -> float:
        judge_response = rubric.judge(prompt, completion, answer, state, **kwargs)
        match = re.search(r"(A|B|C)", judge_response)
        result = match.group(0) if match else "C"
        return 1.0 if result == "C" else 0.0

    rubric.add_reward_func(correct_answer_reward_func, weight=1.0)
    rubric.add_reward_func(incorrect_answer_reward_func, weight=0.0)
    rubric.add_reward_func(not_attempted_answer_reward_func, weight=0.0)

    vf_env = vf.SingleTurnEnv(eval_dataset=eval_dataset, rubric=rubric)
    return vf_env



================================================
FILE: environments/smolagents_math_tools/README.md
================================================
# smolagents-math-tools

**Note:** This environment is not actively maintained, and is not guaranteed to work properly with the present version of `verifiers`.

### Overview
- **Environment ID**: `smolagents-math-tools`
- **Short description**: Multi-turn math tool-use environment using SmolAgents PythonInterpreter and a custom Calculator tool.
- **Tags**: math, tools, multi-turn, smolagents, xml

### Datasets
- **Primary dataset(s)**: Train from example `math`; eval from concatenated `aime2024` + `aime2025` (30 each)
- **Source links**: Uses example loader in `verifiers.utils.data_utils`
- **Split sizes**: Train≈6000; Eval≈60 (AIME 2024/2025 subsets)

### Task
- **Type**: multi-turn tool use
- **Parser**: Custom `SmolagentsParser` with fields `reasoning`, `tool`/`answer`
- **Rubric overview**: Format adherence plus tool execution success metrics; per-tool reward hooks available

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval smolagents-math-tools
```

Configure model and sampling:

```bash
uv run vf-eval smolagents-math-tools \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"use_few_shot": false}'
```

Notes:
- Use `-a` / `--env-args` to toggle few-shot examples in the system.

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `use_few_shot` | bool | `false` | Include SmolAgents few-shot examples in prompt |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `format_reward` | Adherence to `SmolagentsParser` message schema |
| `calculator_reward_func` | Tool execution success rate for calculator (if present) |
| `python_interpreter_reward_func` | Tool execution success rate for PythonInterpreter (if present) |



================================================
FILE: environments/smolagents_math_tools/pyproject.toml
================================================
[project]
name = "smolagents-math-tools"
version = "0.1.0"
dependencies = [
    "verifiers>=0.1.2",
    "smolagents>=1.15.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["smolagents_math_tools.py"]



================================================
FILE: environments/smolagents_math_tools/smolagents_math_tools.py
================================================
import json
import re
import sys
from types import SimpleNamespace
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

from datasets import Dataset, concatenate_datasets
from smolagents.default_tools import PythonInterpreterTool  # type: ignore
from smolagents.tools import Tool  # type: ignore

from verifiers.envs.multiturn_env import MultiTurnEnv
from verifiers.parsers.parser import Parser
from verifiers.parsers.xml_parser import XMLParser
from verifiers.rubrics.rubric import Rubric
from verifiers.types import Messages, RewardFunc, State
from verifiers.utils.data_utils import load_example_dataset

DEFAULT_SMOLA_PROMPT_TEMPLATE = """You are an intelligent assistant designed to solve problems that require careful reasoning.

When tackling a task, you should:
1. Break the problem down into steps
2. Reason carefully about how to solve it
3. Use available tools to help you solve the problem
4. Provide a clear final answer

Available tools:
{tool_descriptions}

Format your response using these XML tags:
<reasoning>
Think step-by-step about how to solve the task.
</reasoning>

<tool>
{{
  "name": "tool_name",
  "args": {{
    "arg1": "value1",
    "arg2": "value2"
  }}
}}
</tool>

<answer>
Your final answer or response to the user's request.
</answer>

First use the <reasoning> tag to think through the problem. When you need to use a tool, use the <tool> tag with the appropriate JSON format. When you're ready to provide the final answer, use the <answer> tag.
"""

MATH_SMOLA_PROMPT_TEMPLATE = """You are an intelligent math assistant designed to solve math problems that require careful reasoning.

When solving a math problem, you should:
1. Break the problem down into steps
2. Reason carefully through each step
3. Use the calculator tool to help with calculations
4. Provide a clear final answer in simplified form

Available tools:
{tool_descriptions}

Format your response using these XML tags:
<reasoning>
Think step-by-step about how to solve the math problem, explaining the approach clearly.
</reasoning>

<tool>
{{
  "name": "calculator", 
  "args": {{
    "expression": "math expression to calculate"
  }}
}}
</tool>

<answer>
Your final answer to the math problem, in simplified form.
</answer>

First use the <reasoning> tag to think through the problem. When you need to calculate something, use the <tool> tag with the calculator. When you're ready to provide the final answer, use the <answer> tag.
"""


"""
Few shot examples for SmolaAgents tool-based assistants.
"""

CALCULATOR_SMOLA_FEW_SHOTS = [
    {
        "role": "user",
        "content": "Janet has 3 times as many marbles as Tom. If Tom has 12 marbles, how many marbles do they have together?",
    },
    {
        "role": "assistant",
        "content": """<reasoning>
Let's solve this step by step:
1. First, calculate how many marbles Janet has (3 times Tom's)
</reasoning>

<tool>
{
  "name": "calculator", 
  "args": {
    "expression": "3 * 12"
  }
}
</tool>

<result>
36
</result>

<reasoning>
2. Now we know:
   - Tom has 12 marbles
   - Janet has 36 marbles
   Let's add them together
</reasoning>

<tool>
{
  "name": "calculator", 
  "args": {
    "expression": "36 + 12"
  }
}
</tool>

<result>
48
</result>

<reasoning>
Together they have 48 marbles:
- Tom: 12 marbles
- Janet: 36 marbles (3 times Tom's)
- Total: 48 marbles
</reasoning>

<answer>
48 marbles
</answer>""",
    },
    {
        "role": "user",
        "content": "Samantha is baking cookies. Each batch requires 2.5 cups of flour. If she has 10 cups of flour, how many complete batches can she make?",
    },
    {
        "role": "assistant",
        "content": """<reasoning>
To find how many complete batches Samantha can make, I need to divide the total amount of flour by the amount needed per batch.
</reasoning>

<tool>
{
  "name": "calculator", 
  "args": {
    "expression": "10 / 2.5"
  }
}
</tool>

<result>
4.0
</result>

<reasoning>
Samantha has 10 cups of flour and each batch requires 2.5 cups of flour.
10 ÷ 2.5 = 4
So Samantha can make 4 complete batches of cookies with her 10 cups of flour.
</reasoning>

<answer>
4 batches
</answer>""",
    },
]

SEARCH_SMOLA_FEW_SHOTS = [
    {
        "role": "user",
        "content": "Question: Which of the following is an example of mechanical weathering?\\n\\nChoices:\\nA. A rock turns red when iron minerals are exposed to oxygen.\\nB. A rock breaks apart after water freezes and expands in a crack.\\nC. A rock dissolves as acidic rainwater flows over its surface.\\nD. A rock changes composition when exposed to extreme heat.",
    },
    {
        "role": "assistant",
        "content": """<reasoning>
I need to understand what mechanical weathering is and which of these options matches that phenomenon. Let me search for information about mechanical weathering.
</reasoning>

<tool>
{
  "name": "web_search",
  "args": {
    "query": "what is mechanical weathering examples"
  }
}
</tool>

<result>
• Mechanical Weathering - Definition, Processes and Types
  Mechanical weathering, also known as physical weathering, is the process by which rocks are broken down into smaller pieces by external forces. Examples include frost wedging (when water freezes and expands in cracks), thermal expansion/contraction, and pressure release.

• Mechanical and Chemical Weathering - Earth Science
  Mechanical weathering breaks rocks into smaller pieces without changing their composition. Ice wedging occurs when water gets into cracks, freezes, and expands, breaking the rock apart. Other forms include plant root growth, animal activity, and abrasion.

• Mechanical Weathering: Definition, Types, Examples - Geology Science
  Types of mechanical weathering include frost wedging, salt crystallization, thermal expansion, plant activity, and animal activity. The most common mechanical weathering process in cold climates is frost wedging, where water enters rock cracks, freezes, and expands.
</result>

<reasoning>
Based on the search results, mechanical weathering is the process by which rocks are broken down into smaller pieces by external forces, without changing their chemical composition. Some examples include:

1. Frost wedging - when water gets into cracks, freezes, and expands, breaking the rock apart
2. Thermal expansion/contraction
3. Plant root growth
4. Animal activity
5. Abrasion

Looking at the answer choices:
A. A rock turns red when iron minerals are exposed to oxygen - This is chemical weathering, as it involves a chemical change (oxidation).
B. A rock breaks apart after water freezes and expands in a crack - This is frost wedging, a form of mechanical weathering.
C. A rock dissolves as acidic rainwater flows over its surface - This is chemical weathering (dissolution).
D. A rock changes composition when exposed to extreme heat - This is a chemical/thermal change, not mechanical weathering.

The answer is B, as it's the only option that describes mechanical weathering.
</reasoning>

<answer>
B
</answer>""",
    },
]

TOOL_SMOLA_FEW_SHOTS = [
    {"role": "user", "content": "What is the current working directory?"},
    {
        "role": "assistant",
        "content": """<reasoning>
Let's use the pwd command to find out the current working directory.
</reasoning>

<tool>
{
  "name": "python_interpreter",
  "args": {
    "code": "import os; print(os.getcwd())"
  }
}
</tool>

<result>
Stdout:
/Users/user/project
Output: None
</result>

<reasoning>
The current working directory is /Users/user/project.
</reasoning>

<answer>
/Users/user/project
</answer>""",
    },
    {
        "role": "user",
        "content": "How many Python files are in the current directory and its subdirectories?",
    },
    {
        "role": "assistant",
        "content": """<reasoning>
Let's use Python to count the number of Python files in the current directory and its subdirectories.
</reasoning>

<tool>
{
  "name": "python_interpreter",
  "args": {
    "code": "import os; count = 0; \\nfor root, dirs, files in os.walk('.'): \\n    count += sum(1 for file in files if file.endswith('.py')); \\nprint(count)"
  }
}
</tool>

<result>
Stdout:
42
Output: None
</result>

<reasoning>
There are 42 Python files in the current directory and its subdirectories.
</reasoning>

<answer>
42
</answer>""",
    },
]


class CalculatorTool(Tool):
    """A calculator tool for evaluating mathematical expressions."""

    name = "calculator"
    description = "Evaluates a single line of Python math expression. No imports or variables allowed."
    inputs = {
        "expression": {
            "type": "string",
            "description": "A mathematical expression using only numbers and basic operators (+,-,*,/,**,())",
        }
    }
    output_type = "string"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.allowed = set("0123456789+-*/.() ")
        self.is_initialized = True

    def forward(self, expression: str) -> str:  # type: ignore
        """Evaluates a single line of Python math expression. No imports or variables allowed.

        Args:
            expression: A mathematical expression using only numbers and basic operators (+,-,*,/,**,())

        Returns:
            The result of the calculation or an error message

        Examples:
            "2 + 2" -> "4"
            "3 * (17 + 4)" -> "63"
            "100 / 5" -> "20.0"
        """
        if not all(c in self.allowed for c in expression):
            return "Error: Invalid characters in expression"

        try:
            # Safely evaluate the expression with no access to builtins
            result = eval(expression, {"__builtins__": {}}, {})
            return str(result)
        except Exception as e:
            return f"Error: {str(e)}"


class SmolagentsParser(Parser):
    def __init__(self, fields: List[Union[str, Tuple[str, ...]]]):
        """
        Initialize the parser with field definitions.

        Each field may be:
          - a string (e.g. "reasoning"): the XML tag is fixed.
          - a tuple of alternatives (e.g. ("code", "answer")): the first element is
            the canonical name used for formatting, and all elements are allowed tags
            when parsing.

        The schema is assumed to have no duplicate names.
        """
        self._fields: List[
            Tuple[str, List[str]]
        ] = []  # List of (canonical, [alternatives])
        seen = set()
        for field in fields:
            if isinstance(field, str):
                canonical = field
                alternatives = [field]
            elif isinstance(field, tuple):
                if not field:
                    raise ValueError("Field tuple cannot be empty.")
                canonical = field[0]
                if not all(isinstance(alt, str) for alt in field):
                    raise TypeError("All alternatives in a tuple must be strings.")
                alternatives = list(field)
            else:
                raise TypeError("Each field must be a string or a tuple of strings.")
            if canonical in seen:
                raise ValueError(f"Duplicate field name: {canonical}")
            seen.add(canonical)
            self._fields.append((canonical, alternatives))

    def get_format_reward_func(self) -> Callable:
        """
        Return a reward function that checks if messages follow the expected format.

        The function does not make assumptions about which fields should start/end the message
        or the specific order of fields. It checks that:
        - At least one field from the schema is present in each message
        - Fields have proper content and spacing
        """

        def format_reward_func(completion, **kwargs) -> float:
            """
            Reward function that checks if each step follows the expected format.

            Args:
                completion: List[Dict[str, str]] | str
                **kwargs: additional kwargs

            Returns:
                float: Score between 0 and 1.
            """
            model_messages = self.get_assistant_messages(completion)
            if not model_messages:
                return 0.0

            # Calculate format adherence for each message
            format_scores = []
            for msg in model_messages:
                content = msg["content"]  # type: ignore
                parsed = self.parse(content)  # type: ignore
                parsed_no_strip = self.parse(content, strip=False)  # type: ignore

                # Check if the message has at least one valid field
                has_any_field = False
                fields_with_content = 0
                total_fields = 0

                # Keep track of which expected fields are present
                expected_field_count = len(
                    self._fields
                )  # Total number of expected field sets
                present_field_sets = (
                    set()
                )  # Which field sets have at least one alternative present

                # Check proper spacing for fields
                has_correct_spacing = True

                for i, (canonical, alternatives) in enumerate(self._fields):
                    field_set_present = False
                    for alt in alternatives:
                        if hasattr(parsed, alt) and getattr(parsed, alt) is not None:
                            has_any_field = True
                            fields_with_content += 1
                            total_fields += 1
                            field_set_present = True

                            # Check if field exists in non-stripped version too (proper spacing)
                            if not (
                                hasattr(parsed_no_strip, alt)
                                and getattr(parsed_no_strip, alt) is not None
                            ):
                                has_correct_spacing = False
                        elif (
                            content.count(f"<{alt}>") > 0  # type: ignore
                            or content.count(f"</{alt}>") > 0  # type: ignore
                        ):
                            # Tag exists but content wasn't properly parsed
                            total_fields += 1
                            field_set_present = True

                    # If any alternative from this field set was present, count it
                    if field_set_present:
                        present_field_sets.add(i)

                # Calculate format score components
                format_score = 0.0

                # Check if any field from the first field set starts the message
                starts_with_any_field = False
                first_field_set = self._fields[0][
                    1
                ]  # Get alternatives for first field set
                for alt in first_field_set:
                    if content.strip().startswith(f"<{alt}>"):  # type: ignore
                        starts_with_any_field = True
                        break

                # Check if any field from the last field set ends the message
                ends_with_any_field = False
                last_field_set = self._fields[-1][
                    1
                ]  # Get alternatives for last field set
                for alt in last_field_set:
                    if content.strip().endswith(f"</{alt}>"):  # type: ignore
                        ends_with_any_field = True
                        break

                # Weight the score based on different criteria
                if has_any_field:
                    # Calculate the proportion of expected field sets that are present
                    field_set_ratio = len(present_field_sets) / expected_field_count
                    format_score += 0.4 * field_set_ratio

                if has_correct_spacing:
                    format_score += 0.2

                if starts_with_any_field:
                    format_score += 0.2

                if ends_with_any_field:
                    format_score += 0.2

                format_scores.append(format_score)

            # Return average format adherence
            if not format_scores:
                return 0.0
            return sum(format_scores) / len(format_scores)

        return format_reward_func

    def get_fields(self) -> List[str]:
        """Return a list of the canonical field names (in order)."""
        return [canonical for canonical, _ in self._fields]

    def format(self, **kwargs) -> str:
        """
        Format the provided keyword arguments into an XML string.

        For fields with alternatives (tuple), the canonical name (the first element)
        is used as the XML tag. The method looks for a provided value using any of the
        allowed names (preferring the canonical if present).

        Example usage:
            parser = SmolaParser(['reasoning', ('code', 'answer')])
            formatted_str = parser.format(reasoning="...", code="...")
        """
        parts = []
        for canonical, alternatives in self._fields:
            value = None
            # Look for a provided value using any of the acceptable keys,
            # preferring the canonical name if it exists.
            if canonical in kwargs:
                value = kwargs[canonical]
            else:
                for alt in alternatives:
                    if alt in kwargs:
                        value = kwargs[alt]
                        break
            if value is None:
                raise ValueError(
                    f"Missing value for field '{canonical}' (allowed: {alternatives})"
                )
            # Use the canonical name as the tag for formatting.
            parts.append(f"<{canonical}>\n{value}\n</{canonical}>")
        return "\n".join(parts)

    def parse(self, text: str, strip: bool = True) -> Any:
        """
        Parse the given XML string and return an object with attributes corresponding
        to all allowed tags in the schema.

        For each field defined:
          - If it is a simple field (e.g. 'reasoning'), the output object will have
            an attribute 'reasoning' set to the text content (or None if missing).
          - If it is defined with alternatives (e.g. ("code", "answer")), the output
            object will have attributes for *each* allowed tag name. For example,
            if the schema is ['reasoning', ('code', 'answer')], then both
            `result.code` and `result.answer` are always accessible. If a tag is not
            found in the XML, its corresponding attribute is set to None.

        This implementation also attempts to parse tool JSON content when available.
        """
        results: Dict[str, Optional[str]] = {}
        for canonical, alternatives in self._fields:
            # For each allowed alternative tag, search independently.
            for alt in alternatives:
                # Regex pattern to capture the content between the tags.
                pattern = rf"<{alt}>\s*(.*?)\s*</{alt}>"
                match = re.search(pattern, text, re.DOTALL)
                if match:
                    content = match.group(1).strip() if strip else match.group(1)
                    # If the field contains valid JSON and it's a tool field, parse it
                    if alt == "tool" and content:
                        try:
                            # Try to parse as JSON but preserve the string for the result
                            json.loads(content)
                        except json.JSONDecodeError:
                            pass
                    results[alt] = content
                else:
                    results[alt] = None
        return SimpleNamespace(**results)


class LegacyToolRubric(Rubric):
    def __init__(
        self,
        parser: Parser = XMLParser(fields=["reasoning", ("tool", "answer")]),
        env_parser: Parser = XMLParser(fields=["result"]),
        tools: List[Callable] = [],
    ):
        super().__init__(parser=parser)
        self.parser = parser
        self.env_parser = env_parser
        self.tools = {
            tool.__name__ if hasattr(tool, "__name__") else str(tool): tool
            for tool in tools
        }
        self.reward_funcs = [
            self.correct_answer_reward_func,
            self.tool_execution_reward_func,
            self.parser.get_format_reward_func(),
        ]
        self.reward_weights = [
            1.0,
            0.2,
            0.2,
        ]
        for tool_name in self.tools.keys():
            self.reward_funcs.append(self.get_named_tool_reward_func(tool_name))
            self.reward_weights.append(0.0)
            self.reward_funcs.append(self.get_named_tool_count_reward_func(tool_name))
            self.reward_weights.append(0.0)
            self.reward_funcs.append(self.get_named_tool_attempt_reward_func(tool_name))
            self.reward_weights.append(0.0)

    def evaluate_code(self, code_str, answer, **kwargs) -> float:
        import io
        import signal
        import sys
        from contextlib import redirect_stdout

        try:
            test_cases = json.loads(answer)["test_cases"]
        except Exception:
            return 0.0
        # strip ```python and ``` if present at the beginning and end of the code
        code_str = code_str.strip()
        if code_str.startswith("```python"):
            code_str = code_str[9:]
        elif code_str.startswith("```"):
            code_str = code_str[3:]
        if code_str.endswith("```"):
            code_str = code_str[:-3]
        code_str = code_str.strip()

        def timeout_handler(signum, frame):
            raise TimeoutError("Code execution timed out")

        def normalize_output(output):
            # Normalize line endings and whitespace
            return "\n".join(line.strip() for line in output.splitlines())

        total_cases = 0
        passed = 0

        for test in test_cases:
            output = io.StringIO()
            sys.stdin = io.StringIO(test["input"])
            try:
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(10)
                with redirect_stdout(output):
                    exec(code_str)
                signal.alarm(0)
                actual = normalize_output(output.getvalue())
                expected = normalize_output(test["output"])

                # Compare each line individually
                actual_lines = actual.splitlines()
                expected_lines = expected.splitlines()
                total_cases += len(expected_lines)
                for a, e in zip(actual_lines, expected_lines):
                    if a == e:
                        passed += 1

            except Exception as e:
                sys.stdin = sys.__stdin__
                return 0.0
            sys.stdin = sys.__stdin__

        return passed / total_cases if total_cases else 0.0

    def correct_answer_reward_func(self, completion, answer, **kwargs) -> float:
        """Reward function that checks if the final answer matches the expected answer."""
        response = str(self.parser.parse_answer(completion))
        return 1.0 if answer == response else 0.0

    def tool_execution_reward_func(
        self, completion: List[Dict[str, str]], **kwargs
    ) -> float:
        """
        Reward function that checks tool execution success.

        Uses XMLParser to identify proper tool calls.
        """
        tool_attempts = 0
        successful_executions = 0

        # Find assistant messages with tools and their responses
        for i, msg in enumerate(completion):
            if msg["role"] == "assistant":
                # Use parser to check for tool tag
                parsed = self.parser.parse(msg["content"])
                if hasattr(parsed, "tool") and parsed.tool is not None:
                    # Found a properly formatted tool message
                    if i + 1 < len(completion) and completion[i + 1]["role"] == "user":
                        tool_attempts += 1
                        # Check response with env_parser
                        parsed_response = self.env_parser.parse(
                            completion[i + 1]["content"]
                        )
                        if (
                            hasattr(parsed_response, "result")
                            and parsed_response.result is not None
                            and not parsed_response.result.startswith("Error:")
                        ):
                            successful_executions += 1

        # Calculate reward
        if tool_attempts == 0:
            return 0.0
        return successful_executions / tool_attempts

    def get_named_tool_reward_func(self, tool_name: str) -> Callable:
        """
        Returns a reward function that checks tool execution success for a specific tool.

        Uses XMLParser to identify proper tool calls.
        """

        def tool_reward_func(completion: List[Dict[str, str]], **kwargs) -> float:
            """
            Reward function that checks execution success for the {tool_name} tool.

            Uses XMLParser to identify proper tool calls for the specified tool.
            """
            import json

            tool_attempts = 0
            successful_executions = 0

            # Find assistant messages with the specific tool and their responses
            for i, msg in enumerate(completion):
                if msg["role"] == "assistant":
                    # Use parser to check for tool tag
                    parsed = self.parser.parse(msg["content"])
                    if hasattr(parsed, "tool") and parsed.tool is not None:
                        try:
                            command = json.loads(parsed.tool)
                            if (
                                isinstance(command, dict)
                                and command.get("name") == tool_name
                            ):
                                # Found a properly formatted tool message for the specific tool
                                if (
                                    i + 1 < len(completion)
                                    and completion[i + 1]["role"] == "user"
                                ):
                                    tool_attempts += 1
                                    # Check response with env_parser
                                    parsed_response = self.env_parser.parse(
                                        completion[i + 1]["content"]
                                    )
                                    if (
                                        hasattr(parsed_response, "result")
                                        and parsed_response.result is not None
                                        and not parsed_response.result.startswith(
                                            "Error:"
                                        )
                                    ):
                                        successful_executions += 1
                        except json.JSONDecodeError:
                            pass

            # Calculate reward
            if tool_attempts == 0:
                return 0.0
            return successful_executions / tool_attempts

        # Create a function with the dynamic name based on tool_name
        tool_reward_func.__name__ = f"{tool_name}_reward_func"
        return tool_reward_func

    def get_named_tool_count_reward_func(self, tool_name: str) -> Callable:
        """
        Returns a reward function that counts the number of times the {tool_name} tool is used.
        """

        def tool_count_reward_func(completion: List[Dict[str, str]], **kwargs) -> float:
            """
            Reward function that counts the number of times the {tool_name} tool is used.
            """
            import json

            successful_executions = 0.0
            for i, msg in enumerate(completion):
                if msg["role"] == "assistant":
                    parsed = self.parser.parse(msg["content"])
                    if hasattr(parsed, "tool") and parsed.tool is not None:
                        try:
                            command = json.loads(parsed.tool)
                            if (
                                isinstance(command, dict)
                                and command.get("name") == tool_name
                            ):
                                # Found a properly formatted tool message for the specific tool
                                if (
                                    i + 1 < len(completion)
                                    and completion[i + 1]["role"] == "user"
                                ):
                                    parsed_response = self.env_parser.parse(
                                        completion[i + 1]["content"]
                                    )
                                    if (
                                        hasattr(parsed_response, "result")
                                        and parsed_response.result is not None
                                        and not parsed_response.result.startswith(
                                            "Error:"
                                        )
                                    ):
                                        successful_executions += 1
                        except json.JSONDecodeError:
                            pass
            return successful_executions

        tool_count_reward_func.__name__ = f"{tool_name}_count_reward_func"
        return tool_count_reward_func

    def get_named_tool_attempt_reward_func(self, tool_name: str) -> Callable:
        """
        Returns a reward function that counts the number of times the {tool_name} tool is used.
        """

        def tool_attempt_reward_func(
            completion: List[Dict[str, str]], **kwargs
        ) -> float:
            """
            Reward function that counts the number of times the {tool_name} tool is used.
            """
            import json

            attempted_executions = 0.0
            for i, msg in enumerate(completion):
                if msg["role"] == "assistant":
                    parsed = self.parser.parse(msg["content"])
                    if hasattr(parsed, "tool") and parsed.tool is not None:
                        try:
                            command = json.loads(parsed.tool)
                            if (
                                isinstance(command, dict)
                                and command.get("name") == tool_name
                            ):
                                attempted_executions += 1
                        except json.JSONDecodeError:
                            pass
            return attempted_executions

        tool_attempt_reward_func.__name__ = f"{tool_name}_attempt_reward_func"
        return tool_attempt_reward_func


class SmolagentsToolRubric(LegacyToolRubric):
    def __init__(
        self,
        parser: SmolagentsParser = SmolagentsParser(
            fields=["reasoning", ("tool", "answer")]
        ),
        env_parser: SmolagentsParser = SmolagentsParser(fields=["result"]),
        tools: List[Any] = [],
    ):
        super().__init__()
        self.parser = parser
        self.env_parser = env_parser
        self.tools = {tool.name: tool for tool in tools}
        self.reward_funcs = [
            self.parser.get_format_reward_func(),
        ]
        self.reward_weights = [
            1.0,
            0.2,
        ]
        for tool_name in self.tools.keys():
            self.add_reward_func(self.get_named_tool_reward_func(tool_name), weight=0.0)

    def evaluate_code(self, code_str, answer, **kwargs) -> float:
        import io
        import signal
        from contextlib import redirect_stdout

        try:
            test_cases = json.loads(answer)["test_cases"]
        except Exception as e:
            print(f"Error parsing test cases: {e}")
            return 0.0
        # strip ```python and ``` if present at the beginning and end of the code
        code_str = code_str.strip()
        if code_str.startswith("```python"):
            code_str = code_str[9:]
        elif code_str.startswith("```"):
            code_str = code_str[3:]
        if code_str.endswith("```"):
            code_str = code_str[:-3]
        code_str = code_str.strip()

        def timeout_handler(signum, frame):
            raise TimeoutError("Code execution timed out")

        def normalize_output(output):
            # Normalize line endings and whitespace
            return "\n".join(line.strip() for line in output.splitlines())

        total_cases = 0
        passed = 0

        for test in test_cases:
            output = io.StringIO()
            sys.stdin = io.StringIO(test["input"])
            try:
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(10)
                with redirect_stdout(output):
                    exec(code_str)
                signal.alarm(0)
                actual = normalize_output(output.getvalue())
                expected = normalize_output(test["output"])

                # Compare each line individually
                actual_lines = actual.splitlines()
                expected_lines = expected.splitlines()
                total_cases += len(expected_lines)
                for a, e in zip(actual_lines, expected_lines):
                    if a == e:
                        passed += 1

            except Exception as e:
                sys.stdin = sys.__stdin__
                return 0.0
            sys.stdin = sys.__stdin__

        return passed / total_cases if total_cases else 0.0


class SmolagentsToolEnv(MultiTurnEnv):
    def __init__(
        self,
        dataset: Dataset | None = None,
        eval_dataset: Dataset | None = None,
        tools: List[Any] = [],
        system_prompt: str = "",
        few_shot: List[Dict[str, str]] = [],
        mask_env_response: bool = True,
        max_steps: int = 10,
        **kwargs,
    ):
        # Format the system prompt with tool descriptions
        tool_descriptions = self._format_tool_descriptions(tools)
        formatted_prompt = system_prompt.format(tool_descriptions=tool_descriptions)
        super().__init__(
            dataset=dataset,
            eval_dataset=eval_dataset,
            system_prompt=formatted_prompt,
            few_shot=few_shot,
            mask_env_response=mask_env_response,
            max_steps=max_steps,
            **kwargs,
        )
        self.dataset_name = dataset
        self.max_steps = max_steps
        self.tools = {tool.name: tool for tool in tools}
        self.rubric = SmolagentsToolRubric(tools=tools)
        self.llm_parser = SmolagentsParser(fields=["reasoning", ("tool", "answer")])
        self.env_parser = SmolagentsParser(fields=["result"])

    def _format_tool_descriptions(self, tools: List[Any]) -> str:
        """Formats tool schemas into a user-friendly description string."""
        descriptions = []
        for tool in tools:
            desc = [f"{tool.name}: {tool.description}"]

            desc.append("\nArguments:")
            for arg_name, arg_info in tool.inputs.items():
                desc.append(f"  - {arg_name}: {arg_info['description']}")

            desc.append(f"\nReturns: {tool.output_type}")

            descriptions.append("\n".join(desc))

        return "\n\n".join(descriptions)

    def get_reward_funcs(self, **kwargs: Any) -> List[RewardFunc]:
        return self.rubric.get_reward_funcs()

    def get_reward_weights(self, **kwargs: Any) -> List[float]:
        return self.rubric.get_reward_weights()

    def _get_step_count(self, messages: Messages) -> int:
        """Count the number of tool uses in the message history, excluding few-shot examples."""
        assert isinstance(messages, list)
        step_count = 0

        # Skip messages that are part of few-shot examples
        # We need to determine where the actual conversation starts
        # System message + few-shot examples + user query = start of actual conversation
        conversation_start = 1  # Start after system message
        if self.few_shot:
            # Account for all few-shot messages
            conversation_start += len(self.few_shot)

        # Only count tool uses from the actual conversation
        for message in messages[conversation_start:]:
            if message.get("role") == "assistant":
                step_count += 1
        return step_count

    def is_completed(self, messages: Messages, state: State, **kwargs: Any) -> bool:
        assert isinstance(messages, list)
        try:
            # Check if we've hit max steps by counting tool uses in the message history
            step_count = self._get_step_count(messages)
            if step_count > self.max_steps:
                return True

            parsed = self.llm_parser.parse(messages[-1]["content"])  # type: ignore
            # Check if we got a valid answer field (not just None from failed parsing)
            return hasattr(parsed, "answer") and parsed.answer is not None
        except Exception:
            return False

    def call_tool(self, tool_json: str, **kwargs: Any) -> str:
        """Call a Smolagents Tool object based on JSON command."""
        try:
            command = json.loads(tool_json)
            if not isinstance(command, dict):
                return 'Error: Tool command must be a JSON object, e.g. \'{"name": "tool_name", "args": {"arg1": "value1", "arg2": "value2"}}\''

            tool_name = command.get("name")
            if not tool_name:
                return 'Error: Tool command must specify \'name\', e.g. \'{"name": "tool_name", "args": {"arg1": "value1", "arg2": "value2"}}\''

            if tool_name not in self.tools:
                return (
                    f"Error: Unknown tool '{tool_name}'. "
                    + 'Please format your tool call as \'{"name": "tool_name", "args": {"arg1": "value1", "arg2": "value2"}}\''
                )

            tool = self.tools[tool_name]
            tool_args = command.get("args", {})
            if isinstance(tool_args, str):
                return f"Error: Arguments for {tool_name} must be a JSON object matching the tool's input schema, not a string."

            # Call the tool object with arguments
            result = tool(**tool_args)
            return str(result)
        except json.JSONDecodeError:
            return 'Error: Invalid JSON format. Please format your tool call as \'{"name": "tool_name", "args": {"arg1": "value1", "arg2": "value2"}}\''
        except Exception as e:
            return (
                f"Error: {str(e)}. "
                + 'Please format your tool call as \'{"name": "tool_name", "args": {"arg1": "value1", "arg2": "value2"}}\''
            )

    def env_response(
        self, messages: Messages, state: State, **kwargs: Any
    ) -> Tuple[Messages, State]:
        assert isinstance(messages, list)
        try:
            parsed = self.llm_parser.parse(messages[-1]["content"])  # type: ignore
            # Check if we got a valid tool field (not just None from failed parsing)
            if hasattr(parsed, "tool") and parsed.tool is not None:
                result = self.call_tool(parsed.tool)
                if len(result.strip()) > 0:
                    return [
                        {
                            "role": "user",
                            "content": self.env_parser.format(result=result),
                        }
                    ], {}
                else:
                    return [
                        {
                            "role": "user",
                            "content": "Error: Tool execution returned empty output.",
                        }
                    ], {}
        except Exception:
            pass
        return [
            {
                "role": "user",
                "content": "Error: Tool command not found or invalid XML format. Please ensure correct formatting.",
            }
        ], {}


def load_environment(use_few_shot: bool = False, **kwargs):
    dataset = load_example_dataset("math", "train", n=6000)

    eval_aime24 = load_example_dataset("aime2024", n=30)
    eval_aime25 = load_example_dataset("aime2025", n=30)
    eval_dataset = concatenate_datasets([eval_aime24, eval_aime25]).shuffle(seed=0)

    # Use SmolaAgents' PythonInterpreterTool as a replacement for the python tool
    python_tool = PythonInterpreterTool(authorized_imports=["math", "sympy", "numpy"])
    # Add our custom calculator tool
    calculator_tool = CalculatorTool()

    vf_env = SmolagentsToolEnv(
        dataset=dataset,
        eval_dataset=eval_dataset,
        system_prompt=MATH_SMOLA_PROMPT_TEMPLATE,
        few_shot=CALCULATOR_SMOLA_FEW_SHOTS if use_few_shot else [],
        tools=[python_tool, calculator_tool],
        max_steps=5,
        **kwargs,
    )

    return vf_env



================================================
FILE: environments/summarize_text/README.md
================================================
# summarize-text

### Overview
- **Environment ID**: `summarize-text`
- **Short description**: Summarize a paragraph into three sentences using a specified XML response format.
- **Tags**: summarization, single-turn, xml, lcs

### Datasets
- **Primary dataset(s)**: `agentlans/wikipedia-paragraphs` mapped to `question`=`text`, `answer`=`text`
- **Source links**: Hugging Face Datasets
- **Split sizes**: Uses the `train` split for evaluation

### Task
- **Type**: single-turn
- **Parser**: `XMLParser(["think","answer"])`
- **Rubric overview**: (1) Exactly 3 sentences; (2) LCS similarity to source; (3) format check

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval summarize-text
```

Configure model and sampling:

```bash
uv run vf-eval summarize-text \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7
```

Notes:
- Reports are written under `./environments/summarize_text/reports/` and auto-embedded below.

### Environment Arguments
This loader does not expose custom arguments.

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `sentence_reward_func` | 1.0 if exactly three sentences, else 0.0 |
| `lcs_reward_func` | LCS similarity between source and parsed summary |
| `format_reward` | Adherence to `<think>`/`<answer>` XML format |

## Evaluation Reports

<!-- Do not edit below this line. Content is auto-generated. -->
<!-- vf:begin:reports -->
<p>No reports found. Run <code>uv run vf-eval vf-summarize-text -a '{"key": "value"}'</code> to generate one.</p>
<!-- vf:end:reports -->



================================================
FILE: environments/summarize_text/pyproject.toml
================================================
[project]
name = "summarize-text"
version = "0.1.0"
dependencies = [
    "verifiers>=0.1.2",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["summarize_text.py"]



================================================
FILE: environments/summarize_text/summarize_text.py
================================================
from datasets import load_dataset

import verifiers as vf


def load_environment(**kwargs) -> vf.Environment:
    dataset = load_dataset("agentlans/wikipedia-paragraphs", split="train")
    dataset = dataset.map(lambda x: {"question": x["text"], "answer": x["text"]})

    parser = vf.XMLParser(["think", "answer"], answer_field="answer")
    system_prompt = f"""Respond in the following format:
    {parser.get_format_str()}

    Summarize the given text in 3 sentences."""

    def sentence_reward_func(completion, **kwargs) -> float:
        """
        Count the number of sentences in the completion.
        """
        response = parser.parse_answer(completion) or ""
        return 1.0 if len(response.split(".")) == 3 else 0.0

    def lcs_reward_func(completion, answer, **kwargs) -> float:
        """
        LCS ratio of the prompt and the parsed completion.
        """

        def lcs_ratio(x: str, y: str) -> float:
            """
            Return the longest common subsequence ratio of x and y.
            """
            from difflib import SequenceMatcher

            return SequenceMatcher(None, x, y).ratio()

        response = parser.parse_answer(completion) or ""
        return lcs_ratio(response, answer)

    rubric = vf.Rubric(
        funcs=[
            sentence_reward_func,
            lcs_reward_func,
            parser.get_format_reward_func(),
        ],
        weights=[1.0, 0.2, 0.2],
    )

    vf_env = vf.SingleTurnEnv(
        eval_dataset=dataset,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
        max_concurrent=10,
    )
    return vf_env



================================================
FILE: environments/tool_test/README.md
================================================
# tool-test

### Overview
- **Environment ID**: `tool-test`
- **Short description**: Sanity-check tool-calling environment that asks models to invoke a random subset of dummy tools.
- **Tags**: tools, single-turn, function-calling, sanity

### Datasets
- **Primary dataset(s)**: Synthetic HF dataset generated in-memory with prompts specifying required tools
- **Source links**: N/A (programmatically generated)
- **Split sizes**: Controlled by `num_train_examples` and `num_eval_examples`

### Task
- **Type**: tool use (single-turn ToolEnv)
- **Parser**: default `Parser`
- **Rubric overview**: ToolRubric checks tool execution and adds exact match on the required tool set

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval tool-test
```

Configure model and sampling:

```bash
uv run vf-eval tool-test \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"num_train_examples": 1000, "num_eval_examples": 100}'
```


### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `num_train_examples` | int | `1000` | Number of training examples |
| `num_eval_examples` | int | `100` | Number of evaluation examples |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | 1.0 if called tool set equals required set, else 0.0 |
| ToolRubric metrics | Tool execution success and format adherence |



================================================
FILE: environments/tool_test/pyproject.toml
================================================
[project]
name = "tool-test"
description = "Test environment for tool use"
tags = ["tool-test", "single-turn", "tool-use", "train", "example", "debug"]
version = "0.1.1"
requires-python = ">=3.11,<3.13"
dependencies = [
    "verifiers>=0.1.2.post0",
    "datasets",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["tool_test.py"]



================================================
FILE: environments/tool_test/tool_test.py
================================================
import random

from datasets import Dataset

import verifiers as vf


# dummy tools for sanity checking parallel tool calls
def tool_A(x: int) -> int:
    """
    Tool for adding 1 to an integer.

    Args:
        x: The integer to add 1 to.

    Returns:
        The integer plus 1.
    """
    return x + 1


def tool_B(x: str) -> str:
    """
    Tool for concatenating a string with "2".

    Args:
        x: The string to concatenate with "2".

    Returns:
        The string concatenated with "2".
    """
    return x + "2"


def tool_C(x: float) -> float:
    """
    Tool for adding 3.0 to a float.

    Args:
        x: The float to add 3.0 to.

    Returns:
        The float plus 3.0.
    """
    return x + 3.0


def tool_D(x: bool) -> bool:
    """
    Tool for negating a boolean.

    Args:
        x: The boolean to negate.

    Returns:
        The negated boolean.
    """
    return not x


tool_list = [tool_A, tool_B, tool_C, tool_D]
tool_name_list = [tool.__name__ for tool in tool_list]


def tool_call_reward_func(completion, info):
    # check if completion tool calls exactly matches info tool calls
    tool_calls = completion[-1].get("tool_calls", [])
    called_tool_names = sorted([call.function.name for call in tool_calls])
    expected_tool_names = sorted(info["tool_names"])
    if called_tool_names == expected_tool_names:
        return 1.0
    else:
        return 0.0


def load_environment(
    num_train_examples: int = 1000, num_eval_examples: int = 100
) -> vf.ToolEnv:
    """
    Loads a custom environment.
    """

    # for each row, choose random non-empty subset of tools
    # prompt: call the following tools
    # completion: call the following tools

    # for each row, choose random non-empty subset of tools
    # prompt: call the following tools
    # completion: call the following tools
    train_rows = []
    eval_rows = []
    for i in range(num_train_examples + num_eval_examples):
        tool_names = random.sample(
            tool_name_list, random.randint(1, len(tool_name_list))
        )

        prompt = [
            {
                "role": "user",
                "content": f"Call the following tools with arguments of your choice: {tool_names}",
            }
        ]
        info = {"tool_names": tool_names}
        if i < num_train_examples:
            train_rows.append({"prompt": prompt, "info": info})
        else:
            eval_rows.append({"prompt": prompt, "info": info})

    dataset = Dataset.from_list(train_rows)
    eval_dataset = Dataset.from_list(eval_rows)
    parser = vf.Parser()
    rubric = vf.ToolRubric(tools=tool_list)
    rubric.add_reward_func(tool_call_reward_func)
    vf_env = vf.ToolEnv(
        dataset=dataset,
        eval_dataset=eval_dataset,
        parser=parser,
        rubric=rubric,
        tools=tool_list,
        max_turns=1,
    )
    return vf_env



================================================
FILE: environments/toxicity_explanation/README.md
================================================
# toxicity-explanation

### Overview
- **Environment ID**: `toxicity-explanation`
- **Short description**: Judge-based evaluation for toxicity classification with explanations using Civil Comments.
- **Tags**: toxicity, classification, explanation, judge, single-turn

### Datasets
- **Primary dataset(s)**: `google/civil_comments` mapped to toxicity targets and metadata
- **Source links**: Hugging Face Datasets
- **Split sizes**: Train split; size optionally limited via `max_examples`

### Task
- **Type**: single-turn
- **Parser**: default `Parser`
- **Rubric overview**: `JudgeRubric` with a numeric (0–10) rubric normalized to 0–1; evaluates correctness and explanation quality

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval toxicity-explanation
```

Configure model and sampling:

```bash
uv run vf-eval toxicity-explanation \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"judge_model": "gpt-4.1-mini", "judge_base_url": "https://api.openai.com/v1", "judge_api_key_var": "OPENAI_API_KEY", "max_examples": -1}'
```

Notes:
- Use `-a` / `--env-args` to configure the judge model/provider and dataset size.

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `judge_model` | str | `"gpt-4.1-mini"` | Judge model name |
| `judge_base_url` | str | `"https://api.openai.com/v1"` | Judge provider base URL |
| `judge_api_key_var` | str | `"OPENAI_API_KEY"` | Env var containing judge API key |
| `max_examples` | int | `-1` | If > 0, limit dataset to this many examples |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | Normalized judge score (0–1) |



================================================
FILE: environments/toxicity_explanation/pyproject.toml
================================================
[project]
name = "toxicity-explanation"
version = "0.1.0"
description = "Explain why a given text is toxic"
tags = ["toxicity-explanation", "llm-judge", "single-turn", "train", "eval"]
requires-python = ">=3.11"
dependencies = [
    "datasets",
    "openai",
    "verifiers>=0.1.2.post0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["toxicity_explanation.py"]



================================================
FILE: environments/toxicity_explanation/toxicity_explanation.py
================================================
import os

from datasets import load_dataset
from openai import OpenAI

import verifiers as vf


def load_environment(
    judge_model: str = "gpt-4.1-mini",
    judge_base_url: str = "https://api.openai.com/v1",
    judge_api_key_var: str = "OPENAI_API_KEY",
    max_examples: int = -1,
    **kwargs,
) -> vf.Environment:
    """
    Loads the toxicity explanation environment.

    This environment tests a model's ability to:
    1. Classify text as toxic or non-toxic
    2. Provide context-aware explanations for the classification

    Uses JudgeRubric to evaluate both accuracy and explanation quality.

    Args:
        max_examples: Maximum number of examples to load (-1 for all examples)
    """

    # Load the Civil Comments toxicity dataset
    dataset = load_dataset("google/civil_comments", split="train")

    # Limit dataset size if requested
    if max_examples > 0:
        dataset = dataset.select(range(max_examples))  # type: ignore

    # Transform the dataset to the expected format
    def transform_example(example):
        # Civil Comments uses continuous scores from 0 to 1
        # We'll consider a comment toxic if the toxicity score is >= 0.5
        is_toxic = example["toxicity"] >= 0.5

        # Create toxicity details based on scores above threshold
        toxicity_details = []
        threshold = 0.5

        if example.get("severe_toxicity", 0) >= threshold:
            toxicity_details.append("severely toxic")
        if example.get("obscene", 0) >= threshold:
            toxicity_details.append("obscene")
        if example.get("threat", 0) >= threshold:
            toxicity_details.append("threatening")
        if example.get("insult", 0) >= threshold:
            toxicity_details.append("insulting")
        if example.get("identity_attack", 0) >= threshold:
            toxicity_details.append("identity attack")

        # Store all relevant info in the info dict
        return {
            "question": f"Analyze the following text for toxicity and explain your reasoning:\n\n{example['text']}",
            "answer": "toxic" if is_toxic else "non-toxic",  # For JudgeRubric
            "info": {
                "is_toxic": is_toxic,
                "categories": toxicity_details if toxicity_details else ["non-toxic"],
                "text": example["text"],
                "label": "toxic" if is_toxic else "non-toxic",
            },
        }

    dataset = dataset.map(transform_example, remove_columns=dataset.column_names)  # type: ignore

    # System prompt for the model
    system_prompt = """You are a content moderation assistant. For each text, you must:

1. Classify whether the text is toxic or non-toxic
2. Explain your reasoning with specific examples from the text
3. Consider context and intent, not just individual words

Respond in the following format:
<classification>toxic/non-toxic</classification>
<explanation>Your detailed explanation here</explanation>"""

    # Parser to extract classification and explanation
    parser = vf.Parser()

    # Judge client
    judge_client = OpenAI(api_key=os.getenv(judge_api_key_var), base_url=judge_base_url)

    # Create a comprehensive evaluation prompt
    judge_prompt = """Evaluate this toxicity classification task.

Text to analyze: {question}

Expected classification: {answer}

Model's response: {response}

Provide a score from 0 to 10 where:
- 0-3: Incorrect classification or poor explanation
- 4-6: Correct classification but weak explanation
- 7-8: Correct classification with good explanation
- 9-10: Correct classification with excellent, nuanced explanation

Consider:
1. Classification accuracy (is it correctly identified as {answer}?)
2. Explanation quality (does it cite specific examples?)
3. Contextual understanding (does it consider intent and context?)

Respond with ONLY a single number from 0 to 10."""

    # Create the JudgeRubric
    rubric = vf.JudgeRubric(
        judge_client=judge_client,
        judge_model=judge_model,
        judge_prompt=judge_prompt,
        parser=parser,
    )

    # Add a reward function that calls the judge method and extracts the score
    def comprehensive_evaluation(prompt, completion, answer, state, **kwargs) -> float:
        # Call the judge method to get the judge's response
        judge_response = rubric.judge(prompt, completion, answer, state, **kwargs)

        # Extract the numeric score from the judge response
        try:
            # Try to find a number in the response
            import re

            numbers = re.findall(r"\b([0-9]|10)\b", judge_response)
            if numbers:
                score = float(numbers[0]) / 10.0  # Normalize to 0-1
                return max(0.0, min(1.0, score))  # Clamp to [0, 1]
        except Exception:
            pass
        return 0.0

    rubric.add_reward_func(comprehensive_evaluation, weight=1.0)

    # Create the environment
    env = vf.SingleTurnEnv(
        dataset=dataset,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
        **kwargs,
    )

    return env



================================================
FILE: environments/wiki_search/README.md
================================================
# wiki-search

### Overview
- **Environment ID**: `wiki-search`
- **Short description**: Multi-turn tool-use QA over a small Wikipedia corpus using ChromaDB and OpenAI embeddings, with judge-based scoring.
- **Tags**: retrieval, tools, multi-turn, embeddings, judge

### Datasets
- **Primary dataset(s)**: `willcb/wiki-trivia-questions` (HF) and a local wiki markdown corpus indexed in ChromaDB
- **Source links**: Hugging Face Datasets, ChromaDB
- **Split sizes**: Uses the `train` split for prompts; corpus is on-disk

### Task
- **Type**: multi-turn tool use
- **Parser**: XML-like tool formatting inside `<tool>` blocks; final answer in `<answer>`
- **Rubric overview**: Combines the default tool rubric with a `JudgeRubric` for answer quality

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval wiki-search
```

Configure model and sampling:

```bash
uv run vf-eval wiki-search \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"judge_model": "gpt-4.1-mini", "judge_base_url": "https://api.openai.com/v1", "judge_api_key_var": "OPENAI_API_KEY", "embed_model": "text-embedding-3-small", "embed_base_url": "https://api.openai.com/v1", "embed_api_key_var": "OPENAI_API_KEY"}'
```

Notes:
- Requires a prebuilt ChromaDB at `notebooks/.chroma_db` and markdown pages under `notebooks/data/wiki`.
- Reports are written under `./environments/wiki_search/reports/` and auto-embedded below.

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `judge_model` | str | `"gpt-4.1-mini"` | Judge model name |
| `judge_base_url` | str | `"https://api.openai.com/v1"` | Judge provider base URL |
| `judge_api_key_var` | str | `"OPENAI_API_KEY"` | Env var for judge API key |
| `embed_model` | str | `"text-embedding-3-small"` | Embedding model name |
| `embed_base_url` | str | `"https://api.openai.com/v1"` | Embedding provider base URL |
| `embed_api_key_var` | str | `"OPENAI_API_KEY"` | Env var for embed API key |
| `wiki_dir` | str | `notebooks/data/wiki` | Path to markdown wiki pages |
| `chroma_db_dir` | str | `notebooks/.chroma_db` | Path to ChromaDB index |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| ToolRubric metrics | Tool execution success and format adherence |
| JudgeRubric metrics | Judge-scored answer quality |




================================================
FILE: environments/wiki_search/pyproject.toml
================================================
[project]
name = "wiki-search"
description = "Agentic RAG over local wiki pages for trivia Q&A"
tags = ["wiki-search", "multi-turn", "agentic-search", "rag", "train", "eval"]
requires-python = ">=3.11"
version = "0.1.2"
dependencies = [
    "verifiers>=0.1.3",
    "chromadb",
    "datasets",
    "openai"
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["wiki_search.py"]



================================================
FILE: environments/wiki_search/wiki_search.py
================================================
import os

import chromadb  # type: ignore
from chromadb.utils import embedding_functions  # type: ignore
from datasets import load_dataset
from openai import OpenAI

import verifiers as vf
from verifiers.rubrics.judge_rubric import JudgeRubric

WIKI_DIR = "data/wiki"
CHROMA_DB_DIR = ".chroma_db"


def load_environment(
    max_turns: int = 10,
    judge_model: str = "gpt-4.1-mini",
    judge_base_url: str = "https://api.openai.com/v1",
    judge_api_key_var: str = "OPENAI_API_KEY",
    embed_model: str = "text-embedding-3-small",
    embed_base_url: str = "https://api.openai.com/v1",
    embed_api_key_var: str = "OPENAI_API_KEY",
    wiki_dir: str = WIKI_DIR,
    chroma_db_dir: str = CHROMA_DB_DIR,
) -> vf.Environment:
    openai_ef = embedding_functions.OpenAIEmbeddingFunction(
        api_key=os.getenv(embed_api_key_var, "EMPTY"),
        model_name=embed_model,
        base_url=embed_base_url,
    )
    db_client = chromadb.PersistentClient(path=chroma_db_dir)
    collection = db_client.get_collection("wiki_titles", embedding_function=openai_ef)  # type: ignore

    def search_pages(query: str) -> list[dict]:
        """Search for top 10 relevant articles using title embedding similarity.

        Args:
            query (str): The query to search for.

        Returns:
            list[dict]: A list of dicts with page_id and title.

        Examples:
            "basketball" -> [{"page_id": "basketball", "title": "Basketball"}, {"page_id": "basketball_rules", "title": "Basketball Rules"}, ...]
        """
        results = collection.query(query_texts=[query], n_results=10)

        # Format results
        output = []
        for i in range(len(results["ids"][0])):
            output.append(
                {
                    "page_id": results["ids"][0][i],
                    "title": results["metadatas"][0][i]["title"],  # type: ignore
                }
            )

        return output

    def view_sections(page_id: str) -> list[dict]:
        """View the sections of a page.

        Args:
            page_id (str): The ID of the page to view.

        Returns:
            list[dict]: A list of dicts with section_id and section_name.

        Examples:
            "basketball" -> [{"section_id": "basketball:history", "section_name": "History"}, ...]
        """
        # Find the file for this page_id
        results = collection.get(ids=[page_id])
        if not results["ids"]:
            raise ValueError(f"Page not found: {page_id}")

        filename = results["metadatas"][0]["title"] + ".md"  # type: ignore
        filepath = os.path.join(wiki_dir, filename)  # type: ignore

        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()

        sections = []

        lines = content.split("\n")
        for i, line in enumerate(lines):
            if line.startswith("#"):
                # Extract section name (remove # and whitespace)
                section_name = line.lstrip("#").strip()
                # Create section ID
                section_id = f"{page_id}:{section_name.lower().replace(' ', '_')}"
                sections.append(
                    {
                        "section_id": section_id,
                        "section_name": section_name,
                        "start_line": i,
                    }
                )

        # If no sections found, return the whole page as one section
        if not sections:
            sections.append(
                {
                    "section_id": f"{page_id}:full",
                    "section_name": "Full Page",
                    "start_line": 0,
                }
            )

        return [
            {"section_id": s["section_id"], "section_name": s["section_name"]}
            for s in sections
        ]

    def read_section(section_id: str) -> str:
        """Read a section of a page.

        Args:
            section_id (str): The ID of the section to read.

        Returns:
            str: The content of the section.

        Examples:
            "baseball:finnish_baseball" -> "Finnish baseball is a sport that is played in Finland..."
        """
        # Parse section_id
        if ":" not in section_id:
            raise ValueError(
                "Invalid section_id format. Expected: page_id:section_name"
            )

        page_id, section_name_id = section_id.split(":", 1)

        # Get the file
        results = collection.get(ids=[page_id])
        if not results["ids"]:
            raise ValueError(f"Page not found: {page_id}")

        filename = results["metadatas"][0]["title"] + ".md"  # type: ignore
        filepath = os.path.join(wiki_dir, filename)

        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()

        lines = content.split("\n")

        # Special case for "full" section
        if section_name_id == "full":
            return content

        # Find the section
        section_start = None
        section_end = None

        for i, line in enumerate(lines):
            if line.startswith("#"):
                current_section = line.lstrip("#").strip().lower().replace(" ", "_")
                if current_section == section_name_id and section_start is None:
                    section_start = i
                elif section_start is not None and section_end is None:
                    section_end = i
                    break

        # If section found
        if section_start is not None:
            if section_end is None:
                section_end = len(lines)
            return "\n".join(lines[section_start:section_end])
        else:
            raise ValueError(f"Section not found: {section_id}")

    tools = [
        search_pages,
        view_sections,
        read_section,
    ]

    dataset = load_dataset("willcb/wiki-trivia-questions", split="train")

    parser = vf.ThinkParser()
    vf_env = vf.ToolEnv(
        dataset=dataset,
        parser=parser,
        tools=tools,
        max_turns=max_turns,
    )

    judge_client = OpenAI(base_url=judge_base_url, api_key=os.getenv(judge_api_key_var))
    judge_rubric = JudgeRubric(
        judge_client=judge_client, judge_model=judge_model, parser=vf_env.parser
    )

    async def judge_reward_func(judge, prompt, completion, answer, state) -> float:
        judge_response = await judge(prompt, completion, answer, state)
        if "yes" in judge_response.lower():
            return 1.0
        else:
            return 0.0

    judge_rubric.add_reward_func(judge_reward_func, weight=1.0)
    vf_env.rubric = vf.RubricGroup(rubrics=[judge_rubric, vf_env.rubric])

    return vf_env



================================================
FILE: environments/wordle/README.md
================================================
# wordle

### Overview
- **Environment ID**: `wordle`
- **Short description**: Multi-turn Wordle game environment with optional `<think>` reasoning; rewards correctness, partial credit, turns, and format.
- **Tags**: games, multi-turn, wordle, xml, feedback

### Datasets
- **Primary dataset(s)**: TextArena `Wordle-v0` (environment provides episodes)
- **Source links**: TextArena
- **Split sizes**: Number of episodes controlled via args

### Task
- **Type**: multi-turn (game interaction)
- **Parser**: `XMLParser` with `think`/`guess` or just `guess` depending on `use_think`
- **Rubric overview**: Exact guess match, partial credit from feedback, turns-based reward, and format check

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval wordle
```

Configure model and sampling:

```bash
uv run vf-eval wordle \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"num_train_examples": 2000, "num_eval_examples": 20, "use_think": true}'
```

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `num_train_examples` | int | `2000` | Number of training episodes |
| `num_eval_examples` | int | `20` | Number of evaluation episodes |
| `use_think` | bool | `true` | Use `<think>` with `guess`; if false, guess-only format |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `check_answer_reward_func` | 1.0 if final guess equals target, else 0.0 |
| `partial_credit_reward_func` | Partial credit from greens/yellows in feedback |
| `count_turns_reward_func` | Higher score for solving in fewer turns |
| `format_reward` | Adherence to expected XML format |



================================================
FILE: environments/wordle/pyproject.toml
================================================
[project]
name = "wordle"
description = "Game environment for Wordle, built on top of TextArena"
tags = ["textarena", "multi-turn", "reasoning", "game", "train", "eval"]
version = "0.1.4"
dependencies = [
    "verifiers>=0.1.2.post0",
    "nltk",
    "textarena",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["wordle.py"]



================================================
FILE: environments/wordle/wordle.py
================================================
import verifiers as vf
from verifiers.envs.textarena_env import TextArenaEnv

### prompts
THINK_GUESS_SYSTEM_PROMPT = """You are a competitive game player. \
Make sure you read the game instructions carefully, and always follow the required format.

In each turn, think step-by-step inside <think>...</think> tags, \
then follow the instructions inside <guess>...</guess> tags."""

NOTHINK_GUESS_SYSTEM_PROMPT = """You are a competitive game player. \
Make sure you read the game instructions carefully, and always follow the required format.

In each turn, give only your guess inside <guess>...</guess> tags."""


### feedback functions
def wordle_feedback_fn(observation: str) -> str:
    if "Feedback:" in observation:
        return observation.split("Feedback:")[-1]
    else:
        return observation


### reward functions
def check_answer_reward_func(parser, completion, answer, **kwargs) -> float:
    guess = parser.parse_answer(completion)
    return 1.0 if guess == "[" + answer + "]" else 0.0


def count_turns_reward_func(parser, completion, answer, **kwargs) -> float:
    num_turns = len([x for x in completion if x["role"] == "assistant"])
    is_correct = check_answer_reward_func(parser, completion, answer, **kwargs)
    return is_correct / (num_turns + 1)


def partial_credit_reward_func(parser, completion, **kwargs) -> float:
    """Reward function that gives partial credit for the correct guess."""
    final_env_response = parser.get_user_messages(completion)[-1]["content"].strip()
    guess, scoring = final_env_response.split("\n")[:2]
    num_greens = scoring.count("G")
    num_yellows = scoring.count("Y")
    return 0.2 * num_greens + 0.1 * num_yellows


### environment loader
def load_environment(
    num_train_examples: int = 2000,
    num_eval_examples: int = 20,
    use_think: bool = True,
):
    if use_think:
        system_prompt = THINK_GUESS_SYSTEM_PROMPT
        parser = vf.XMLParser(fields=["think", "guess"], answer_field="guess")
    else:
        system_prompt = NOTHINK_GUESS_SYSTEM_PROMPT
        parser = vf.XMLParser(fields=["guess"], answer_field="guess")

    rubric = vf.Rubric(parser=parser)
    rubric.add_reward_func(check_answer_reward_func)
    rubric.add_reward_func(partial_credit_reward_func)
    rubric.add_reward_func(count_turns_reward_func)
    rubric.add_reward_func(parser.get_format_reward_func(), weight=0.2)

    vf_env = TextArenaEnv(
        game="Wordle-v0",
        num_train_examples=num_train_examples,
        num_eval_examples=num_eval_examples,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
        feedback_fn=wordle_feedback_fn,
    )
    return vf_env



================================================
FILE: environments/xlam_function_calling/README.md
================================================
# xlam-function-calling

### Overview
- **Environment ID**: `xlam-function-calling`
- **Short description**: Function-calling reproduction from XLAM-60k: model must emit a JSON array of tool calls.

### Datasets
- **Primary dataset(s)**: `Salesforce/xlam-function-calling-60k` (train split)
- **Source links**: Hugging Face Datasets
- **Split sizes**: Uses the `train` split for training/evaluation

### Task
- **Type**: single-turn
- **Parser**: `XMLParser(["think","tool"], answer_field="tool")`
- **Rubric overview**: Exact set equality between parsed JSON array and target tool list

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval xlam-function-calling
```

Configure model and sampling:

```bash
uv run vf-eval xlam-function-calling \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7
```

### Environment Arguments
This loader does not expose custom arguments.

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | 1.0 if parsed tool JSON exactly matches target, else 0.0 |




================================================
FILE: environments/xlam_function_calling/pyproject.toml
================================================
[project]
name = "xlam-function-calling"
version = "0.1.1"
description = "XLM Function Calling environment"
tags = ["salesforce-xlam", "single-turn", "function-calling", "tool-use", "train"]
requires-python = ">=3.11"
dependencies = [
    "verifiers>=0.1.2.post0",
    "datasets",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["xlam_function_calling.py"]



================================================
FILE: environments/xlam_function_calling/xlam_function_calling.py
================================================
import json

from datasets import load_dataset

import verifiers as vf


def load_environment() -> vf.Environment:
    def format_sys_prompt(tools: list[dict]) -> str:
        tool_str = json.dumps(tools, indent=2)
        return f"""You are a helpful assistant that can use tools to answer questions and perform tasks. You have the following tools:
{tool_str}

Think step-by-step inside <think>...</think> tags, then call one or more tools inside <tool>...</tool> tags \
as a JSON array with 'name' and 'arguments' keys for each tool call."""

    def process_example(x):
        prompt = [
            {"role": "system", "content": format_sys_prompt(json.loads(x["tools"]))},
            {"role": "user", "content": x["query"]},
        ]
        return {
            "prompt": prompt,
            "answer": x["answers"],
            "task": "xlam-function-calling",
        }

    dataset = load_dataset("Salesforce/xlam-function-calling-60k", split="train")
    dataset = dataset.map(process_example, remove_columns=dataset.column_names)  # type: ignore

    parser = vf.XMLParser(fields=["think", "tool"], answer_field="tool")

    def check_tools_reward_func(completion, answer) -> float:
        try:
            called_tools = json.loads(parser.parse_answer(completion) or "[]")
            target_tools = json.loads(answer)
            for called_tool in called_tools:
                if called_tool not in target_tools:
                    return 0
            for target_tool in target_tools:
                if target_tool not in called_tools:
                    return 0
            return 1
        except Exception:
            return 0

    rubric = vf.Rubric(funcs=[check_tools_reward_func])

    vf_env = vf.SingleTurnEnv(
        dataset=dataset,
        parser=parser,
        rubric=rubric,
    )
    return vf_env



================================================
FILE: environments/xml_tool_env/README.md
================================================
# xml-tool-env

### Overview
- **Environment ID**: `xml-tool-env`
- **Short description**: Multi-turn XML-formatted tool-use environment with automatic tool schema inference from Python callables.
- **Tags**: tools, multi-turn, xml, schema-inference

### Datasets
- **Primary dataset(s)**: Loaded via `load_example_dataset(dataset_name, split)`
- **Source links**: Uses example loader in `verifiers.utils.data_utils`
- **Split sizes**: Based on provided split

### Task
- **Type**: multi-turn tool use
- **Parser**: `XMLParser(["think", ("tool","answer")])` and env `XMLParser(["result"])`
- **Rubric overview**: Correctness on final answer, tool execution success, and format adherence; per-tool metrics available

### Quickstart
Run an evaluation with default settings (example):

```bash
uv run vf-eval xml-tool-env -a '{"dataset_name": "math", "split": "train"}'
```

Configure model and sampling:

```bash
uv run vf-eval xml-tool-env \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{"dataset_name": "math", "split": "train"}'
```

Notes:
- Provide `tools` as Python callables; schemas are inferred from signatures/docstrings.
- Reports are written under `./environments/xml_tool_env/reports/` and auto-embedded below.

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `dataset_name` | str | — | Example dataset name |
| `split` | str | — | Split to load |
| `tools` | List[Callable] | `[]` | Tool functions to expose |
| `system_prompt` | str | `""` | Prompt template (auto-formats with tool descriptions when `format_prompt=True`) |
| `format_prompt` | bool | `true` | Whether to insert tool descriptions into the prompt |
| `max_turns` | int | `10` | Maximum number of tool-use turns |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | Final answer correctness |
| `tool_execution_reward_func` | Success rate of tool calls with non-error results |
| `format_reward` | Adherence to `<think>` and `<tool>` XML format |
| Tool-specific metrics | Per-tool success/count/attempt rates |

## Evaluation Reports

<!-- Do not edit below this line. Content is auto-generated. -->
<!-- vf:begin:reports -->
<p>No reports found. Run <code>uv run vf-eval xml-tool-env -a '{"key": "value"}'</code> to generate one.</p>
<!-- vf:end:reports -->



================================================
FILE: environments/xml_tool_env/pyproject.toml
================================================
[project]
name = "xml-tool-env"
version = "0.1.0"
dependencies = [
    "verifiers>=0.1.2",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["xml_tool_env.py"]



================================================
FILE: environments/xml_tool_env/xml_tool_env.py
================================================
import inspect
import json
from typing import Any, Callable, Dict, List, Tuple

from verifiers import (
    Messages,
    MultiTurnEnv,
    RewardFunc,
    Rubric,
    State,
    XMLParser,
)
from verifiers.utils.data_utils import load_example_dataset


def infer_schema_from_function(func: Callable) -> Dict[str, Any]:
    """Infers a tool schema from a function's signature and docstring."""
    sig = inspect.signature(func)
    doc = inspect.getdoc(func) or ""

    # Parse docstring sections
    doc_parts = doc.split("\n\n")
    description = doc_parts[0].strip()

    # Extract examples if present
    examples = []
    return_description = ""
    for part in doc_parts:
        if part.startswith("Examples:"):
            examples = [line.strip() for line in part.split("\n")[1:] if line.strip()]
        elif part.startswith("Returns:"):
            return_description = part.split("\n")[1].strip()

    return_type = str(
        sig.return_annotation.__name__
        if sig.return_annotation != inspect.Parameter.empty
        else "any"
    )

    print(f"return_description: {return_description} ({return_type})")
    # Build args schema
    args = {}
    for name, param in sig.parameters.items():
        param_doc = ""
        for part in doc_parts:
            if part.strip().startswith("Args:"):
                for line in part.split("\n")[1:]:
                    if line.strip().startswith(f"{name}:"):
                        param_doc = line.strip()[len(name) + 1 :].strip()

        args[name] = {
            "type": str(
                param.annotation.__name__
                if param.annotation != inspect.Parameter.empty
                else "any"
            ),
            "description": param_doc,
        }
        if param.default != inspect.Parameter.empty:
            args[name]["default"] = param.default

    return {
        "name": func.__name__,
        "description": description,
        "args": args,
        "returns": return_description + f" ({return_type})",
        "examples": examples,
    }


def format_tool_descriptions(schemas: List[Dict[str, Any]]) -> str:
    """Formats tool schemas into a user-friendly description string."""
    descriptions = []
    for schema in schemas:
        desc = [f"{schema['name']}: {schema['description']}"]

        desc.append("\nArguments:")
        for arg_name, arg_info in schema["args"].items():
            default = (
                f" (default: {arg_info['default']})" if "default" in arg_info else ""
            )
            desc.append(f"  - {arg_name}: {arg_info['description']}{default}")

        if schema["examples"]:
            desc.append("\nExamples:")
            for example in schema["examples"]:
                desc.append(f"  {example}")

        if schema["returns"]:
            desc.append(f"\nReturns: {schema['returns']}")

        descriptions.append("\n".join(desc))

    return "\n\n".join(descriptions)


class XMLToolRubric(Rubric):
    def __init__(
        self,
        parser: XMLParser = XMLParser(fields=["think", ("tool", "answer")]),
        env_parser: XMLParser = XMLParser(fields=["result"]),
        tools: List[Callable] = [],
    ):
        super().__init__(parser=parser)
        self.parser = parser
        self.env_parser = env_parser
        self.tools = {
            tool.__name__ if hasattr(tool, "__name__") else str(tool): tool
            for tool in tools
        }
        self.reward_funcs = [
            self.correct_answer_reward_func,
            self.tool_execution_reward_func,
            self.parser.get_format_reward_func(),
        ]
        self.reward_weights = [
            1.0,
            0.2,
            0.2,
        ]
        for tool_name in self.tools.keys():
            self.reward_funcs.append(self.get_named_tool_reward_func(tool_name))
            self.reward_weights.append(0.0)
            self.reward_funcs.append(self.get_named_tool_count_reward_func(tool_name))
            self.reward_weights.append(0.0)
            self.reward_funcs.append(self.get_named_tool_attempt_reward_func(tool_name))
            self.reward_weights.append(0.0)

    def evaluate_code(self, code_str, answer, **kwargs) -> float:
        import io
        import signal
        import sys
        from contextlib import redirect_stdout

        try:
            test_cases = json.loads(answer)["test_cases"]
        except Exception:
            return 0.0
        # strip ```python and ``` if present at the beginning and end of the code
        code_str = code_str.strip()
        if code_str.startswith("```python"):
            code_str = code_str[9:]
        elif code_str.startswith("```"):
            code_str = code_str[3:]
        if code_str.endswith("```"):
            code_str = code_str[:-3]
        code_str = code_str.strip()

        def timeout_handler(signum, frame):
            raise TimeoutError("Code execution timed out")

        def normalize_output(output):
            # Normalize line endings and whitespace
            return "\n".join(line.strip() for line in output.splitlines())

        total_cases = 0
        passed = 0

        for test in test_cases:
            output = io.StringIO()
            sys.stdin = io.StringIO(test["input"])
            try:
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(10)
                with redirect_stdout(output):
                    exec(code_str)
                signal.alarm(0)
                actual = normalize_output(output.getvalue())
                expected = normalize_output(test["output"])

                # Compare each line individually
                actual_lines = actual.splitlines()
                expected_lines = expected.splitlines()
                total_cases += len(expected_lines)
                for a, e in zip(actual_lines, expected_lines):
                    if a == e:
                        passed += 1

            except Exception as e:
                sys.stdin = sys.__stdin__
                return 0.0
            sys.stdin = sys.__stdin__

        return passed / total_cases if total_cases else 0.0

    def correct_answer_reward_func(self, completion, answer, **kwargs) -> float:
        """Reward function that checks if the final answer matches the expected answer."""
        response = str(self.parser.parse_answer(completion))
        return 1.0 if answer == response else 0.0

    def tool_execution_reward_func(
        self, completion: List[Dict[str, str]], **kwargs
    ) -> float:
        """
        Reward function that checks tool execution success.

        Uses XMLParser to identify proper tool calls.
        """
        tool_attempts = 0
        successful_executions = 0

        # Find assistant messages with tools and their responses
        for i, msg in enumerate(completion):
            if msg["role"] == "assistant":
                # Use parser to check for tool tag
                parsed = self.parser.parse(msg["content"])
                if hasattr(parsed, "tool") and parsed.tool is not None:
                    # Found a properly formatted tool message
                    if i + 1 < len(completion) and completion[i + 1]["role"] == "user":
                        tool_attempts += 1
                        # Check response with env_parser
                        parsed_response = self.env_parser.parse(
                            completion[i + 1]["content"]
                        )
                        if (
                            hasattr(parsed_response, "result")
                            and parsed_response.result is not None
                            and not parsed_response.result.startswith("Error:")
                        ):
                            successful_executions += 1

        # Calculate reward
        if tool_attempts == 0:
            return 0.0
        return successful_executions / tool_attempts

    def get_named_tool_reward_func(self, tool_name: str) -> Callable:
        """
        Returns a reward function that checks tool execution success for a specific tool.

        Uses XMLParser to identify proper tool calls.
        """

        def tool_reward_func(completion: List[Dict[str, str]], **kwargs) -> float:
            """
            Reward function that checks execution success for the {tool_name} tool.

            Uses XMLParser to identify proper tool calls for the specified tool.
            """
            import json

            tool_attempts = 0
            successful_executions = 0

            # Find assistant messages with the specific tool and their responses
            for i, msg in enumerate(completion):
                if msg["role"] == "assistant":
                    # Use parser to check for tool tag
                    parsed = self.parser.parse(msg["content"])
                    if hasattr(parsed, "tool") and parsed.tool is not None:
                        try:
                            command = json.loads(parsed.tool)
                            if (
                                isinstance(command, dict)
                                and command.get("name") == tool_name
                            ):
                                # Found a properly formatted tool message for the specific tool
                                if (
                                    i + 1 < len(completion)
                                    and completion[i + 1]["role"] == "user"
                                ):
                                    tool_attempts += 1
                                    # Check response with env_parser
                                    parsed_response = self.env_parser.parse(
                                        completion[i + 1]["content"]
                                    )
                                    if (
                                        hasattr(parsed_response, "result")
                                        and parsed_response.result is not None
                                        and not parsed_response.result.startswith(
                                            "Error:"
                                        )
                                    ):
                                        successful_executions += 1
                        except json.JSONDecodeError:
                            pass

            # Calculate reward
            if tool_attempts == 0:
                return 0.0
            return successful_executions / tool_attempts

        # Create a function with the dynamic name based on tool_name
        tool_reward_func.__name__ = f"{tool_name}_reward_func"
        return tool_reward_func

    def get_named_tool_count_reward_func(self, tool_name: str) -> Callable:
        """
        Returns a reward function that counts the number of times the {tool_name} tool is used.
        """

        def tool_count_reward_func(completion: List[Dict[str, str]], **kwargs) -> float:
            """
            Reward function that counts the number of times the {tool_name} tool is used.
            """
            import json

            successful_executions = 0.0
            for i, msg in enumerate(completion):
                if msg["role"] == "assistant":
                    parsed = self.parser.parse(msg["content"])
                    if hasattr(parsed, "tool") and parsed.tool is not None:
                        try:
                            command = json.loads(parsed.tool)
                            if (
                                isinstance(command, dict)
                                and command.get("name") == tool_name
                            ):
                                # Found a properly formatted tool message for the specific tool
                                if (
                                    i + 1 < len(completion)
                                    and completion[i + 1]["role"] == "user"
                                ):
                                    parsed_response = self.env_parser.parse(
                                        completion[i + 1]["content"]
                                    )
                                    if (
                                        hasattr(parsed_response, "result")
                                        and parsed_response.result is not None
                                        and not parsed_response.result.startswith(
                                            "Error:"
                                        )
                                    ):
                                        successful_executions += 1
                        except json.JSONDecodeError:
                            pass
            return successful_executions

        tool_count_reward_func.__name__ = f"{tool_name}_count_reward_func"
        return tool_count_reward_func

    def get_named_tool_attempt_reward_func(self, tool_name: str) -> Callable:
        """
        Returns a reward function that counts the number of times the {tool_name} tool is used.
        """

        def tool_attempt_reward_func(
            completion: List[Dict[str, str]], **kwargs
        ) -> float:
            """
            Reward function that counts the number of times the {tool_name} tool is used.
            """
            import json

            attempted_executions = 0.0
            for i, msg in enumerate(completion):
                if msg["role"] == "assistant":
                    parsed = self.parser.parse(msg["content"])
                    if hasattr(parsed, "tool") and parsed.tool is not None:
                        try:
                            command = json.loads(parsed.tool)
                            if (
                                isinstance(command, dict)
                                and command.get("name") == tool_name
                            ):
                                attempted_executions += 1
                        except json.JSONDecodeError:
                            pass
            return attempted_executions

        tool_attempt_reward_func.__name__ = f"{tool_name}_attempt_reward_func"
        return tool_attempt_reward_func


class XMLToolEnv(MultiTurnEnv):
    def __init__(
        self,
        tools: List[Callable] = [],
        system_prompt: str = "",
        format_prompt: bool = True,
        parser: XMLParser = XMLParser(fields=["think", ("tool", "answer")]),
        env_parser: XMLParser = XMLParser(fields=["result"]),
        max_turns: int = 10,
        **kwargs,
    ):
        rubric = XMLToolRubric(tools=tools, parser=parser, env_parser=env_parser)
        self.tool_schemas = [infer_schema_from_function(tool) for tool in tools]
        self.tools = {tool.__name__: tool for tool in tools}

        if format_prompt:
            tool_descriptions = format_tool_descriptions(self.tool_schemas)
            formatted_prompt = system_prompt.format(tool_descriptions=tool_descriptions)
        else:
            formatted_prompt = system_prompt
        super().__init__(
            system_prompt=formatted_prompt,
            parser=parser,
            rubric=rubric,
            max_turns=max_turns,
            **kwargs,
        )
        self.env_parser = env_parser

    def get_reward_funcs(self, **kwargs) -> List[RewardFunc]:
        return self.rubric.get_reward_funcs()

    def get_reward_weights(self, **kwargs) -> List[float]:
        return self.rubric.get_reward_weights()

    def is_completed(self, messages: Messages, state: State, **kwargs: Any) -> bool:
        return self.parser.parse_answer(messages) is not None

    def call_tool(self, tool_json: str, max_chars: int = 1024, **kwargs) -> str:
        """Call a tool based on JSON command."""
        try:
            command = json.loads(tool_json)
            if not isinstance(command, dict):
                return 'Error: Tool command must be a JSON object, e.g. \'{"name": "tool_name", "args": {"arg1": "value1", "arg2": "value2"}}\''

            tool_name = command.get("name")
            if not tool_name:
                return 'Error: Tool command must specify \'name\', e.g. \'{"name": "tool_name", "args": {"arg1": "value1", "arg2": "value2"}}\''

            if tool_name not in self.tools:
                return (
                    f"Error: Unknown tool '{tool_name}. "
                    + 'Please format your tool call as \'{"name": "tool_name", "args": {"arg1": "value1", "arg2": "value2"}}\''
                )

            tool_func = self.tools[tool_name]
            tool_args = command.get("args", {})
            if isinstance(tool_args, str):
                tool_schema = next(
                    (
                        schema["args"]
                        for schema in self.tool_schemas
                        if schema["name"] == tool_name
                    ),
                    None,
                )
                return f"Error: Arguments for {tool_name} must be a JSON object with schema {tool_schema}, not a string."

            # Call the tool function with arguments
            result = tool_func(**tool_args)
            if max_chars > 0 and len(str(result)) > max_chars:
                result = str(result)[:max_chars] + "..."
            return str(result)
        except Exception as e:
            return (
                f"Error: {str(e)}. "
                + 'Please format your tool call as \'{{"name": "tool_name", "args": {{"arg1": "value1", "arg2": "value2"}}}}\''
            )

    def env_response(
        self, messages: Messages, state: State, **kwargs
    ) -> Tuple[Messages, State]:
        try:
            parsed = self.parser.parse(messages[-1]["content"])  # type: ignore
            # Check if we got a valid tool field (not just None from failed parsing)
            if hasattr(parsed, "tool") and parsed.tool is not None:
                result = self.call_tool(parsed.tool)
                if len(result.strip()) > 0:
                    return [
                        {
                            "role": "user",
                            "content": self.env_parser.format(result=result),
                        }
                    ], state
                else:
                    return [
                        {
                            "role": "user",
                            "content": "Error: Tool execution returned empty output.",
                        }
                    ], state
        except Exception:
            pass
        return [
            {
                "role": "user",
                "content": "Error: Tool command not found or invalid XML format. Please ensure correct formatting.",
            }
        ], state


def load_environment(
    dataset_name: str,
    split: str,
    tools: List[Callable] = [],
    system_prompt: str = "",
    format_prompt: bool = True,
    parser: XMLParser = XMLParser(fields=["think", ("tool", "answer")]),
    env_parser: XMLParser = XMLParser(fields=["result"]),
    max_turns: int = 10,
    **kwargs,
) -> XMLToolEnv:
    dataset = load_example_dataset(name=dataset_name, split=split)
    return XMLToolEnv(
        dataset=dataset,
        tools=tools,
        system_prompt=system_prompt,
        format_prompt=format_prompt,
        parser=parser,
        env_parser=env_parser,
        max_turns=max_turns,
        **kwargs,
    )



================================================
FILE: examples/sft.py
================================================
import argparse

from datasets import load_dataset
from trl import SFTConfig, SFTTrainer  # type: ignore

import verifiers as vf

"""
accelerate launch --config-file configs/zero3.yaml --num-processes 8 examples/sft.py
"""


def main(args):
    # convenience function for FA2 initialization
    model, tokenizer = vf.get_model_and_tokenizer(args.model, use_liger=False)
    dataset = load_dataset(args.dataset, split="train")

    tok_counts = []
    for row in dataset:
        # count tokens in (prompt, completion)
        messages = row["prompt"] + row["completion"]  # type: ignore
        toks = tokenizer.apply_chat_template(messages, tokenize=True)
        tok_counts.append(len(toks))

    # tok count stats
    print(f"Dataset size: {len(tok_counts)}")
    print(f"Min tokens: {min(tok_counts)}")
    print(f"Max tokens: {max(tok_counts)}")
    print(f"Mean tokens: {sum(tok_counts) / len(tok_counts)}")
    print(f"Median tokens: {sorted(tok_counts)[len(tok_counts) // 2]}")

    args = SFTConfig(
        max_length=args.max_length,
        output_dir=args.output_dir,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        gradient_checkpointing=True,
        bf16=True,
        learning_rate=args.learning_rate,
        num_train_epochs=args.num_train_epochs,
        weight_decay=args.weight_decay,
        max_grad_norm=args.max_grad_norm,
        report_to="wandb",
        save_strategy="epoch",
        save_total_limit=1,
        logging_steps=1,
        save_only_model=True,
        log_on_each_node=True,
        push_to_hub=True,
        hub_model_id=args.name_to_save,
    )

    trainer = SFTTrainer(
        model=model,
        args=args,
        train_dataset=dataset,  # type: ignore
    )
    trainer.train()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", "-m", type=str, default="willcb/Qwen3-1.7B")
    parser.add_argument("--dataset", "-d", type=str, default="willcb/V3-wordle")
    parser.add_argument("--output-dir", "-o", type=str, default="outputs")
    parser.add_argument("--name-to-save", "-n", type=str, default="Qwen3-1.7B-Wordle")
    parser.add_argument("--max-length", "-l", type=int, default=8192)
    parser.add_argument("--per-device-train-batch-size", "-b", type=int, default=2)
    parser.add_argument("--gradient-accumulation-steps", "-g", type=int, default=1)
    parser.add_argument("--learning-rate", "-r", type=float, default=2e-5)
    parser.add_argument("--num-train-epochs", "-e", type=int, default=3)
    parser.add_argument("--weight-decay", "-w", type=float, default=0.01)
    parser.add_argument("--max-grad-norm", "-g", type=float, default=0.1)
    parser.add_argument("--push-to-hub", "-p", type=bool, default=True)
    args = parser.parse_args()
    main(args)



================================================
FILE: examples/grpo/train_arc_1d.py
================================================
import verifiers as vf

"""
# install
vf-install reasoning-gym (-p /path/to/environments)

# quick eval
vf-eval reasoning-gym (-m model_name in endpoints.py)

inference:
CUDA_VISIBLE_DEVICES=0,1,2,3 vf-vllm --model willcb/Qwen3-14B-Arc-1D-SFT \
    --tensor-parallel-size 2 --data-parallel-size 2 \
    --enforce-eager --disable-log-requests

training:
CUDA_VISIBLE_DEVICES=4,5,6,7 accelerate launch --num-processes 4 \
    --config-file configs/zero3.yaml examples/grpo/train_arc_1d.py
"""

size = "14B"
model_name = f"willcb/Qwen3-{size}-Arc-1D-SFT"
model, tokenizer = vf.get_model_and_tokenizer(model_name)

vf_env = vf.load_environment(
    env_id="reasoning-gym", gym="arc_1d", num_samples=4000, seed=1
)

run_name = f"arc_1d-grpo-{size}"
training_args = vf.grpo_defaults(run_name=run_name)
training_args.num_iterations = 1
training_args.per_device_train_batch_size = 4
training_args.num_generations = 16
training_args.gradient_accumulation_steps = 8
training_args.max_seq_len = 4096
training_args.max_steps = 500

trainer = vf.GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    env=vf_env,
    args=training_args,
)
trainer.train()



================================================
FILE: examples/grpo/train_continuation_quality.py
================================================
import verifiers as vf

"""
# install
vf-install continuation-quality (-p /path/to/environments)

# quick eval
vf-eval continuation-quality (-m model_name in endpoints.py)

inference:
CUDA_VISIBLE_DEVICES=0 vf-vllm --model Qwen/Qwen2.5-0.5B \
    --enforce-eager --disable-log-requests

training:
CUDA_VISIBLE_DEVICES=1 accelerate launch --num-processes 1 \
    --config-file configs/zero3.yaml examples/grpo/train_continuation_quality.py
"""

model_name = "Qwen/Qwen2.5-0.5B"
vf_env = vf.load_environment(env_id="continuation-quality")
model, tokenizer = vf.get_model_and_tokenizer(model_name)
trainer = vf.GRPOTrainer(
    env=vf_env,
    model=model,
    processing_class=tokenizer,
    args=vf.grpo_defaults(run_name="continuation-quality"),
)
trainer.train()



================================================
FILE: examples/grpo/train_gsm8k.py
================================================
import verifiers as vf

"""
# install
vf-install gsm8k (-p /path/to/environments)

# quick eval
vf-eval gsm8k (-m model_name in endpoints.py)

inference:
CUDA_VISIBLE_DEVICES=0 vf-vllm --model willcb/Qwen3-0.6B --enforce-eager --disable-log-requests

training:
CUDA_VISIBLE_DEVICES=1 accelerate launch --num-processes 1 \
    --config-file configs/zero3.yaml examples/grpo/train_gsm8k.py
"""

vf_env = vf.load_environment(env_id="gsm8k", num_eval_examples=100)

model_name = "willcb/Qwen3-0.6B"
run_name = "gsm8k-grpo_" + model_name.split("/")[-1].lower()

model, tokenizer = vf.get_model_and_tokenizer(model_name)
training_args = vf.grpo_defaults(run_name=run_name)

training_args.per_device_train_batch_size = 12
training_args.num_generations = 12
training_args.gradient_accumulation_steps = 8
training_args.max_tokens = 2048
training_args.max_seq_len = 2048
training_args.eval_strategy = "steps"
training_args.eval_steps = 10
training_args.save_strategy = "steps"
training_args.save_steps = 100
training_args.max_steps = 200
training_args.eval_strategy = "steps"
training_args.eval_steps = 10

trainer = vf.GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    env=vf_env,
    args=training_args,
    peft_config=vf.lora_defaults(),
)
trainer.train()



================================================
FILE: examples/grpo/train_math_group.py
================================================
import verifiers as vf

"""
# install
vf-install math-group (-p /path/to/environments)

# quick eval
vf-eval math-group (-m model_name in endpoints.py)

inference:
CUDA_VISIBLE_DEVICES=0 vf-vllm --model willcb/Qwen3-0.6B \
    --enforce-eager --disable-log-requests

training:
CUDA_VISIBLE_DEVICES=1 accelerate launch --num-processes 1 \
    --config-file configs/zero3.yaml examples/grpo/train_math_group.py
"""

vf_env = vf.load_environment(env_id="math-group")

model_name = "willcb/Qwen3-0.6B"
model, tokenizer = vf.get_model_and_tokenizer(model_name)
run_name = "math-grpo_" + model_name.split("/")[-1].lower()

training_args = vf.grpo_defaults(run_name=run_name)
training_args.per_device_train_batch_size = 16
training_args.num_generations = 16
training_args.gradient_accumulation_steps = 8
training_args.num_iterations = 1
training_args.max_prompt_length = 512
training_args.max_completion_length = 2048
training_args.max_steps = 100

trainer = vf.GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    env=vf_env,
    args=training_args,
)
trainer.train()



================================================
FILE: examples/grpo/train_math_python.py
================================================
import verifiers as vf

"""
# install
vf-install math-python (-p /path/to/environments)

# eval
vf-eval math-python (-m model_name in endpoints.py)

# inference
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 vf-vllm --model 'willcb/Qwen3-1.7B' \
    --data-parallel-size 6 --enforce-eager --disable-log-requests \
    --enable-auto-tool-choice --tool-call-parser hermes

# training
CUDA_VISIBLE_DEVICES=6,7 accelerate launch --num-processes 2 \
    --config-file configs/zero3.yaml examples/grpo/train_math_python.py
"""

vf_env = vf.load_environment(env_id="math-python")

model_name = "willcb/Qwen3-1.7B"
model, tokenizer = vf.get_model_and_tokenizer(model_name)
run_name = "math-python_" + model_name.split("/")[-1].lower()

training_args = vf.grpo_defaults(run_name=run_name)
training_args.per_device_train_batch_size = 8
training_args.num_generations = 16
training_args.gradient_accumulation_steps = 8
training_args.max_tokens = 2048
training_args.max_seq_len = 4096
training_args.max_steps = 200
training_args.mask_env_responses = True
training_args.max_grad_norm = 0.1
training_args.beta = 0.1

trainer = vf.GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    env=vf_env,
    args=training_args,
    # lora_config=vf.lora_defaults()
)
trainer.train()



================================================
FILE: examples/grpo/train_reverse_text.py
================================================
import verifiers as vf

"""
# install
vf-install reverse-text (-p /path/to/environments)

# quick eval
vf-eval reverse-text (-m model_name in endpoints.py)

inference:
CUDA_VISIBLE_DEVICES=0 vf-vllm --model willcb/Qwen2.5-0.5B-Reverse-SFT \
    --enforce-eager --disable-log-requests

training:
CUDA_VISIBLE_DEVICES=1 accelerate launch --num-processes 1 \
    --config-file configs/zero3.yaml examples/grpo/train_reverse_text.py
"""

model_name = "willcb/Qwen2.5-0.5B-Reverse-SFT"
model, tokenizer = vf.get_model_and_tokenizer(model_name)

vf_env = vf.load_environment(env_id="reverse-text")

args = vf.grpo_defaults(run_name="reverse-text")
args.per_device_train_batch_size = 12
args.num_generations = 12
args.gradient_accumulation_steps = 8
args.max_steps = 100
args.eval_strategy = "steps"
args.eval_steps = 2
args.max_tokens = 1024

trainer = vf.GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    env=vf_env,
    peft_config=vf.lora_defaults(),
    args=args,
)
trainer.train()



================================================
FILE: examples/grpo/train_self_reward.py
================================================
import verifiers as vf

"""
# install
vf-install self-reward (-p /path/to/environments)

# quick eval
vf-eval self-reward (-m model_name in endpoints.py)

inference:
CUDA_VISIBLE_DEVICES=0 vf-vllm --model Qwen/Qwen2.5-7B-Instruct \
    --enforce-eager --disable-log-requests

training:
CUDA_VISIBLE_DEVICES=1 accelerate launch --num-processes 1 \
    --config-file configs/zero3.yaml examples/grpo/train_self_reward.py
"""

model_name = "Qwen/Qwen2.5-7B-Instruct"
vf_env = vf.load_environment(env_id="self-reward", model_name=model_name)
model, tokenizer = vf.get_model_and_tokenizer(model_name)
trainer = vf.GRPOTrainer(
    env=vf_env,
    model=model,
    processing_class=tokenizer,
    args=vf.grpo_defaults(run_name="self-reward"),
)
trainer.train()



================================================
FILE: examples/grpo/train_sentence_repeater.py
================================================
import verifiers as vf

"""
# install
vf-install sentence-repeater (-p /path/to/environments)

# quick eval
vf-eval sentence-repeater (-m model_name in endpoints.py)

inference:
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 vf-vllm --model Qwen/Qwen2.5-1.5B-Instruct \
    --data-parallel-size 6 --enforce-eager --disable-log-requests

training:
CUDA_VISIBLE_DEVICES=6,7 accelerate launch --num-processes 2 \
    --config-file configs/zero3.yaml examples/grpo/train_sentence_repeater.py
"""

model_name = "Qwen/Qwen2.5-1.5B-Instruct"
model, tokenizer = vf.get_model_and_tokenizer(model_name)

vf_env = vf.load_environment(env_id="sentence-repeater")

run_name = "sentence-repeater-grpo-qwen1.5b"
training_args = vf.grpo_defaults(run_name=run_name)
training_args.per_device_train_batch_size = 8
training_args.num_generations = 16
training_args.gradient_accumulation_steps = 8
training_args.max_tokens = 1024  # per turn
training_args.max_seq_len = 4096
training_args.max_steps = 200
training_args.eval_strategy = "steps"
training_args.eval_steps = 20
training_args.mask_env_responses = True
training_args.max_grad_norm = 0.1
training_args.beta = 0.0
training_args.async_generation_timeout = 600

trainer = vf.GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    env=vf_env,
    args=training_args,
)
trainer.train()



================================================
FILE: examples/grpo/train_tool_test.py
================================================
import verifiers as vf

"""
# install
vf-install tool-test (-p /path/to/environments)

# quick eval
vf-eval tool-test (-m model_name in endpoints.py)

inference:
CUDA_VISIBLE_DEVICES=0 vf-vllm --model willcb/Qwen3-0.6B \
    --enforce-eager --disable-log-requests \
    --enable-auto-tool-choice --tool-call-parser hermes

training:
CUDA_VISIBLE_DEVICES=1 accelerate launch --num-processes 1 \
    --config-file configs/zero3.yaml examples/grpo/train_tool_test.py
"""

vf_env = vf.load_environment(env_id="tool-test", num_eval_examples=100)

model_name = "willcb/Qwen3-0.6B"
run_name = "tool-test_" + model_name.split("/")[-1].lower()

model, tokenizer = vf.get_model_and_tokenizer(model_name)
training_args = vf.grpo_defaults(run_name=run_name)

training_args.per_device_train_batch_size = 12
training_args.num_generations = 12
training_args.gradient_accumulation_steps = 8
training_args.max_tokens = 2048
training_args.max_seq_len = 2048
training_args.eval_strategy = "steps"
training_args.eval_steps = 10
training_args.save_strategy = "steps"
training_args.save_steps = 100
training_args.max_steps = 200
training_args.eval_strategy = "steps"
training_args.eval_steps = 10

trainer = vf.GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    env=vf_env,
    args=training_args,
    peft_config=vf.lora_defaults(),
)
trainer.train()



================================================
FILE: examples/grpo/train_wiki_search.py
================================================
import verifiers as vf

"""
# install
vf-install wiki-search (-p /path/to/environments)

# quick eval
vf-eval wiki-search (-m model_name in endpoints.py)

inference:
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 vf-vllm --model willcb/Qwen3-8B-Wiki-Search-SFT \
    --data-parallel-size 6 --enforce-eager --disable-log-requests

training:
CUDA_VISIBLE_DEVICES=6,7 accelerate launch --num-processes 2 \
    --config-file configs/zero3.yaml examples/grpo/train_wiki_search.py
"""

vf_env = vf.load_environment(env_id="wiki-search")

model_name = "willcb/Qwen3-8B-Wiki-Search-SFT"
model, tokenizer = vf.get_model_and_tokenizer(model_name)
run_name = "wiki-trivia-grpo_" + model_name.split("/")[-1].lower()

training_args = vf.grpo_defaults(run_name=run_name)
training_args.per_device_train_batch_size = 8
training_args.num_generations = 16
training_args.gradient_accumulation_steps = 16
training_args.num_iterations = 1
training_args.num_train_epochs = 5
training_args.max_seq_len = 4096
training_args.max_steps = 500
training_args.save_steps = 100

trainer = vf.GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    env=vf_env,
    args=training_args,
)
trainer.train()



================================================
FILE: examples/grpo/train_wordle.py
================================================
import argparse

import verifiers as vf

"""
# install
vf-install wordle (-p /path/to/environments)

# quick eval
vf-eval wordle -m (model_name in endpoints.py)

1.7b inference:
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 vf-vllm --model willcb/Qwen3-1.7B-Wordle \
    --data-parallel-size 6 --enforce-eager --disable-log-requests

1.7b training:
CUDA_VISIBLE_DEVICES=6,7 accelerate launch --num-processes 2 \
    --config-file configs/zero3.yaml examples/grpo/train_wordle.py --size 1.7B

4b inference:
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 vf-vllm --model willcb/Qwen3-4B-Wordle \
    --data-parallel-size 6 --enforce-eager --disable-log-requests

4b training:
CUDA_VISIBLE_DEVICES=6,7 accelerate launch --num-processes 2 \
    --config-file configs/zero3.yaml examples/grpo/train_wordle.py --size 4B
"""


def main(args):
    size = args.size
    model_name = f"willcb/Qwen3-{size}-Wordle"
    model, tokenizer = vf.get_model_and_tokenizer(model_name)
    vf_env = vf.load_environment(env_id="wordle", use_think=True)
    run_name = f"wordle-grpo-{size}"
    training_args = vf.grpo_defaults(run_name=run_name)
    training_args.per_device_train_batch_size = 8
    training_args.num_generations = 16
    training_args.gradient_accumulation_steps = 8
    training_args.max_tokens = 1024  # per turn
    training_args.max_seq_len = 4096
    training_args.max_steps = 200
    training_args.eval_strategy = "steps"
    training_args.eval_steps = 20
    training_args.mask_env_responses = True
    training_args.max_grad_norm = 0.1
    training_args.beta = 0.0

    trainer = vf.GRPOTrainer(
        model=model,
        processing_class=tokenizer,
        env=vf_env,
        args=training_args,
        # lora_config=vf.lora_defaults()
    )
    trainer.train()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--size", "-s", type=str, default="1.7B")
    args = parser.parse_args()
    main(args)



================================================
FILE: tests/README.md
================================================
# Verifiers Test Suite

This directory contains the test suite for the verifiers package.

## Setup

**Prerequisites:**
- Install `uv` package manager (https://docs.astral.sh/uv/getting-started/installation/)
- Ensure Python 3.11+ is available

Install test dependencies:

```bash
uv sync --extra tests
```

## Running Tests

Run all tests:

```bash
uv run pytest
```

Run specific test files:

```bash
uv run pytest tests/test_parser.py
uv run pytest tests/test_xml_parser.py
uv run pytest tests/test_think_parser.py
```

Run with coverage:

```bash
uv run pytest --cov=verifiers
```

Run only unit tests:

```bash
uv run pytest -m unit
```

## Test Structure

- `conftest.py` - Pytest configuration and shared fixtures
- `test_parser.py` - Tests for the base Parser class
- `test_xml_parser.py` - Tests for the XMLParser class  
- `test_think_parser.py` - Tests for the ThinkParser class
- `test_environment.py` - Tests for the base Environment class
- `test_singleturn_env.py` - Tests for the SingleTurnEnv class
- `test_multiturn_env.py` - Tests for the MultiTurnEnv class

## Test Markers

- `unit` - Fast unit tests (default for all current tests)
- `integration` - Integration tests
- `slow` - Slow-running tests
- `asyncio` - Async tests

## Async Testing & Mocking

The test suite includes comprehensive support for testing async Environment classes:

### AsyncOpenAI Client Mocking
- `mock_openai_client` fixture provides a fully mocked AsyncOpenAI client
- Supports both chat completions and regular completions
- No actual API calls are made during testing

### Test Datasets
- `sample_dataset` - Basic question/answer dataset
- `sample_chat_dataset` - Pre-formatted chat messages
- Custom datasets can be created using `Dataset.from_dict()`

### Async Test Examples
```python
@pytest.mark.asyncio
async def test_my_async_function(mock_openai_client):
    env = SingleTurnEnv(client=mock_openai_client, model="test", ...)
    result = await env.rollout(...)
    assert result[0] == expected_completion

# MultiTurnEnv testing
@pytest.mark.asyncio  
async def test_multiturn_conversation(mock_multiturn_env):
    # Configure sequential responses
    responses = ["response1", "response2", "final DONE"]
    mock_multiturn_env.client.chat.completions.create.side_effect = [
        create_mock_response(resp) for resp in responses
    ]
    
    completion, state = await mock_multiturn_env.rollout(...)
    assert len(completion) > 1  # Multiple turns
```

### Environment Testing
- **SingleTurnEnv**: Simple request-response testing
- **MultiTurnEnv**: Complex multi-turn conversation testing with:
  - Turn-by-turn conversation flow
  - Max turns limiting
  - Environment response integration
  - Completion detection logic
  - State management across turns
- Tests cover both chat and completion message formats
- Mocked responses simulate real OpenAI API behavior
- Error handling and edge cases are tested
- No real LLM requests are made

## Adding New Tests

1. Create test files following the `test_*.py` naming convention
2. Use the fixtures from `conftest.py` for common instances
3. Add appropriate test markers (`@pytest.mark.asyncio` for async tests)
4. Use `mock_openai_client` for Environment testing
5. Follow the existing test structure and naming conventions


================================================
FILE: tests/__init__.py
================================================
# Test package for verifiers


================================================
FILE: tests/conftest.py
================================================
"""Pytest configuration and fixtures for verifiers tests."""

from unittest.mock import AsyncMock, MagicMock

import pytest
from datasets import Dataset

from verifiers import (
    MultiTurnEnv,
    Parser,
    Rubric,
    SingleTurnEnv,
    ThinkParser,
    XMLParser,
)


@pytest.fixture
def basic_parser():
    """Return a basic Parser instance."""
    return Parser()


@pytest.fixture
def xml_parser():
    """Return an XMLParser instance with common fields."""
    return XMLParser(fields=["reasoning", "answer"], answer_field="answer")


@pytest.fixture
def xml_parser_with_alternatives():
    """Return an XMLParser instance with alternative field names."""
    return XMLParser(fields=["reasoning", ("code", "answer")], answer_field="answer")


@pytest.fixture
def think_parser():
    """Return a ThinkParser instance."""
    return ThinkParser()


@pytest.fixture
def think_parser_with_extractor():
    """Return a ThinkParser instance with custom extraction function."""

    def extract_boxed(text):
        """Simple boxed answer extractor for testing."""
        import re

        match = re.search(r"\\boxed\{([^}]+)\}", text)
        return match.group(1) if match else text

    return ThinkParser(extract_fn=extract_boxed)


# Async test fixtures for Environment testing


class MockAsyncOpenAI:
    """Mock AsyncOpenAI client that maps conversation inputs to outputs."""

    def __init__(self):
        self.chat_completions = {}  # Maps conversation history to responses
        self.text_completions = {}  # Maps prompts to responses
        self.default_chat_response = "This is a test response"
        self.default_text_response = "This is a test completion"
        self.base_url = "http://localhost/v1/"  # For testing URL parsing

        # Create mock structure
        self.chat = MagicMock()
        self.completions = MagicMock()
        self.chat.completions = MagicMock()

        # Set up async methods
        self.chat.completions.create = AsyncMock(
            side_effect=self._handle_chat_completion
        )
        self.completions.create = AsyncMock(side_effect=self._handle_text_completion)

    def add_chat_response(
        self, messages, response, finish_reason="stop", tool_calls=None
    ):
        """Add a mapped response for specific messages."""
        # Convert messages to a hashable key
        key = self._messages_to_key(messages)
        self.chat_completions[key] = {
            "content": response,
            "finish_reason": finish_reason,
            "tool_calls": tool_calls,
        }

    def add_text_response(self, prompt, response, finish_reason="stop"):
        """Add a mapped response for specific prompt."""
        self.text_completions[prompt] = {
            "text": response,
            "finish_reason": finish_reason,
        }

    def set_default_responses(self, chat_response=None, text_response=None):
        """Set default responses when no mapping found."""
        if chat_response:
            self.default_chat_response = chat_response
        if text_response:
            self.default_text_response = text_response

    async def _handle_chat_completion(self, messages, **kwargs):
        """Handle chat completion requests."""
        key = self._messages_to_key(messages)

        if key in self.chat_completions:
            response_data = self.chat_completions[key]
        else:
            response_data = {
                "content": self.default_chat_response,
                "finish_reason": "stop",
                "tool_calls": None,
            }

        # Create mock response that mimics ChatCompletion
        from openai.types.chat.chat_completion import ChatCompletion, Choice
        from openai.types.chat.chat_completion_message import ChatCompletionMessage

        # Create a proper mock that will pass isinstance checks
        mock_response = MagicMock(spec=ChatCompletion)
        mock_choice = MagicMock(spec=Choice)
        mock_message = MagicMock(spec=ChatCompletionMessage)

        # Set the attributes
        mock_message.content = response_data["content"]
        mock_message.role = "assistant"
        mock_message.tool_calls = response_data.get("tool_calls", None)
        mock_choice.message = mock_message
        mock_choice.finish_reason = response_data["finish_reason"]
        mock_choice.index = 0

        mock_response.choices = [mock_choice]
        mock_response.id = "test-id"
        mock_response.model = "test-model"
        mock_response.object = "chat.completion"

        return mock_response

    async def _handle_text_completion(self, prompt, **kwargs):
        """Handle text completion requests."""
        if prompt in self.text_completions:
            response_data = self.text_completions[prompt]
        else:
            response_data = {
                "text": self.default_text_response,
                "finish_reason": "stop",
            }

        # Create mock response that mimics Completion
        from openai.types.completion import Completion
        from openai.types.completion_choice import CompletionChoice

        # Create a proper mock that will pass isinstance checks
        mock_response = MagicMock(spec=Completion)
        mock_choice = MagicMock(spec=CompletionChoice)

        # Set the attributes
        mock_choice.text = response_data["text"]
        mock_choice.finish_reason = response_data["finish_reason"]
        mock_choice.index = 0

        mock_response.choices = [mock_choice]
        mock_response.id = "test-id"
        mock_response.model = "test-model"
        mock_response.object = "text_completion"

        return mock_response

    def _messages_to_key(self, messages):
        """Convert messages list to a hashable key."""
        # Create a simplified representation for hashing
        key_parts = []
        for msg in messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            key_parts.append(f"{role}:{content}")
        return tuple(key_parts)


@pytest.fixture
def mock_openai_client():
    """Return a mocked AsyncOpenAI client with input-output mapping."""
    return MockAsyncOpenAI()


@pytest.fixture
def sample_dataset():
    """Return a sample dataset for testing."""
    return Dataset.from_dict(
        {
            "question": ["What is 2+2?", "What is the capital of France?"],
            "answer": ["4", "Paris"],
        }
    )


@pytest.fixture
def sample_chat_dataset():
    """Return a sample dataset with chat format."""
    return Dataset.from_dict(
        {
            "prompt": [
                [{"role": "user", "content": "What is 2+2?"}],
                [{"role": "user", "content": "What is the capital of France?"}],
            ],
            "answer": ["4", "Paris"],
        }
    )


@pytest.fixture
def mock_singleturn_env(mock_openai_client, sample_dataset):
    """Return a SingleTurnEnv with mocked client and dataset."""
    return SingleTurnEnv(
        client=mock_openai_client,
        model="test-model",
        dataset=sample_dataset,
        system_prompt="You are a helpful assistant.",
        parser=Parser(),
        rubric=Rubric(),
    )


@pytest.fixture
def mock_singleturn_env_completion(mock_openai_client):
    """Return a SingleTurnEnv for completion format testing."""
    completion_dataset = Dataset.from_dict(
        {
            "prompt": ["Calculate 2+2:", "Name the capital of France:"],
            "answer": ["4", "Paris"],
        }
    )
    return SingleTurnEnv(
        client=mock_openai_client,
        model="test-model",
        dataset=completion_dataset,
        message_type="completion",
        parser=Parser(),
        rubric=Rubric(),
    )


# MultiTurnEnv test fixtures


class SimpleMultiTurnEnv(MultiTurnEnv):
    """Simple concrete implementation of MultiTurnEnv for testing."""

    def __init__(self, completion_condition="answer", **kwargs):
        super().__init__(**kwargs)
        self.completion_condition = (
            completion_condition  # "answer", "max_turns", "error"
        )
        self.env_response_count = 0

    def is_completed(self, messages, state, **kwargs):
        """Simple completion logic for testing."""
        if self.completion_condition == "answer":
            # Complete when assistant says "DONE"
            if messages and messages[-1].get("role") == "assistant":
                return "DONE" in messages[-1].get("content", "")
        elif self.completion_condition == "max_turns":
            # Never complete naturally (test max_turns)
            return False
        elif self.completion_condition == "error":
            # Complete on any error
            if messages and messages[-1].get("role") == "assistant":
                return messages[-1].get("content", "").startswith("[ERROR]")
        return False

    def env_response(self, messages, state, **kwargs):
        """Simple environment response for testing."""
        self.env_response_count += 1

        if self.completion_condition == "answer":
            # Encourage completion after a few turns
            if self.env_response_count >= 2:
                return [{"role": "user", "content": "Please finish with DONE"}], state
            else:
                return [
                    {
                        "role": "user",
                        "content": f"Continue (turn {self.env_response_count})",
                    }
                ], state
        else:
            return [
                {
                    "role": "user",
                    "content": f"Environment response {self.env_response_count}",
                }
            ], state


@pytest.fixture
def mock_multiturn_env(mock_openai_client, sample_chat_dataset):
    """Return a MultiTurnEnv for basic testing."""
    return SimpleMultiTurnEnv(
        client=mock_openai_client,
        model="test-model",
        dataset=sample_chat_dataset,
        max_turns=3,
        completion_condition="answer",
        parser=Parser(),
        rubric=Rubric(),
    )


@pytest.fixture
def mock_multiturn_env_max_turns(mock_openai_client, sample_chat_dataset):
    """Return a MultiTurnEnv that tests max_turns limiting."""
    return SimpleMultiTurnEnv(
        client=mock_openai_client,
        model="test-model",
        dataset=sample_chat_dataset,
        max_turns=2,
        completion_condition="max_turns",  # Never complete naturally
        parser=Parser(),
        rubric=Rubric(),
    )



================================================
FILE: tests/mock_client_guide.md
================================================
# MockAsyncOpenAI Guide: Input-Output Mapping

## Overview

The `MockAsyncOpenAI` provides a sophisticated mock OpenAI client that maps specific inputs to specific outputs, making tests more predictable and order-independent.

## Key Features

### 1. Input-Output Mapping
- **Chat completions**: Maps conversation history to specific responses
- **Text completions**: Maps prompts to specific responses
- **Order independence**: Responses are consistent regardless of call order
- **Default responses**: Fallback responses for unmapped inputs

### 2. Smart Hashing
- Conversations are converted to hashable keys for consistent lookup
- Message order and content are preserved in the mapping
- System prompts are included in the mapping for realistic testing

## Basic Usage

### Setting Up Chat Response Mappings

```python
from tests.conftest import MockAsyncOpenAI

client = MockAsyncOpenAI()

# Add specific chat response mapping
client.add_chat_response(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is 2+2?"}
    ],
    response="The answer is 4",
    finish_reason="stop"
)

# Use the client
response = await client.chat.completions.create(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is 2+2?"}
    ]
)
assert response.choices[0].message.content == "The answer is 4"
```

### Setting Up Text Completion Mappings

```python
# Add text completion mapping
client.add_text_response(
    prompt="Calculate 2+2:",
    response="4",
    finish_reason="stop"
)

# Use the client
response = client.completions.create(prompt="Calculate 2+2:")
assert response.choices[0].text == "4"
```

### Custom Default Responses

```python
# Set custom default responses
client.set_default_responses(
    chat_response="I don't know that",
    text_response="No answer available"
)

# Unmapped requests will use these defaults
response = client.completions.create(prompt="Unknown question")
assert response.choices[0].text == "No answer available"
```

## Advanced Usage

### Testing Order Independence

```python
# Set up multiple mappings
client.add_chat_response(
    messages=[{"role": "user", "content": "Question A"}],
    response="Answer A"
)
client.add_chat_response(
    messages=[{"role": "user", "content": "Question B"}],
    response="Answer B"
)

# Test in different orders
response_a = await client.chat.completions.create(
    messages=[{"role": "user", "content": "Question A"}]
)
response_b = await client.chat.completions.create(
    messages=[{"role": "user", "content": "Question B"}]
)

# Test reverse order
response_b2 = await client.chat.completions.create(
    messages=[{"role": "user", "content": "Question B"}]
)
response_a2 = await client.chat.completions.create(
    messages=[{"role": "user", "content": "Question A"}]
)

# All responses are consistent
assert response_a.choices[0].message.content == response_a2.choices[0].message.content
assert response_b.choices[0].message.content == response_b2.choices[0].message.content
```

### Using with Environments

```python
from verifiers.envs import SingleTurnEnv
from verifiers.parsers import Parser
from verifiers.rubrics import Rubric

# Set up client with specific mappings
client = MockAsyncOpenAI()
client.add_chat_response(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is 2+2?"}
    ],
    response="The answer is 4"
)

# Create environment
env = SingleTurnEnv(
    client=client,
    model="test-model",
    dataset=your_dataset,
    system_prompt="You are a helpful assistant.",
    parser=Parser(),
    rubric=Rubric()
)

# Run rollouts - responses will be consistent
completion, state = await env.rollout(
    client=client,
    model="test-model",
    prompt=[{"role": "user", "content": "What is 2+2?"}],
    answer="4"
)
```

## Comparison with Alternative Approaches

### Order-Dependent Mocking
```python
@pytest.fixture
def mock_client():
    client = AsyncMock()
    client.chat.completions.create.side_effect = [
        mock_response_1,
        mock_response_2,
        mock_response_3
    ]
    return client
```

### Input-Output Mapping
```python
@pytest.fixture
def mock_client():
    client = MockAsyncOpenAI()
    client.add_chat_response(messages=conversation_1, response="Response 1")
    client.add_chat_response(messages=conversation_2, response="Response 2")
    client.add_chat_response(messages=conversation_3, response="Response 3")
    return client
```

## Benefits

1. **Predictable Testing**: Responses are always the same for the same inputs
2. **Order Independence**: Tests don't break when execution order changes
3. **Realistic Simulation**: Mimics how a real API would respond to specific inputs
4. **Easier Debugging**: Clear mapping between inputs and outputs
5. **Flexible Defaults**: Handle unmapped cases gracefully

## Implementation Details

### Message Hashing
- Messages are converted to tuples of `"role:content"` strings
- This creates a consistent, hashable key for lookup
- Preserves message order and content exactly

### Response Structure
- Chat responses include `message.content` and `finish_reason`
- Text responses include `text` and `finish_reason`
- Response objects mirror OpenAI's API structure

### Error Handling
- Unmapped inputs return default responses (never fail)
- Invalid inputs log warnings but don't crash tests
- Graceful fallbacks for edge cases

## Best Practices

1. **Set up mappings early**: Add all expected input-output pairs before running tests
2. **Use realistic inputs**: Include system prompts and full conversation context
3. **Test edge cases**: Include unmapped inputs to verify default behavior
4. **Keep mappings simple**: One mapping per distinct conversation or prompt
5. **Group related tests**: Use fixtures to share common mappings across test methods


================================================
FILE: tests/mock_openai_client.py
================================================
"""
Mock OpenAI client for testing purposes.
"""

from typing import List, Dict
from unittest.mock import Mock


class MockCompletion:
    def __init__(self, content: str, finish_reason: str = "stop"):
        self.message = Mock()
        self.message.content = content
        self.text = content
        self.finish_reason = finish_reason


class MockCompletionResponse:
    def __init__(self, content: str, finish_reason: str = "stop"):
        self.choices = [MockCompletion(content, finish_reason)]


class MockChatCompletions:
    def __init__(
        self, responses: Dict[str, str] = None, default_response: str = "Test response"
    ):
        self.responses = responses or {}
        self.default_response = default_response
        self.call_count = 0
        self.last_messages = None
        self.last_model = None

    def create(
        self, model: str, messages: List[Dict[str, str]], **kwargs
    ) -> MockCompletionResponse:
        self.call_count += 1
        self.last_messages = messages
        self.last_model = model

        # Handle special error cases
        if kwargs.get("max_tokens", 0) == 1:
            return MockCompletionResponse("", "length")

        # Check for specific response patterns
        last_message = messages[-1]["content"] if messages else ""
        for pattern, response in self.responses.items():
            if pattern in last_message:
                return MockCompletionResponse(response)

        return MockCompletionResponse(self.default_response)


class MockCompletions:
    def __init__(
        self, responses: Dict[str, str] = None, default_response: str = "Test response"
    ):
        self.responses = responses or {}
        self.default_response = default_response
        self.call_count = 0
        self.last_prompt = None
        self.last_model = None

    def create(self, model: str, prompt: str, **kwargs) -> MockCompletionResponse:
        self.call_count += 1
        self.last_prompt = prompt
        self.last_model = model

        # Handle special error cases
        if kwargs.get("max_tokens", 0) == 1:
            return MockCompletionResponse("", "length")

        # Check for specific response patterns
        for pattern, response in self.responses.items():
            if pattern in prompt:
                return MockCompletionResponse(response)

        return MockCompletionResponse(self.default_response)


class MockOpenAIClient:
    """
    Mock OpenAI client that simulates API responses for testing.
    """

    def __init__(
        self,
        chat_responses: Dict[str, str] = None,
        completion_responses: Dict[str, str] = None,
        default_chat_response: str = "Test chat response",
        default_completion_response: str = "Test completion response",
        base_url: str = "http://localhost:8000",
    ):
        self.base_url = base_url
        self.chat = Mock()
        self.chat.completions = MockChatCompletions(
            chat_responses, default_chat_response
        )
        self.completions = MockCompletions(
            completion_responses, default_completion_response
        )

    def reset_counters(self):
        """Reset call counters for testing."""
        self.chat.completions.call_count = 0
        self.completions.call_count = 0


def create_mock_math_client():
    """Create a mock client that responds appropriately to math problems."""
    math_responses = {
        "What is 2+2": "<think>\n2 + 2 = 4\n</think>\n<answer>4</answer>",
        "What is 5*3": "<think>\n5 * 3 = 15\n</think>\n<answer>15</answer>",
        "solve x+1=3": "<think>\nx + 1 = 3\nx = 3 - 1\nx = 2\n</think>\n<answer>2</answer>",
    }
    return MockOpenAIClient(chat_responses=math_responses)


def create_mock_tool_client():
    """Create a mock client that responds with tool usage."""
    tool_responses = {
        "calculate": """<reasoning>
I need to use the calculator tool to compute this.
</reasoning>
<tool>
{"name": "calculator", "args": {"expression": "2+2"}}
</tool>""",
        "search": """<reasoning>
I need to search for information about this topic.
</reasoning>
<tool>
{"name": "search", "args": {"query": "python programming"}}
</tool>""",
    }
    return MockOpenAIClient(chat_responses=tool_responses)


def create_mock_error_client():
    """Create a mock client that simulates various error conditions."""

    def create_error(**kwargs):
        if "longer than the maximum" in str(kwargs):
            raise Exception(
                "This model's maximum context length is 4096 tokens. Your message was longer than the maximum."
            )
        if "exceeds the model" in str(kwargs):
            raise Exception("Input exceeds the model's context window.")
        raise Exception("Generic error")

    client = MockOpenAIClient()
    client.chat.completions.create = create_error
    client.completions.create = create_error
    return client



================================================
FILE: tests/test_env_group.py
================================================
"""Tests for the EnvGroup class."""

from unittest.mock import AsyncMock

import pytest
from datasets import Dataset

from verifiers import EnvGroup, Rubric, SingleTurnEnv
from verifiers.envs.env_group import EnvGroupRubric
from verifiers.types import RolloutScores


class TestEnvGroupRubric:
    """Test cases for the EnvGroupRubric class."""

    def test_env_group_rubric_initialization(self, mock_openai_client):
        """Test EnvGroupRubric initialization with multiple environments."""

        # Create test environments with different rubrics
        def func1(completion, **kwargs):
            return 1.0

        def func2(completion, **kwargs):
            return 0.5

        def func3(completion, **kwargs):
            return 0.8

        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(funcs=[func1, func2], weights=[1.0, 0.5]),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(funcs=[func2, func3], weights=[0.7, 1.0]),
        )

        env_map = {"task1": env1, "task2": env2}
        rubric = EnvGroupRubric(env_map)

        assert rubric.env_map == env_map
        # Should have all unique reward function names
        assert set(rubric.all_reward_names) == {"func1", "func2", "func3"}

    @pytest.mark.asyncio
    async def test_env_group_rubric_score_rollout(self, mock_openai_client):
        """Test scoring a rollout with EnvGroupRubric."""

        # Create test environments
        def func1(completion, **kwargs):
            return 0.8

        def func2(completion, **kwargs):
            return 0.6

        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(funcs=[func1], weights=[1.0]),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(funcs=[func2], weights=[1.0]),
        )

        env_map = {"math": env1, "code": env2}
        rubric = EnvGroupRubric(env_map)

        # Test scoring for "math" task
        result = await rubric.score_rollout(
            prompt="Test prompt",
            completion="Test completion",
            answer="Test answer",
            state={},
            task="math",
        )

        assert "func1" in result.metrics
        assert "func2" in result.metrics
        assert result.metrics["func1"] == 0.8  # From env1
        assert result.metrics["func2"] == 0.0  # Not in env1, so 0.0
        assert result.reward == 0.8

    @pytest.mark.asyncio
    async def test_env_group_rubric_unknown_task(self, mock_openai_client):
        """Test scoring with unknown task returns zeros."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env_map = {"known_task": env1}
        rubric = EnvGroupRubric(env_map)

        result = await rubric.score_rollout(
            prompt="Test", completion="Test", task="unknown_task"
        )

        assert result.reward == 0.0


class TestEnvGroup:
    """Test cases for the EnvGroup class."""

    def test_env_group_initialization(self, mock_openai_client):
        """Test EnvGroup initialization with multiple environments."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2])

        assert len(env_group.envs) == 2
        assert env_group.env_names == ["env_0", "env_1"]
        assert env_group.env_map["env_0"] == env1
        assert env_group.env_map["env_1"] == env2

    def test_env_group_with_custom_names(self, mock_openai_client):
        """Test EnvGroup with custom environment names."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2], env_names=["math", "code"])

        assert env_group.env_names == ["math", "code"]
        assert env_group.env_map["math"] == env1
        assert env_group.env_map["code"] == env2

    def test_env_group_empty_envs_fails(self):
        """Test that EnvGroup fails with empty environments list."""
        with pytest.raises(
            ValueError, match="EnvGroup requires at least one environment"
        ):
            EnvGroup(envs=[])

    def test_env_group_mismatched_names_fails(self, mock_openai_client):
        """Test that EnvGroup fails when env_names length doesn't match envs."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        with pytest.raises(
            ValueError, match="Number of env_names must match number of envs"
        ):
            EnvGroup(envs=[env1], env_names=["math", "code"])

    def test_env_group_dataset_concatenation(self, mock_openai_client):
        """Test that EnvGroup properly concatenates datasets with task labels."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict(
                {"question": ["q1", "q2"], "answer": ["a1", "a2"]}
            ),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q3"], "answer": ["a3"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2], env_names=["math", "code"])

        # Check concatenated dataset
        dataset = env_group.get_dataset()
        assert len(dataset) == 3
        assert "task" in dataset.column_names

        # Check task labels
        tasks = dataset["task"]
        assert tasks[0] == "math"
        assert tasks[1] == "math"
        assert tasks[2] == "code"

    def test_env_group_rubric_type(self, mock_openai_client):
        """Test that EnvGroup creates EnvGroupRubric."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1])

        assert isinstance(env_group.rubric, EnvGroupRubric)
        assert env_group.rubric.env_map["env_0"] == env1

    @pytest.mark.asyncio
    async def test_env_group_rollout_routing(self, mock_openai_client):
        """Test that rollout is properly routed to the correct sub-environment."""
        # Create environments with different behaviors
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(),
        )

        # Mock the rollout methods to return different values
        env1.rollout = AsyncMock(return_value=("response1", {"env": "env1"}))
        env2.rollout = AsyncMock(return_value=("response2", {"env": "env2"}))

        env_group = EnvGroup(envs=[env1, env2], env_names=["math", "code"])

        # Test routing to math environment
        result1, state1 = await env_group.rollout(
            client=mock_openai_client,
            model="test-model",
            prompt="Test prompt",
            task="math",
        )

        assert result1 == "response1"
        assert state1["env"] == "env1"
        env1.rollout.assert_called_once()
        env2.rollout.assert_not_called()

        # Reset mocks
        env1.rollout.reset_mock()
        env2.rollout.reset_mock()

        # Test routing to code environment
        result2, state2 = await env_group.rollout(
            client=mock_openai_client,
            model="test-model",
            prompt="Test prompt",
            task="code",
        )

        assert result2 == "response2"
        assert state2["env"] == "env2"
        env1.rollout.assert_not_called()
        env2.rollout.assert_called_once()

    def test_get_env_for_task(self, mock_openai_client):
        """Test getting environment for a specific task."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2], env_names=["math", "code"])

        assert env_group.get_env_for_task("math") == env1
        assert env_group.get_env_for_task("code") == env2
        # Unknown task returns first environment as fallback
        assert env_group.get_env_for_task("unknown") == env1

    @pytest.mark.asyncio
    async def test_env_group_generate(self, mock_openai_client):
        """Test generate method with EnvGroup."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2], env_names=["math", "code"])

        # Mock the scoring
        env_group.rubric.score_rollouts = AsyncMock(
            return_value=RolloutScores(reward=[0.8, 0.9], metrics={})
        )

        inputs = {
            "prompt": [
                [{"role": "user", "content": "Math question"}],
                [{"role": "user", "content": "Code question"}],
            ],
            "answer": ["math_answer", "code_answer"],
            "task": ["math", "code"],
        }

        results = await env_group.a_generate(
            inputs, client=mock_openai_client, model="test-model"
        )

        assert hasattr(results, "completion")
        assert hasattr(results, "state")
        assert hasattr(results, "reward")
        assert len(results.completion) == 2

    def test_env_group_with_mixed_datasets(self, mock_openai_client):
        """Test EnvGroup with environments having different dataset configurations."""
        # Environment with both train and eval datasets
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            eval_dataset=Dataset.from_dict({"question": ["eq1"], "answer": ["ea1"]}),
            rubric=Rubric(),
        )

        # Environment with only eval dataset
        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            eval_dataset=Dataset.from_dict({"question": ["eq2"], "answer": ["ea2"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2], env_names=["task1", "task2"])

        # Should have concatenated train dataset from both envs
        train_dataset = env_group.get_dataset()
        assert len(train_dataset) == 2
        assert train_dataset["task"][0] == "task1"
        assert train_dataset["task"][1] == "task2"

        # Should have concatenated eval datasets from both
        eval_dataset = env_group.get_eval_dataset()
        assert len(eval_dataset) == 2
        assert eval_dataset["task"][0] == "task1"
        assert eval_dataset["task"][1] == "task2"



================================================
FILE: tests/test_environment.py
================================================
"""Tests for the base Environment class."""

from unittest.mock import AsyncMock, Mock

import pytest
from datasets import Dataset

from verifiers import Environment, Parser, Rubric
from verifiers.types import GenerateOutputs, RolloutScores


# Create a concrete implementation for testing the abstract base class
class SimpleEnvironment(Environment):
    """Simple implementation of Environment for testing."""

    async def rollout(
        self,
        client,
        model,
        prompt,
        answer="",
        task="default",
        info={},
        sampling_args={},
        **kwargs,
    ):
        """Simple test rollout implementation."""
        response = await self.get_model_response(
            prompt=prompt, client=client, model=model, sampling_args=sampling_args
        )
        if self.message_type == "chat":
            completion = [
                {"role": "assistant", "content": response.choices[0].message.content}
            ]
            state = {"responses": [response]}
        else:
            completion = response.choices[0].text
            state = {}
        return completion, state


class TestEnvironmentBase:
    """Test cases for the base Environment class."""

    def test_environment_initialization(self, mock_openai_client, sample_dataset):
        """Test that Environment initializes correctly."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )
        assert env.client == mock_openai_client
        assert env.model == "test-model"
        assert env.message_type == "chat"
        assert isinstance(env.parser, Parser)
        assert isinstance(env.rubric, Rubric)

    def test_environment_with_eval_dataset_only(
        self, mock_openai_client, sample_dataset
    ):
        """Test Environment with only eval_dataset."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )
        assert env.dataset is None
        assert env.eval_dataset is not None

    def test_environment_no_datasets_raises_error(self, mock_openai_client):
        """Test that Environment raises error when no datasets provided."""
        with pytest.raises(
            ValueError, match="Either dataset or eval_dataset must be provided"
        ):
            SimpleEnvironment(
                client=mock_openai_client,
                model="test-model",
                parser=Parser(),
                rubric=Rubric(),
            )

    def test_completion_mode_with_system_prompt_raises_error(
        self, mock_openai_client, sample_dataset
    ):
        """Test that completion mode with system prompt raises error."""
        with pytest.raises(ValueError, match="not supported for completion tasks"):
            SimpleEnvironment(
                client=mock_openai_client,
                model="test-model",
                dataset=sample_dataset,
                message_type="completion",
                system_prompt="test prompt",
                parser=Parser(),
                rubric=Rubric(),
            )

    def test_format_prompt(self, mock_openai_client, sample_dataset):
        """Test prompt formatting."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )

        prompt = "What is 2+2?"
        system_prompt = "You are a helpful assistant."
        few_shot = [
            {"role": "user", "content": "What is 1+1?"},
            {"role": "assistant", "content": "2"},
        ]

        formatted = env.format_prompt(prompt, system_prompt, few_shot)

        assert len(formatted) == 4
        assert formatted[0]["role"] == "system"
        assert formatted[0]["content"] == system_prompt
        assert formatted[1]["role"] == "user"
        assert formatted[1]["content"] == "What is 1+1?"
        assert formatted[2]["role"] == "assistant"
        assert formatted[2]["content"] == "2"
        assert formatted[3]["role"] == "user"
        assert formatted[3]["content"] == prompt

    def test_get_dataset(self, mock_openai_client, sample_dataset):
        """Test dataset retrieval."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )

        # Get full dataset
        full_dataset = env.get_dataset()
        assert len(full_dataset) == 2

        # Get subset
        subset = env.get_dataset(n=1)
        assert len(subset) == 1

    @pytest.mark.asyncio
    async def test_get_model_response_chat(self, mock_openai_client):
        """Test get_model_response with chat format."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["test"], "answer": ["test"]}),
            parser=Parser(),
            rubric=Rubric(),
        )

        prompt = [{"role": "user", "content": "Hello"}]
        response = await env.get_model_response(
            prompt=prompt,
            client=mock_openai_client,
            model="test-model",
            message_type="chat",
        )

        # Check response structure
        assert hasattr(response, "choices")
        assert len(response.choices) > 0
        assert hasattr(response.choices[0], "message")
        assert hasattr(response.choices[0].message, "content")
        mock_openai_client.chat.completions.create.assert_called_once()

    @pytest.mark.asyncio
    async def test_get_model_response_completion(self, mock_openai_client):
        """Test get_model_response with completion format."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"prompt": ["test"], "answer": ["test"]}),
            message_type="completion",
            parser=Parser(),
            rubric=Rubric(),
        )

        prompt = "Complete this:"
        response = await env.get_model_response(
            prompt=prompt,
            client=mock_openai_client,
            model="test-model",
            message_type="completion",
        )

        # Check response structure
        assert hasattr(response, "choices")
        assert len(response.choices) > 0
        assert hasattr(response.choices[0], "text")
        mock_openai_client.completions.create.assert_called_once()

    def test_process_chat_format(self, mock_openai_client, sample_dataset):
        """Test processing chat format conversations."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )

        # Create a mock tokenizer
        mock_tokenizer = Mock()

        def apply_template(conversation, tokenize=False, add_generation_prompt=True):
            # Return deterministic token ids ensuring prefix property
            return list(range(10 if add_generation_prompt else 14))

        mock_tokenizer.apply_chat_template = Mock(side_effect=apply_template)
        mock_tokenizer.encode = Mock(side_effect=lambda text: list(range(len(text))))

        prompt = [{"role": "user", "content": "What is 2+2?"}]
        completion = [{"role": "assistant", "content": "4"}]
        # Minimal vLLM-style chat completion with tokens/logprobs
        token_entries = [
            Mock(logprob=-0.1, token="token_id:11"),
            Mock(logprob=-0.2, token="token_id:12"),
            Mock(logprob=-0.3, token="token_id:13"),
            Mock(logprob=-0.4, token="token_id:14"),
        ]
        mock_choice = Mock()
        mock_choice.logprobs = Mock()
        mock_choice.logprobs.content = token_entries
        mock_chat_completion = Mock()
        mock_chat_completion.choices = [mock_choice]
        state = {"responses": [mock_chat_completion]}

        (
            prompt_ids,
            prompt_mask,
            completion_ids,
            completion_mask,
            completion_logprobs,
        ) = env.process_chat_format_vllm(
            prompt, completion, state, mock_tokenizer, mask_env_responses=False
        )

        assert isinstance(prompt_ids, list)
        assert isinstance(prompt_mask, list)
        assert isinstance(completion_ids, list)
        assert isinstance(completion_mask, list)
        assert len(prompt_ids) == len(prompt_mask)
        assert len(completion_ids) == len(completion_mask)
        assert all(m == 0 for m in prompt_mask)  # Prompt mask should be all 0s
        assert all(m == 1 for m in completion_mask)  # Completion mask should be all 1s

    def test_process_completion_format(self, mock_openai_client, sample_dataset):
        """Test processing completion format text."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )

        # Create a mock tokenizer
        mock_tokenizer = Mock()
        mock_tokenizer.encode = Mock(side_effect=lambda text: list(range(len(text))))

        prompt = "Complete this: 2+2="
        completion = "4"
        # Minimal vLLM-style completion covering entire completion string
        mock_choice = Mock()
        mock_choice.text = completion
        mock_choice.logprobs = Mock()
        mock_choice.logprobs.tokens = ["token_id:1"] * len(completion)
        mock_choice.logprobs.token_logprobs = [-0.1] * len(completion)
        mock_completion = Mock()
        mock_completion.choices = [mock_choice]
        state = {"responses": [mock_completion], "responses_start_idx": [0]}

        (
            prompt_ids,
            prompt_mask,
            completion_ids,
            completion_mask,
            completion_logprobs,
        ) = env.process_completion_format_vllm(
            prompt, completion, state, mock_tokenizer
        )

        assert isinstance(prompt_ids, list)
        assert isinstance(prompt_mask, list)
        assert isinstance(completion_ids, list)
        assert isinstance(completion_mask, list)
        assert len(prompt_ids) == len(prompt)
        assert len(completion_ids) == len(completion)
        assert all(m == 0 for m in prompt_mask)
        assert all(m == 1 for m in completion_mask)

    def test_process_env_results_chat(self, mock_openai_client, sample_dataset):
        """Test processing environment results for chat format."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )

        # Create a mock tokenizer
        mock_tokenizer = Mock()

        # Track the conversation state
        def mock_apply_chat_template(
            conversation, tokenize=False, add_generation_prompt=True
        ):
            # Return token id list; size scales with number of messages
            return list(range(len(conversation) * 5))

        def mock_encode(text, **kwargs):
            return list(range(3))

        mock_tokenizer.apply_chat_template = Mock(side_effect=mock_apply_chat_template)
        mock_tokenizer.encode = Mock(side_effect=mock_encode)

        prompts = [[{"role": "user", "content": "Hello"}]]
        completions = [[{"role": "assistant", "content": "Hi there!"}]]
        # Minimal vLLM-style chat completion mock for assistant turn
        token_entries = [
            Mock(logprob=-0.1, token="token_id:101"),
            Mock(logprob=-0.2, token="token_id:102"),
            Mock(logprob=-0.3, token="token_id:103"),
        ]
        mock_choice = Mock()
        mock_choice.logprobs = Mock()
        mock_choice.logprobs.content = token_entries
        mock_chat_completion = Mock()
        mock_chat_completion.choices = [mock_choice]
        states = [{"responses": [mock_chat_completion]}]
        rewards = [1.0]

        results = env.process_env_results_vllm(
            prompts, completions, states, rewards, mock_tokenizer
        )

        assert hasattr(results, "prompt_ids")
        assert hasattr(results, "prompt_mask")
        assert hasattr(results, "completion_ids")
        assert hasattr(results, "completion_mask")
        assert hasattr(results, "completion_logprobs")
        assert hasattr(results, "rewards")
        assert len(results.rewards) == 1
        assert results.rewards[0] == 1.0

    def test_process_env_results_with_truncation(
        self, mock_openai_client, sample_dataset
    ):
        """Test processing environment results with sequence length truncation."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )

        # Create a mock tokenizer
        mock_tokenizer = Mock()

        # Track the conversation state
        def mock_apply_chat_template(
            conversation, tokenize=False, add_generation_prompt=True
        ):
            # Return deterministic token ids based on number of messages
            # Ensures prefix property when conversation grows
            return list(range(len(conversation) * 5))

        def mock_encode(text, **kwargs):
            # Not used by chat path; keep for compatibility
            return list(range(3))

        mock_tokenizer.apply_chat_template = Mock(side_effect=mock_apply_chat_template)
        mock_tokenizer.encode = Mock(side_effect=mock_encode)

        prompts = [[{"role": "user", "content": "Hello"}]]
        completions = [[{"role": "assistant", "content": "Hi there!"}]]
        # Produce enough assistant tokens to force truncation
        token_entries = [
            Mock(logprob=-0.1, token="token_id:201"),
            Mock(logprob=-0.2, token="token_id:202"),
            Mock(logprob=-0.3, token="token_id:203"),
            Mock(logprob=-0.4, token="token_id:204"),
            Mock(logprob=-0.5, token="token_id:205"),
            Mock(logprob=-0.6, token="token_id:206"),
        ]
        mock_choice = Mock()
        mock_choice.logprobs = Mock()
        mock_choice.logprobs.content = token_entries
        mock_chat_completion = Mock()
        mock_chat_completion.choices = [mock_choice]
        states = [{"responses": [mock_chat_completion]}]
        rewards = [1.0]

        results = env.process_env_results_vllm(
            prompts,
            completions,
            states,
            rewards,
            mock_tokenizer,
            max_seq_len=8,  # Force truncation
            mask_truncated_completions=True,
        )

        # Check that total length respects max_seq_len
        total_len = len(results.prompt_ids[0]) + len(results.completion_ids[0])
        assert total_len <= 8
        # Check that truncated completion is masked
        assert all(m == 0 for m in results.completion_mask[0])

    def test_parse_chat_completion_logprobs(self, mock_openai_client, sample_dataset):
        """Test parsing logprobs from a vLLM chat completion."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )

        # Create mock chat completion with logprobs
        mock_completion = Mock()
        mock_completion.choices = [Mock()]
        mock_completion.choices[0].logprobs = Mock()
        mock_completion.choices[0].logprobs.content = [
            Mock(logprob=-0.5),
            Mock(logprob=-1.2),
            Mock(logprob=-0.3),
        ]

        logprobs = env.parse_chat_completion_logprobs(mock_completion)
        assert logprobs == [-0.5, -1.2, -0.3]

    def test_parse_chat_completion_tokens(self, mock_openai_client, sample_dataset):
        """Test parsing tokens from a vLLM chat completion."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )

        # Create mock chat completion with tokens
        mock_completion = Mock()
        mock_completion.choices = [Mock()]
        mock_completion.choices[0].logprobs = Mock()
        mock_completion.choices[0].logprobs.content = [
            Mock(token="id:1234"),
            Mock(token="id:5678"),
            Mock(token="id:9012"),
        ]

        tokens = env.parse_chat_completion_tokens(mock_completion)
        assert tokens == [1234, 5678, 9012]

    @pytest.mark.asyncio
    async def test_run_rollouts(self, mock_openai_client):
        """Test running multiple rollouts."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["test"], "answer": ["test"]}),
            parser=Parser(),
            rubric=Rubric(),
        )

        prompts = [
            [{"role": "user", "content": "Hello"}],
            [{"role": "user", "content": "Hi"}],
        ]
        answers = ["response1", "response2"]
        tasks = ["default", "default"]
        infos = [{}, {}]

        # Mock the rollout method calls
        results = await env.run_rollouts(
            client=mock_openai_client,
            model="test-model",
            prompts=prompts,
            answers=answers,
            tasks=tasks,
            infos=infos,
        )

        assert len(results) == 2
        assert all(
            len(result) == 2 for result in results
        )  # Each result is (completion, state)

    @pytest.mark.asyncio
    async def test_a_generate_with_score_rollouts(
        self, mock_openai_client, sample_dataset
    ):
        """Test async generate with scoring enabled."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )

        # Mock the rubric scoring
        env.rubric.score_rollouts = AsyncMock(
            return_value=RolloutScores(reward=[1.0], metrics={})
        )

        inputs = {"prompt": [[{"role": "user", "content": "Hello"}]], "answer": ["Hi"]}

        results = await env.a_generate(inputs, score_rollouts=True)

        assert hasattr(results, "completion")
        assert hasattr(results, "state")
        assert hasattr(results, "reward")
        assert results.reward == [1.0]

    def test_generate_sync_wrapper(self, mock_openai_client, sample_dataset):
        """Test synchronous generate wrapper."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )

        # Mock the rubric scoring
        env.rubric.score_rollouts = AsyncMock(
            return_value=RolloutScores(reward=[1.0], metrics={})
        )

        inputs = {"prompt": [[{"role": "user", "content": "Hello"}]], "answer": ["Hi"]}

        results = env.generate(inputs, client=env.client)

        assert hasattr(results, "completion")
        assert hasattr(results, "state")
        assert hasattr(results, "reward")

    def test_make_dataset(self, mock_openai_client, sample_dataset):
        """Test creating a dataset from evaluation results."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )

        results = GenerateOutputs(
            prompt=[[{"role": "user", "content": "Hello"}]],
            completion=[[{"role": "assistant", "content": "Hi"}]],
            answer=["Hi"],
            reward=[1.0],
            task=["default"],
            state=[{"custom_field": "value"}],
            info=[{}],
            metrics={},
        )

        dataset = env.make_dataset(results, state_columns=["custom_field"])

        assert len(dataset) == 1
        assert "prompt" in dataset.column_names
        assert "completion" in dataset.column_names
        assert "answer" in dataset.column_names
        assert "reward" in dataset.column_names
        assert "task" in dataset.column_names
        assert "custom_field" in dataset.column_names



================================================
FILE: tests/test_environment_extra.py
================================================
"""Additional tests for verifiers.envs.environment.Environment.

Covers:
- get_model_response chat tools vs. completion error
- run_rollouts with semaphore
- process_env_results zero_truncated_completions path
- evaluate fallback to train dataset and repeat behavior
- generate called inside an existing event loop
- make_dataset tool call sanitization
"""

from __future__ import annotations

import asyncio
from typing import List

import pytest
from datasets import Dataset

from verifiers.envs.environment import Environment
from verifiers.parsers.parser import Parser
from verifiers.rubrics.rubric import Rubric
from verifiers.types import GenerateOutputs, Info, Messages, SamplingArgs
from verifiers.utils.message_utils import sanitize_tool_calls


# Local simple concrete Environment for testing
class DummyEnvironment(Environment):
    async def rollout(
        self,
        client,
        model,
        prompt: Messages,
        answer: str = "",
        task: str = "default",
        info: Info | None = {},
        sampling_args: SamplingArgs | None = None,
        **kwargs,
    ):
        response = await self.get_model_response(
            prompt=prompt, client=client, model=model, sampling_args=sampling_args
        )
        assert response is not None
        if self.message_type == "chat":
            completion = [
                {"role": "assistant", "content": response.choices[0].message.content}
            ]
            state = {"responses": [response]}
        else:
            completion = response.choices[0].text
            state = {}
        return completion, state


def _make_env(
    mock_openai_client, dataset: Dataset | None = None, **kwargs
) -> DummyEnvironment:
    ds = dataset or Dataset.from_dict({"question": ["q1"], "answer": ["a1"]})
    return DummyEnvironment(
        client=mock_openai_client,
        model="test-model",
        dataset=ds,
        parser=Parser(),
        rubric=Rubric(),
        **kwargs,
    )


@pytest.mark.asyncio
async def test_get_model_response_chat_with_tools(mock_openai_client):
    env = _make_env(mock_openai_client)
    prompt = [{"role": "user", "content": "Hello"}]
    tools = [
        {
            "type": "function",
            "function": {"name": "echo", "description": "echo", "parameters": {}},
        }
    ]
    resp = await env.get_model_response(
        client=mock_openai_client,
        model="test-model",
        prompt=prompt,
        oai_tools=tools,
        message_type="chat",
    )
    # Ensure the client was invoked and received tools kwarg
    assert hasattr(resp, "choices")
    assert mock_openai_client.chat.completions.create.await_count == 1
    kwargs = mock_openai_client.chat.completions.create.await_args.kwargs
    assert "tools" in kwargs and kwargs["tools"] == tools


@pytest.mark.asyncio
async def test_get_model_response_completion_rejects_tools(mock_openai_client):
    env = _make_env(mock_openai_client, message_type="completion")
    with pytest.raises(ValueError, match="oai_tools are not supported for completion"):
        await env.get_model_response(
            client=mock_openai_client,
            model="test-model",
            prompt="Complete this",
            oai_tools=[{"type": "function", "function": {"name": "noop"}}],
            message_type="completion",
        )


def test_run_rollouts_with_semaphore(mock_openai_client):
    env = _make_env(mock_openai_client)
    prompts = [[{"role": "user", "content": "hi"}] for _ in range(3)]
    answers = ["", "", ""]
    coro = env.run_rollouts(
        client=mock_openai_client,
        model="test-model",
        prompts=prompts,
        answers=answers,
        tasks=["default"] * 3,
        infos=[{}] * 3,
        max_concurrent=2,
    )
    results: List = asyncio.run(coro)
    assert len(results) == 3


def test_process_env_results_zero_truncated_reward_vllm(mock_openai_client):
    # Use pre-formatted dataset to avoid map/progress side effects in test
    ds = Dataset.from_dict(
        {
            "prompt": [[{"role": "user", "content": "q"}]],
            "answer": ["a"],
        }
    )
    env = _make_env(mock_openai_client, dataset=ds, message_type="completion")

    # Mock tokenizer: encode maps length to token list
    class Tok:
        def encode(self, text, **kwargs):
            return list(range(len(text)))

    prompts = ["Hello!"]  # 6 tokens
    completions = ["World!!!"]  # 8 tokens
    # Minimal vLLM-style completion response covering entire completion text
    mock_choice = type("C", (), {})()
    mock_choice.text = completions[0]
    mock_choice.logprobs = type("LP", (), {})()
    mock_choice.logprobs.tokens = ["token_id:1"] * len(completions[0])
    mock_choice.logprobs.token_logprobs = [-0.1] * len(completions[0])
    mock_completion = type("R", (), {})()
    mock_completion.choices = [mock_choice]
    states = [{"responses": [mock_completion], "responses_start_idx": [0]}]
    rewards = [1.0]

    out = env.process_env_results_vllm(
        prompts,
        completions,
        states,
        rewards,
        Tok(),
        max_seq_len=10,  # force truncation (6 + 8 > 10)
        mask_truncated_completions=True,
        zero_truncated_completions=True,
    )

    assert out.rewards == [0.0]
    assert len(out.prompt_ids[0]) + len(out.completion_ids[0]) <= 10
    print("end_zero_truncated")


def test_evaluate_fallback_and_repeat(mock_openai_client):
    # No eval_dataset provided -> falls back to train; ensure >= num_examples
    from datasets import Dataset

    ds = Dataset.from_dict({"question": ["q1", "q2"], "answer": ["a1", "a2"]})
    env = _make_env(mock_openai_client, dataset=ds)
    res = env.evaluate(
        client=mock_openai_client,
        model="test-model",
        num_examples=2,
        rollouts_per_example=2,
        score_rollouts=False,
    )
    # Expect n * r rollouts in outputs
    assert len(res.prompt) == 2 * 2
    assert len(res.completion) == 2 * 2


@pytest.mark.asyncio
async def test_generate_inside_running_loop(mock_openai_client):
    env = _make_env(mock_openai_client)
    inputs = {"prompt": [[{"role": "user", "content": "Hi"}]], "answer": [""]}
    # Call the async API directly inside a running event loop to avoid nested sync wrapper issues
    out = await env.a_generate(inputs, client=env.client)
    assert hasattr(out, "completion") and len(out.completion) == 1


def test_sanitize_tool_calls_outputs_strings():
    # Use a lightweight object with model_dump to mimic OAI tool call
    class ToolCall:
        def __init__(self, name: str, args: str):
            self.function = type("F", (), {"name": name, "arguments": args})()

        def model_dump(self):
            return {
                "id": "x",
                "type": "function",
                "function": {
                    "name": self.function.name,
                    "arguments": self.function.arguments,
                },
            }

    msgs = [
        [{"role": "assistant", "content": "", "tool_calls": [ToolCall("echo", "{}")]}]
    ]
    sanitized = sanitize_tool_calls(msgs[0])
    assert isinstance(sanitized[0]["tool_calls"][0], str)


def test_make_dataset_basic_without_tools(mock_openai_client):
    env = _make_env(mock_openai_client)
    results = GenerateOutputs(
        prompt=[[{"role": "user", "content": "Hi"}]],
        completion=[[{"role": "assistant", "content": "Hello"}]],
        answer=[""],
        state=[{}],
        info=[{}],
        task=["default"],
        reward=[1.0],
        metrics={"foo": [0.1]},
    )
    ds = env.make_dataset(results)
    assert len(ds) == 1 and "foo" in ds.column_names


def test_truncation_masks_completion_format_vllm(mock_openai_client):
    # Duplicate of zero_truncated test under a different name to avoid any runner quirk
    ds = Dataset.from_dict(
        {
            "prompt": [[{"role": "user", "content": "q"}]],
            "answer": ["a"],
        }
    )
    env = _make_env(mock_openai_client, dataset=ds, message_type="completion")

    class Tok:
        def encode(self, text, **kwargs):
            return list(range(len(text)))

    prompts = ["Hello!"]
    completions = ["World!!!"]
    # Minimal vLLM-style completion response covering entire completion text
    mock_choice2 = type("C2", (), {})()
    mock_choice2.text = completions[0]
    mock_choice2.logprobs = type("LP2", (), {})()
    mock_choice2.logprobs.tokens = ["token_id:1"] * len(completions[0])
    mock_choice2.logprobs.token_logprobs = [-0.1] * len(completions[0])
    mock_completion2 = type("R2", (), {})()
    mock_completion2.choices = [mock_choice2]
    out = env.process_env_results_vllm(
        prompts,
        completions,
        [{"responses": [mock_completion2], "responses_start_idx": [0]}],
        [1.0],
        Tok(),
        max_seq_len=10,
        mask_truncated_completions=True,
        zero_truncated_completions=True,
    )
    assert out.rewards == [0.0]
    assert len(out.prompt_ids[0]) + len(out.completion_ids[0]) <= 10



================================================
FILE: tests/test_eval_cli.py
================================================
import verifiers.scripts.eval as vf_eval


def _make_fake_env(captured):
    class FakeEnv:
        def evaluate(
            self,
            client,
            model,
            sampling_args=None,
            num_examples=-1,
            rollouts_per_example=1,
            **kwargs,
        ):
            captured["sampling_args"] = dict(sampling_args or {})

            class Result:
                prompt = ["p"]
                completion = ["c"]
                reward = [1.0]
                info = [{}]
                task = ["default"]
                answer = [""]
                metrics = {}

            return Result()

    return FakeEnv()


def test_cli_sampling_args_precedence_over_flags(monkeypatch):
    captured = {}

    # Patch environment loader to return our fake env
    monkeypatch.setattr(
        vf_eval.vf,
        "load_environment",
        lambda env_id, **env_args: _make_fake_env(captured),
    )

    # Patch OpenAI client used by the CLI to a simple dummy
    class DummyOpenAI:
        def __init__(self, api_key=None, base_url=None):
            self.api_key = api_key
            self.base_url = base_url

    monkeypatch.setattr(vf_eval, "OpenAI", DummyOpenAI)

    # Run evaluation with JSON sampling args overriding flags
    vf_eval.eval_environment(
        env="dummy-env",
        env_args={},
        env_dir_path="./environments",
        endpoints_path="./configs/endpoints.py",
        model="gpt-4.1-mini",
        api_key_var="OPENAI_API_KEY",
        api_base_url="https://api.openai.com/v1",
        num_examples=1,
        rollouts_per_example=1,
        max_concurrent_requests=1,
        max_tokens=42,
        temperature=0.9,
        sampling_args={
            "enable_thinking": False,
            "max_tokens": 77,
            "temperature": 0.1,
        },
        verbose=False,
        save_dataset=False,
        save_to_hf_hub=False,
        hf_hub_dataset_name="",
    )

    sa = captured["sampling_args"]
    assert sa["max_tokens"] == 77
    assert sa["temperature"] == 0.1
    assert sa["enable_thinking"] is False


def test_cli_sampling_args_fill_from_flags_when_missing(monkeypatch):
    captured = {}

    # Patch environment loader to return our fake env
    monkeypatch.setattr(
        vf_eval.vf,
        "load_environment",
        lambda env_id, **env_args: _make_fake_env(captured),
    )

    # Patch OpenAI client used by the CLI to a simple dummy
    class DummyOpenAI:
        def __init__(self, api_key=None, base_url=None):
            self.api_key = api_key
            self.base_url = base_url

    monkeypatch.setattr(vf_eval, "OpenAI", DummyOpenAI)

    # Run evaluation with JSON lacking max_tokens/temperature
    vf_eval.eval_environment(
        env="dummy-env",
        env_args={},
        env_dir_path="./environments",
        endpoints_path="./configs/endpoints.py",
        model="gpt-4.1-mini",
        api_key_var="OPENAI_API_KEY",
        api_base_url="https://api.openai.com/v1",
        num_examples=1,
        rollouts_per_example=1,
        max_concurrent_requests=1,
        max_tokens=55,
        temperature=0.8,
        sampling_args={
            "enable_thinking": True,
        },
        verbose=False,
        save_dataset=False,
        save_to_hf_hub=False,
        hf_hub_dataset_name="",
    )

    sa = captured["sampling_args"]
    assert sa["max_tokens"] == 55
    assert sa["temperature"] == 0.8
    assert sa["enable_thinking"] is True



================================================
FILE: tests/test_multiturn_env.py
================================================
"""Tests for the MultiTurnEnv class."""

import pytest
from datasets import Dataset

from verifiers import MultiTurnEnv, Parser, Rubric


class TestMultiTurnEnv:
    """Test cases for the MultiTurnEnv class."""

    def test_multiturn_env_initialization(self, mock_multiturn_env):
        """Test MultiTurnEnv initialization."""
        assert mock_multiturn_env.max_turns == 3
        assert mock_multiturn_env.message_type == "chat"  # Default from parent

    def test_multiturn_env_default_max_turns(
        self, mock_openai_client, sample_chat_dataset
    ):
        """Test MultiTurnEnv default max_turns value."""
        from tests.conftest import SimpleMultiTurnEnv

        env = SimpleMultiTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_chat_dataset,
            parser=Parser(),
            rubric=Rubric(),
        )
        assert env.max_turns == 10  # Default value

    @pytest.mark.asyncio
    async def test_basic_multiturn_rollout(self, mock_multiturn_env):
        """Test basic multi-turn conversation that completes normally."""
        # Configure mock to return responses that lead to completion
        prompt = [{"role": "user", "content": "Start conversation"}]

        # Set up responses for the conversation turns
        mock_multiturn_env.client.add_chat_response(
            messages=[{"role": "user", "content": "Start conversation"}],
            response="First response",
        )
        mock_multiturn_env.client.add_chat_response(
            messages=[
                {"role": "user", "content": "Start conversation"},
                {"role": "assistant", "content": "First response"},
                {"role": "user", "content": "Continue (turn 1)"},
            ],
            response="Second response",
        )
        mock_multiturn_env.client.add_chat_response(
            messages=[
                {"role": "user", "content": "Start conversation"},
                {"role": "assistant", "content": "First response"},
                {"role": "user", "content": "Continue (turn 1)"},
                {"role": "assistant", "content": "Second response"},
                {"role": "user", "content": "Please finish with DONE"},
            ],
            response="Final response DONE",
        )

        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model="test-model",
            prompt=prompt,
            answer="target_answer",
        )

        # Should have: assistant + user + assistant + user + assistant
        assert len(completion) == 5
        assert completion[0]["role"] == "assistant"
        assert completion[0]["content"] == "First response"
        assert completion[1]["role"] == "user"
        assert completion[2]["role"] == "assistant"
        assert completion[2]["content"] == "Second response"
        assert completion[4]["content"] == "Final response DONE"

        # Check state structure
        assert state["answer"] == "target_answer"
        assert state["prompt"] == prompt
        # state["completion"] is initialized to [] but not updated during rollout
        assert state["completion"] == []
        assert "responses" in state
        assert len(state["responses"]) == 3  # Three assistant responses

    @pytest.mark.asyncio
    async def test_max_turns_limiting(self, mock_multiturn_env_max_turns):
        """Test that rollout stops at max_turns."""
        # Set up responses that would continue indefinitely
        mock_multiturn_env_max_turns.client.set_default_responses(
            chat_response="Keep going"
        )

        prompt = [{"role": "user", "content": "Start conversation"}]
        completion, state = await mock_multiturn_env_max_turns.rollout(
            client=mock_multiturn_env_max_turns.client,
            model="test-model",
            prompt=prompt,
            answer="target_answer",
        )

        # Should stop at max_turns=2: assistant + user + assistant (3 messages)
        assert len(completion) == 3
        assert completion[0]["role"] == "assistant"
        assert completion[1]["role"] == "user"
        assert completion[2]["role"] == "assistant"
        assert len(state["responses"]) == 2  # Two assistant responses

    @pytest.mark.asyncio
    async def test_state_initialization(self, mock_multiturn_env):
        """Test that state is properly initialized with all required fields."""
        mock_multiturn_env.client.add_chat_response(
            messages=[{"role": "user", "content": "Test state"}], response="Quick DONE"
        )

        prompt = [{"role": "user", "content": "Test state"}]
        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model="test-model",
            prompt=prompt,
            answer="test_answer",
            task="test_task",
            info={"extra": "data"},
        )

        # Check all state fields are initialized
        assert state["prompt"] == prompt
        # state["completion"] is initialized to [] but not updated during rollout
        assert state["completion"] == []
        assert state["answer"] == "test_answer"
        assert state["task"] == "test_task"
        assert state["info"] == {"extra": "data"}
        assert "responses" in state
        assert isinstance(state["responses"], list)

    @pytest.mark.asyncio
    async def test_immediate_completion(self, mock_multiturn_env):
        """Test completion detection on first turn."""
        mock_multiturn_env.client.add_chat_response(
            messages=[{"role": "user", "content": "Quick question"}],
            response="Immediate DONE",
        )

        prompt = [{"role": "user", "content": "Quick question"}]
        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model="test-model",
            prompt=prompt,
            answer="target_answer",
        )

        # Should complete immediately
        assert len(completion) == 1
        assert completion[0]["content"] == "Immediate DONE"
        assert len(state["responses"]) == 1

    @pytest.mark.asyncio
    async def test_env_response_integration(self, mock_multiturn_env):
        """Test that environment responses are properly integrated."""
        # Set up responses for the conversation turns
        mock_multiturn_env.client.add_chat_response(
            messages=[{"role": "user", "content": "Start conversation"}],
            response="First response",
        )
        mock_multiturn_env.client.add_chat_response(
            messages=[
                {"role": "user", "content": "Start conversation"},
                {"role": "assistant", "content": "First response"},
                {"role": "user", "content": "Continue (turn 1)"},
            ],
            response="Final response DONE",
        )

        prompt = [{"role": "user", "content": "Start conversation"}]
        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model="test-model",
            prompt=prompt,
            answer="target_answer",
        )

        # Verify environment responses are included
        assert len(completion) >= 3
        user_messages = [msg for msg in completion if msg["role"] == "user"]
        assert len(user_messages) >= 1
        assert "Continue (turn 1)" in user_messages[0]["content"]

    @pytest.mark.asyncio
    async def test_prompt_copying(self, mock_multiturn_env):
        """Test that original prompt is not modified."""
        original_prompt = [{"role": "user", "content": "Original message"}]
        prompt_copy = [{"role": "user", "content": "Original message"}]

        mock_multiturn_env.client.add_chat_response(
            messages=[{"role": "user", "content": "Original message"}],
            response="Response DONE",
        )

        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model="test-model",
            prompt=original_prompt,
            answer="test_answer",
        )

        # Original prompt should be unchanged
        assert original_prompt == prompt_copy

    @pytest.mark.asyncio
    async def test_sampling_args_passed_through(self, mock_multiturn_env):
        """Test that sampling arguments are passed to model calls."""
        mock_multiturn_env.client.add_chat_response(
            messages=[{"role": "user", "content": "Test sampling"}],
            response="Quick DONE",
        )

        prompt = [{"role": "user", "content": "Test sampling"}]
        sampling_args = {"temperature": 0.8, "max_tokens": 50}

        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model="test-model",
            prompt=prompt,
            answer="test_answer",
            sampling_args=sampling_args,
        )

        # Verify sampling args were passed
        call_args = mock_multiturn_env.client.chat.completions.create.call_args
        assert "temperature" in call_args.kwargs
        assert "max_completion_tokens" in call_args.kwargs

    @pytest.mark.asyncio
    async def test_completion_format_multiturn(self, mock_openai_client):
        """Test MultiTurnEnv with completion format."""

        class CompletionMultiTurnEnv(MultiTurnEnv):
            def __init__(self, **kwargs):
                super().__init__(message_type="completion", **kwargs)

            def is_completed(self, messages, state, **kwargs):
                return "DONE" in messages

            def env_response(self, messages, state, **kwargs):
                return " Continue.", state

        completion_dataset = Dataset.from_dict(
            {"prompt": ["Start:"], "answer": ["Done"]}
        )

        env = CompletionMultiTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=completion_dataset,
            max_turns=3,
        )

        mock_openai_client.add_text_response("Start:", "First response")
        mock_openai_client.add_text_response(
            "Start:First response Continue.", "Final DONE"
        )

        prompt = "Start:"
        completion, state = await env.rollout(
            client=mock_openai_client, model="test-model", prompt=prompt, answer="Done"
        )

        assert isinstance(completion, str)
        assert "First response" in completion
        assert "DONE" in completion
        assert len(state["responses"]) == 2

    @pytest.mark.asyncio
    async def test_environment_response_state_modification(
        self, mock_openai_client, sample_chat_dataset
    ):
        """Test that environment can modify state between turns."""

        class StatefulMultiTurnEnv(MultiTurnEnv):
            def is_completed(self, messages, state, **kwargs):
                return state.get("turn_count", 0) >= 2

            def env_response(self, messages, state, **kwargs):  # type: ignore
                state["turn_count"] = state.get("turn_count", 0) + 1
                return [
                    {"role": "user", "content": f"Turn {state['turn_count']}"}
                ], state

        env = StatefulMultiTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_chat_dataset,
            max_turns=5,
            parser=Parser(),
            rubric=Rubric(),
        )

        env.client.set_default_responses(chat_response="Continue")  # type: ignore

        prompt = [{"role": "user", "content": "Start"}]
        completion, state = await env.rollout(
            client=env.client,  # type: ignore
            model="test-model",
            prompt=prompt,  # type: ignore
            answer="test",  # type: ignore
        )

        # Should complete when turn_count reaches 2
        assert state["turn_count"] == 2
        assert len(completion) >= 3  # Multiple turns with env responses

    def test_abstract_methods_not_implemented(self):
        """Test that MultiTurnEnv cannot be instantiated directly (abstract class)."""
        # MultiTurnEnv is abstract and should not be instantiable without implementing abstract methods
        with pytest.raises(TypeError):
            # This should fail because MultiTurnEnv has abstract methods
            MultiTurnEnv(model="test-model", parser=Parser(), rubric=Rubric())  # type: ignore

    @pytest.mark.asyncio
    async def test_completion_detection_before_env_response(
        self, mock_openai_client, sample_chat_dataset
    ):
        """Test completion detection works before env_response is called."""

        class ImmediateCompletionEnv(MultiTurnEnv):
            def is_completed(self, messages, state, **kwargs):
                # Complete if we have any assistant message
                return (
                    any(msg.get("role") == "assistant" for msg in messages)
                    if isinstance(messages, list)
                    else False
                )

            def env_response(self, messages, state, **kwargs):  # type: ignore
                # This should never be called due to immediate completion
                return {"role": "user", "content": "Should not appear"}, state

        env = ImmediateCompletionEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_chat_dataset,
            max_turns=5,
            parser=Parser(),
            rubric=Rubric(),
        )

        env.client.add_chat_response(  # type: ignore
            messages=[{"role": "user", "content": "Start"}], response="First response"
        )

        prompt = [{"role": "user", "content": "Start"}]
        completion, state = await env.rollout(
            client=env.client,  # type: ignore
            model="test-model",
            prompt=prompt,  # type: ignore
            answer="test",  # type: ignore
        )

        # Should complete immediately after first assistant response
        assert len(completion) == 1
        assert completion[0]["role"] == "assistant"  # type: ignore
        assert completion[0]["content"] == "First response"  # type: ignore

    @pytest.mark.asyncio
    async def test_responses_stored_in_state(self, mock_multiturn_env):
        """Test that model responses are stored in state['responses']."""
        # Set up a multi-turn conversation
        mock_multiturn_env.client.add_chat_response(
            messages=[{"role": "user", "content": "Start"}], response="First"
        )
        mock_multiturn_env.client.add_chat_response(
            messages=[
                {"role": "user", "content": "Start"},
                {"role": "assistant", "content": "First"},
                {"role": "user", "content": "Continue (turn 1)"},
            ],
            response="Second",
        )
        mock_multiturn_env.client.add_chat_response(
            messages=[
                {"role": "user", "content": "Start"},
                {"role": "assistant", "content": "First"},
                {"role": "user", "content": "Continue (turn 1)"},
                {"role": "assistant", "content": "Second"},
                {"role": "user", "content": "Please finish with DONE"},
            ],
            response="DONE",
        )

        prompt = [{"role": "user", "content": "Start"}]
        completion, state = await mock_multiturn_env.rollout(
            client=mock_multiturn_env.client,
            model="test-model",
            prompt=prompt,
            answer="test",
        )

        # Check that all responses are stored
        assert len(state["responses"]) == 3
        # Each response should have the structure returned by get_model_response
        for response in state["responses"]:
            assert hasattr(response, "choices")
            assert len(response.choices) > 0



================================================
FILE: tests/test_parser.py
================================================
"""Tests for the base Parser class."""

from verifiers import Parser


class TestParser:
    """Test cases for the base Parser class."""

    def test_parser_initialization(self, basic_parser):
        """Test that Parser initializes correctly."""
        assert isinstance(basic_parser, Parser)
        assert hasattr(basic_parser, "logger")

    def test_parser_with_kwargs(self):
        """Test that Parser accepts arbitrary kwargs."""
        parser = Parser(custom_attr="test_value", number=42)
        assert parser.custom_attr == "test_value"
        assert parser.number == 42

    def test_parse_returns_text_as_is(self, basic_parser):
        """Test that parse method returns text unchanged."""
        text = "This is a test string"
        result = basic_parser.parse(text)
        assert result == text

    def test_get_assistant_messages(self, basic_parser):
        """Test extraction of assistant messages from completion."""
        completion = [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": "Hi there"},
            {"role": "user", "content": "How are you?"},
            {"role": "assistant", "content": "I'm doing well"},
        ]
        assistant_messages = basic_parser.get_assistant_messages(completion)
        assert len(assistant_messages) == 2
        assert assistant_messages[0]["content"] == "Hi there"
        assert assistant_messages[1]["content"] == "I'm doing well"

    def test_parse_answer_with_string(self, basic_parser):
        """Test parse_answer with string input."""
        text = "This is an answer"
        result = basic_parser.parse_answer(text)
        assert result == text

    def test_parse_answer_with_completion(self, basic_parser):
        """Test parse_answer with completion list."""
        completion = [
            {"role": "user", "content": "What is 2+2?"},
            {"role": "assistant", "content": "The answer is 4"},
        ]
        result = basic_parser.parse_answer(completion)
        assert result == "The answer is 4"

    def test_get_format_reward_func(self, basic_parser):
        """Test that format reward function returns 1.0 by default."""
        reward_func = basic_parser.get_format_reward_func()
        completion = [{"role": "assistant", "content": "test"}]
        reward = reward_func(completion)
        assert reward == 1.0



================================================
FILE: tests/test_rubric.py
================================================
"""Tests for the Rubric class."""

import pytest

from verifiers import Parser, Rubric


class TestRubric:
    """Test cases for the Rubric class."""

    def test_rubric_initialization_empty(self):
        """Test Rubric initialization with no parameters."""
        rubric = Rubric()

        assert rubric.reward_funcs == []
        assert rubric.reward_weights == []
        assert isinstance(rubric.parser, Parser)

    def test_rubric_initialization_with_functions(self):
        """Test Rubric initialization with reward functions."""

        def reward_func1(completion, answer, **kwargs):
            return 1.0 if completion == answer else 0.0

        def reward_func2(completion, **kwargs):
            return len(completion) * 0.1

        funcs = [reward_func1, reward_func2]
        weights = [1.0, 0.5]

        rubric = Rubric(funcs=funcs, weights=weights)

        assert rubric.reward_funcs == funcs
        assert rubric.reward_weights == weights
        assert len(rubric.get_reward_func_names()) == 2
        assert rubric.get_reward_func_names() == ["reward_func1", "reward_func2"]

    def test_rubric_initialization_functions_without_weights(self):
        """Test Rubric initialization with functions but no explicit weights."""

        def reward_func1(completion, **kwargs):
            return 1.0

        def reward_func2(completion, **kwargs):
            return 0.5

        funcs = [reward_func1, reward_func2]

        rubric = Rubric(funcs=funcs)

        assert rubric.reward_funcs == funcs
        assert rubric.reward_weights == [1.0, 1.0]  # Default weights

    def test_rubric_initialization_with_kwargs(self):
        """Test Rubric initialization with additional kwargs."""
        rubric = Rubric(custom_param="test_value", another_param=42)

        assert rubric.custom_param == "test_value"  # type: ignore
        assert rubric.another_param == 42  # type: ignore

    def test_add_reward_func(self):
        """Test adding reward functions."""
        rubric = Rubric(funcs=[], weights=[])

        def test_func(completion, **kwargs):
            return 1.0

        rubric.add_reward_func(test_func, weight=0.8)

        assert len(rubric.reward_funcs) == 1
        assert rubric.reward_funcs[0] == test_func
        assert rubric.reward_weights == [0.8]
        assert rubric.get_reward_func_names() == ["test_func"]

    def test_add_multiple_reward_funcs(self):
        """Test adding multiple reward functions."""
        # Create fresh rubric to avoid test isolation issues
        rubric = Rubric(funcs=[], weights=[])

        def func1(completion, **kwargs):
            return 1.0

        def func2(completion, **kwargs):
            return 0.5

        rubric.add_reward_func(func1, weight=1.0)
        rubric.add_reward_func(func2, weight=0.3)

        assert len(rubric.reward_funcs) == 2
        assert rubric.get_reward_func_names() == ["func1", "func2"]
        assert rubric.reward_weights == [1.0, 0.3]

    def test_add_reward_func_default_weight(self):
        """Test adding reward function with default weight."""
        rubric = Rubric(funcs=[], weights=[])

        def test_func(completion, **kwargs):
            return 1.0

        rubric.add_reward_func(test_func)

        assert rubric.reward_weights == [1.0]

    def test_get_methods(self):
        """Test getter methods."""

        def func1(completion, **kwargs):
            return 1.0

        def func2(completion, **kwargs):
            return 0.5

        rubric = Rubric(funcs=[func1, func2], weights=[0.8, 0.2])

        assert rubric.get_reward_funcs() == [func1, func2]
        assert rubric.get_reward_weights() == [0.8, 0.2]
        assert rubric.get_reward_func_names() == ["func1", "func2"]

    @pytest.mark.asyncio
    async def test_call_reward_func_with_all_args(self):
        """Test calling reward function with all possible arguments."""

        def comprehensive_func(prompt, completion, answer, state, task, info, **kwargs):
            return len(completion) + len(answer) + len(task)

        rubric = Rubric(funcs=[], weights=[])

        result = await rubric.call_reward_func(
            func=comprehensive_func,
            parser=Parser(),
            prompt="test prompt",
            completion="test completion",
            answer="test answer",
            state={"key": "value"},
            task="test task",
            info={"info_key": "info_value"},
        )

        # len("test completion") + len("test answer") + len("test task")
        expected = len("test completion") + len("test answer") + len("test task")
        assert result == expected

    @pytest.mark.asyncio
    async def test_call_reward_func_with_subset_args(self):
        """Test calling reward function that only uses some arguments."""

        def simple_func(completion, answer, **kwargs):
            return 1.0 if completion == answer else 0.0

        rubric = Rubric(funcs=[], weights=[])

        result = await rubric.call_reward_func(
            func=simple_func,
            parser=Parser(),
            prompt="irrelevant",
            completion="same",
            answer="same",
            state={},
            task="irrelevant",
            info={},
        )

        assert result == 1.0

    @pytest.mark.asyncio
    async def test_call_reward_func_with_var_kwargs(self):
        """Test calling reward function that accepts **kwargs."""

        def kwargs_func(completion, **kwargs):
            return len(kwargs)

        rubric = Rubric(funcs=[], weights=[])

        result = await rubric.call_reward_func(
            func=kwargs_func,
            parser=Parser(),
            prompt="test",
            completion="test",
            answer="test",
            state={},
            task="test",
            info={},
        )

        # Should receive parser, prompt, answer, state, task, info (completion used directly)
        assert result == 6

    @pytest.mark.asyncio
    async def test_call_reward_func_error_handling(self):
        """Test error handling in reward function calls."""

        def error_func(completion, **kwargs):
            raise ValueError("Test error")

        rubric = Rubric(funcs=[], weights=[])

        result = await rubric.call_reward_func(
            func=error_func,
            parser=Parser(),
            prompt="test",
            completion="test",
            answer="test",
            state={},
            task="test",
            info={},
        )

        assert result == 0.0  # Should return 0.0 on error

    @pytest.mark.asyncio
    async def test_score_rollout_single(self):
        """Test scoring a single rollout."""

        def func1(completion, answer, **kwargs):
            return 1.0 if completion == answer else 0.0

        def func2(completion, **kwargs):
            return len(completion) * 0.1

        rubric = Rubric(funcs=[func1, func2], weights=[1.0, 0.5])

        result = await rubric.score_rollout(
            prompt="test prompt",
            completion="test",
            answer="test",
            state={},
            task="test_task",
            info={},
        )

        assert "func1" in result.metrics
        assert "func2" in result.metrics
        assert hasattr(result, "reward")
        assert result.metrics["func1"] == 1.0  # completion == answer
        assert result.metrics["func2"] == 0.4  # len("test") * 0.1
        assert result.reward == 1.0 * 1.0 + 0.4 * 0.5  # Weighted sum

    @pytest.mark.asyncio
    async def test_score_rollout_with_list_completion(self):
        """Test scoring rollout with list-type completion."""

        def list_func(completion, **kwargs):
            return len(completion) if isinstance(completion, list) else 0.0

        rubric = Rubric(funcs=[list_func])

        completion = [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": "Hi there!"},
        ]

        result = await rubric.score_rollout(
            prompt="test",
            completion=completion,  # type: ignore
            answer="test",
            state={},
            task="test",
            info={},
        )

        assert result.metrics["list_func"] == 2.0  # Length of completion list
        assert result.reward == 2.0

    @pytest.mark.asyncio
    async def test_score_rollouts_multiple(self):
        """Test scoring multiple rollouts."""

        def accuracy_func(completion, answer, **kwargs):
            return 1.0 if completion == answer else 0.0

        def length_func(completion, **kwargs):
            return len(str(completion))

        rubric = Rubric(funcs=[accuracy_func, length_func], weights=[1.0, 0.1])

        prompts = ["prompt1", "prompt2", "prompt3"]
        completions = ["answer1", "answer2", "wrong"]
        answers = ["answer1", "answer2", "answer3"]
        states = [{}, {}, {}]
        tasks = ["task1", "task2", "task3"]
        infos = [{}, {}, {}]

        results = await rubric.score_rollouts(
            prompts=prompts,  # type: ignore
            completions=completions,  # type: ignore
            answers=answers,
            states=states,
            tasks=tasks,
            infos=infos,
        )

        assert "accuracy_func" in results.metrics
        assert "length_func" in results.metrics
        assert hasattr(results, "reward")
        assert len(results.metrics["accuracy_func"]) == 3
        assert results.metrics["accuracy_func"] == [
            1.0,
            1.0,
            0.0,
        ]  # First two match, third doesn't
        assert results.metrics["length_func"] == [
            7.0,
            7.0,
            5.0,
        ]  # Lengths of completions

    @pytest.mark.asyncio
    async def test_score_rollouts_with_apply_weights(self):
        """Test scoring rollouts with apply_weights parameter."""

        def func1(completion, **kwargs):
            return 1.0

        def func2(completion, **kwargs):
            return 0.5

        rubric = Rubric(funcs=[func1, func2], weights=[2.0, 3.0])

        prompts = ["test"]
        completions = ["test"]
        answers = ["test"]
        states = [{}]
        tasks = ["test"]
        infos = [{}]

        # Test with apply_weights=True (default)
        results_weighted = await rubric.score_rollouts(
            prompts=prompts,  # type: ignore
            completions=completions,  # type: ignore
            answers=answers,
            states=states,
            tasks=tasks,
            infos=infos,
            apply_weights=True,
        )

        assert results_weighted.reward[0] == 1.0 * 2.0 + 0.5 * 3.0  # 2.0 + 1.5 = 3.5

        # Test with apply_weights=False (should not be used, but test anyway)
        results_unweighted = await rubric.score_rollouts(
            prompts=prompts,  # type: ignore
            completions=completions,  # type: ignore
            answers=answers,
            states=states,
            tasks=tasks,
            infos=infos,
            apply_weights=False,
        )

        # When apply_weights=False, only individual scores are returned, no weighted sum
        assert results_unweighted.reward[0] == 1.0 * 2.0 + 0.5 * 3.0  # Still weighted

    @pytest.mark.asyncio
    async def test_score_rollouts_empty(self):
        """Test scoring empty list of rollouts."""

        def test_func(completion, **kwargs):
            return 1.0

        rubric = Rubric(funcs=[test_func], weights=[1.0])

        # Should handle empty rollouts gracefully
        results = await rubric.score_rollouts(
            prompts=[], completions=[], answers=[], states=[], tasks=[], infos=[]
        )

        # Should return empty lists for each function
        assert "test_func" in results.metrics
        assert hasattr(results, "reward")
        assert results.metrics["test_func"] == []
        assert results.reward == []

    @pytest.mark.asyncio
    async def test_score_rollouts_with_default_infos(self):
        """Test scoring rollouts with default empty infos."""

        def simple_func(completion, **kwargs):
            return 1.0

        rubric = Rubric(funcs=[simple_func], weights=[1.0])

        results = await rubric.score_rollouts(
            prompts=["test"],
            completions=["test"],
            answers=["test"],
            states=[{}],
            tasks=["test"],
            infos=[{}],  # Explicitly provide infos to match other lists
        )

        assert "simple_func" in results.metrics
        assert results.metrics["simple_func"] == [1.0]

    def test_rubric_with_custom_parser(self):
        """Test Rubric with custom parser."""
        custom_parser = Parser()
        rubric = Rubric(funcs=[], weights=[], parser=custom_parser)

        assert rubric.parser is custom_parser

    @pytest.mark.asyncio
    async def test_score_rollouts_with_mixed_return_types(self):
        """Test scoring when reward functions return different types."""

        def scalar_func(completion, **kwargs):
            return 0.5

        def list_func(completion, **kwargs):
            # This should not happen, but test robustness
            return [0.1, 0.2]  # Wrong return type

        rubric = Rubric(funcs=[scalar_func], weights=[1.0])

        results = await rubric.score_rollouts(
            prompts=["test"],
            completions=["test"],
            answers=["test"],
            states=[{}],
            tasks=["test"],
            infos=[{}],
        )

        assert results.metrics["scalar_func"] == [0.5]
        assert results.reward == [0.5]

    @pytest.mark.asyncio
    async def test_call_reward_func_kwargs_filtering(self):
        """Test that functions without **kwargs get filtered kwargs."""
        
        def f_no_kwargs(completion, answer):
            return 0.5

        def f_with_kwargs(completion, **kwargs):
            assert kwargs.get("extra") == 123
            return 1.0

        rubric = Rubric(funcs=[f_no_kwargs, f_with_kwargs], weights=[1.0, 2.0])
        
        result = await rubric.score_rollout(
            prompt=[{"role": "user", "content": "q"}],
            completion=[{"role": "assistant", "content": "a"}],
            answer="ans",
            state={},
            task="default",
            info={},
            extra=123,
        )
        
        # Weighted sum: 0.5*1 + 1.0*2 = 2.5
        assert result.reward == pytest.approx(2.5)
        assert set(result.metrics.keys()) == {"f_no_kwargs", "f_with_kwargs"}

    @pytest.mark.asyncio
    async def test_score_rollout_serial_execution_order(self):
        """Test that serial mode respects execution order."""
        calls = []

        def g1(**kwargs):
            calls.append("g1")
            return 0.2

        def g2(**kwargs):
            calls.append("g2")
            return 0.3

        rubric = Rubric(funcs=[g1, g2], weights=[1.0, 1.0], parallelize_scoring=False)
        
        result = await rubric.score_rollout(
            prompt="q",
            completion="a",
            answer="ans",
            state={},
            task="default",
        )
        
        assert result.reward == pytest.approx(0.5)
        assert calls == ["g1", "g2"]  # serial order respected

    @pytest.mark.asyncio
    async def test_call_reward_func_error_handling_both_paths(self):
        """Test error handling for both kwargs and no-kwargs functions."""
        
        def error_func_no_kwargs(completion, answer):
            raise ValueError("Test error without kwargs")
        
        def error_func_with_kwargs(completion, **kwargs):
            raise RuntimeError("Test error with kwargs")
        
        rubric = Rubric()
        
        # Test both error paths return 0.0
        result1 = await rubric.call_reward_func(
            func=error_func_no_kwargs,
            parser=rubric.parser,
            prompt="test",
            completion="test", 
            answer="test",
            state={},
            task="test",
            info={},
        )
        
        result2 = await rubric.call_reward_func(
            func=error_func_with_kwargs,
            parser=rubric.parser,
            prompt="test",
            completion="test",
            answer="test", 
            state={},
            task="test",
            info={},
        )
        
        assert result1 == 0.0
        assert result2 == 0.0



================================================
FILE: tests/test_rubric_group.py
================================================
"""Tests for the RubricGroup class."""

import pytest

from verifiers import Rubric, RubricGroup


class TestRubricGroup:
    """Test cases for the RubricGroup class."""

    def test_rubric_group_initialization(self):
        """Test RubricGroup initialization with multiple rubrics."""

        def func1(completion, **kwargs):
            return 1.0

        def func2(completion, **kwargs):
            return 0.5

        rubric1 = Rubric(funcs=[func1], weights=[1.0])
        rubric2 = Rubric(funcs=[func2], weights=[0.8])

        rubrics = [rubric1, rubric2]
        group = RubricGroup(rubrics=rubrics)

        assert group.rubrics == rubrics
        assert len(group.rubrics) == 2

    def test_rubric_group_initialization_empty_fails(self):
        """Test that RubricGroup initialization fails with empty rubrics list."""
        with pytest.raises(
            ValueError, match="RubricGroup must have at least one rubric"
        ):
            RubricGroup(rubrics=[])

    def test_rubric_group_get_reward_func_names(self):
        """Test getting aggregated reward function names from all rubrics."""

        def func1(completion, **kwargs):
            return 1.0

        def func2(completion, **kwargs):
            return 0.5

        def func3(completion, **kwargs):
            return 0.3

        rubric1 = Rubric(funcs=[func1, func2], weights=[1.0, 0.5])
        rubric2 = Rubric(funcs=[func3], weights=[0.8])

        group = RubricGroup(rubrics=[rubric1, rubric2])
        names = group.get_reward_func_names()

        assert names == ["func1", "func2", "func3"]

    def test_rubric_group_get_reward_funcs(self):
        """Test getting aggregated reward functions from all rubrics."""

        def func1(completion, **kwargs):
            return 1.0

        def func2(completion, **kwargs):
            return 0.5

        rubric1 = Rubric(funcs=[func1], weights=[1.0])
        rubric2 = Rubric(funcs=[func2], weights=[0.8])

        group = RubricGroup(rubrics=[rubric1, rubric2])
        funcs = group.get_reward_funcs()

        assert len(funcs) == 2
        assert funcs[0] == func1
        assert funcs[1] == func2

    def test_rubric_group_get_reward_weights(self):
        """Test getting aggregated reward weights from all rubrics."""

        def func1(completion, **kwargs):
            return 1.0

        def func2(completion, **kwargs):
            return 0.5

        def func3(completion, **kwargs):
            return 0.3

        rubric1 = Rubric(funcs=[func1, func2], weights=[1.0, 0.7])
        rubric2 = Rubric(funcs=[func3], weights=[0.8])

        group = RubricGroup(rubrics=[rubric1, rubric2])
        weights = group.get_reward_weights()

        assert weights == [1.0, 0.7, 0.8]

    def test_rubric_group_add_reward_func(self):
        """Test adding reward function to RubricGroup (should add to first rubric)."""

        def func1(completion, **kwargs):
            return 1.0

        def new_func(completion, **kwargs):
            return 0.9

        rubric1 = Rubric(funcs=[func1], weights=[1.0])
        rubric2 = Rubric()

        group = RubricGroup(rubrics=[rubric1, rubric2])

        # Should add to first rubric
        group.add_reward_func(new_func, weight=0.6)

        assert len(rubric1.reward_funcs) == 2
        assert len(rubric2.reward_funcs) == 0
        assert rubric1.reward_funcs[1] == new_func
        assert rubric1.reward_weights[1] == 0.6

    def test_rubric_group_add_reward_func_empty_group_fails(self):
        """Test that adding reward function fails if no rubrics exist."""
        # This shouldn't happen due to initialization check, but test edge case
        group = RubricGroup.__new__(RubricGroup)  # Bypass __init__
        group.rubrics = []

        def test_func(completion, **kwargs):
            return 1.0

        with pytest.raises(
            AssertionError, match="RubricGroup must have at least one rubric"
        ):
            group.add_reward_func(test_func)

    @pytest.mark.asyncio
    async def test_rubric_group_score_rollouts_basic(self):
        """Test basic scoring of rollouts with multiple rubrics."""

        def func1(completion, **kwargs):
            return 1.0

        def func2(completion, **kwargs):
            return 0.5

        rubric1 = Rubric(funcs=[func1], weights=[1.0])
        rubric2 = Rubric(funcs=[func2], weights=[0.8])

        group = RubricGroup(rubrics=[rubric1, rubric2])

        # Test data
        prompts = ["What is 1+1?"]
        completions = ["2"]
        answers = ["2"]
        states = [{}]
        tasks = ["default"]
        infos = [{}]

        # Test scoring
        scores = await group.score_rollouts(
            prompts=prompts,
            completions=completions,
            answers=answers,
            states=states,
            tasks=tasks,
            infos=infos,
        )

        # Should have scores from both rubrics
        assert "func1" in scores.metrics
        assert "func2" in scores.metrics
        assert hasattr(scores, "reward")
        assert len(scores.metrics["func1"]) == 1
        assert len(scores.metrics["func2"]) == 1
        assert scores.metrics["func1"][0] == 1.0
        assert scores.metrics["func2"][0] == 0.5

    @pytest.mark.asyncio
    async def test_rubric_group_score_rollouts_duplicate_names(self):
        """Test that duplicate reward function names are summed up."""

        def func1(completion, **kwargs):
            return 1.0

        def func2(completion, **kwargs):
            return 0.5

        # Create two rubrics with same function name
        rubric1 = Rubric(funcs=[func1], weights=[1.0])
        rubric2 = Rubric(funcs=[func1], weights=[0.5])  # Same function name

        group = RubricGroup(rubrics=[rubric1, rubric2])

        # Test data
        prompts = ["What is 1+1?"]
        completions = ["2"]
        answers = ["2"]
        states = [{}]
        tasks = ["default"]
        infos = [{}]

        # Test scoring
        scores = await group.score_rollouts(
            prompts=prompts,
            completions=completions,
            answers=answers,
            states=states,
            tasks=tasks,
            infos=infos,
        )

        # Should have summed scores for duplicate function names
        assert "func1" in scores.metrics
        assert len(scores.metrics["func1"]) == 1
        assert (
            scores.metrics["func1"][0] == 2.0
        )  # 1.0 + 1.0 (same function called twice)

    @pytest.mark.asyncio
    async def test_rubric_group_score_rollouts_with_kwargs(self):
        """Test scoring rollouts with additional kwargs."""

        def func1(completion, custom_param=None, **kwargs):
            return 1.0 if custom_param == "test" else 0.5

        rubric1 = Rubric(funcs=[func1], weights=[1.0])
        rubric2 = Rubric(funcs=[func1], weights=[0.8])

        group = RubricGroup(rubrics=[rubric1, rubric2])

        # Test data
        prompts = ["What is 1+1?"]
        completions = ["2"]
        answers = ["2"]
        states = [{}]
        tasks = ["default"]
        infos = [{}]

        # Test scoring with custom kwargs
        scores = await group.score_rollouts(
            prompts=prompts,
            completions=completions,
            answers=answers,
            states=states,
            tasks=tasks,
            infos=infos,
            custom_param="test",
        )

        # Should pass custom kwargs to reward functions
        assert "func1" in scores.metrics
        assert len(scores.metrics["func1"]) == 1
        assert (
            scores.metrics["func1"][0] == 2.0
        )  # 1.0 + 1.0 (both should get custom_param="test")

    @pytest.mark.asyncio
    async def test_rubric_group_score_rollouts_single_rubric(self):
        """Test scoring rollouts with a single rubric (edge case)."""

        def func1(completion, **kwargs):
            return 1.0

        rubric1 = Rubric(funcs=[func1], weights=[1.0])

        group = RubricGroup(rubrics=[rubric1])

        # Test data
        prompts = ["What is 1+1?"]
        completions = ["2"]
        answers = ["2"]
        states = [{}]
        tasks = ["default"]
        infos = [{}]

        # Test scoring
        scores = await group.score_rollouts(
            prompts=prompts,
            completions=completions,
            answers=answers,
            states=states,
            tasks=tasks,
            infos=infos,
        )

        # Should work with single rubric
        assert "func1" in scores.metrics
        assert hasattr(scores, "reward")
        assert len(scores.metrics["func1"]) == 1
        assert scores.metrics["func1"][0] == 1.0

    @pytest.mark.asyncio
    async def test_rubric_group_score_rollouts_empty_data(self):
        """Test scoring empty rollouts."""

        def func1(completion, **kwargs):
            return 1.0

        rubric1 = Rubric(funcs=[func1], weights=[1.0])

        group = RubricGroup(rubrics=[rubric1])

        # Test with empty data
        prompts = []
        completions = []
        answers = []
        states = []
        tasks = []
        infos = []

        # Test scoring
        scores = await group.score_rollouts(
            prompts=prompts,
            completions=completions,
            answers=answers,
            states=states,
            tasks=tasks,
            infos=infos,
        )

        # Should return empty scores but with correct structure
        assert "func1" in scores.metrics
        assert hasattr(scores, "reward")
        assert len(scores.metrics["func1"]) == 0
        assert len(scores.reward) == 0

    def test_rubric_group_mixed_rubric_types(self):
        """Test RubricGroup with different types of rubrics."""

        def func1(completion, **kwargs):
            return 1.0

        def func2(completion, **kwargs):
            return 0.5

        # Create rubrics with different configurations
        rubric1 = Rubric(funcs=[func1], weights=[1.0])
        rubric2 = Rubric(funcs=[func2], weights=[0.3], custom_attr="test")

        group = RubricGroup(rubrics=[rubric1, rubric2])

        # Should aggregate functions and weights correctly
        assert group.get_reward_func_names() == ["func1", "func2"]
        assert group.get_reward_weights() == [1.0, 0.3]

    @pytest.mark.asyncio
    async def test_rubric_group_with_max_concurrent(self):
        """Test RubricGroup with max_concurrent parameter."""

        def func1(completion, **kwargs):
            return 1.0

        rubric1 = Rubric(funcs=[func1], weights=[1.0])

        group = RubricGroup(rubrics=[rubric1])

        # Test data
        prompts = ["What is 1+1?", "What is 2+2?"]
        completions = ["2", "4"]
        answers = ["2", "4"]
        states = [{}, {}]
        tasks = ["default", "default"]
        infos = [{}, {}]

        # Test scoring with max_concurrent parameter
        scores = await group.score_rollouts(
            prompts=prompts,
            completions=completions,
            answers=answers,
            states=states,
            tasks=tasks,
            infos=infos,
            max_concurrent=1,  # Force sequential execution
        )

        # Should work with max_concurrent parameter
        assert "func1" in scores.metrics
        assert hasattr(scores, "reward")
        assert len(scores.metrics["func1"]) == 2
        assert scores.metrics["func1"][0] == 1.0
        assert scores.metrics["func1"][1] == 1.0

    def test_rubric_group_inheritance(self):
        """Test that RubricGroup properly inherits from Rubric."""
        rubric = Rubric()
        group = RubricGroup(rubrics=[rubric])

        assert isinstance(group, Rubric)
        assert hasattr(group, "logger")
        assert hasattr(group, "parser")



================================================
FILE: tests/test_singleturn_env.py
================================================
"""Tests for the SingleTurnEnv class."""

from unittest.mock import AsyncMock, MagicMock

import pytest
from datasets import Dataset

from verifiers import Parser, Rubric, SingleTurnEnv
from verifiers.types import RolloutScores


class TestSingleTurnEnv:
    """Test cases for the SingleTurnEnv class."""

    def test_singleturn_env_initialization_chat(
        self, mock_openai_client, sample_dataset
    ):
        """Test SingleTurnEnv initialization with chat format."""
        env = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            message_type="chat",
            system_prompt="You are helpful.",
            parser=Parser(),
            rubric=Rubric(),
        )
        assert env.message_type == "chat"
        assert env.client == mock_openai_client
        assert env.model == "test-model"

    def test_singleturn_env_initialization_completion(self, mock_openai_client):
        """Test SingleTurnEnv initialization with completion format."""
        completion_dataset = Dataset.from_dict(
            {
                "prompt": ["Calculate 2+2:", "What is the capital?"],
                "answer": ["4", "It depends on the country"],
            }
        )

        env = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=completion_dataset,
            message_type="completion",
            parser=Parser(),
            rubric=Rubric(),
        )
        assert env.message_type == "completion"

    @pytest.mark.asyncio
    async def test_is_completed_method(self, mock_singleturn_env):
        """Test the is_completed method logic."""
        # No responses yet
        messages = [{"role": "user", "content": "Hello"}]
        state = {"responses": []}
        assert not await mock_singleturn_env.is_completed(messages, state)

        # With responses
        state = {"responses": [MagicMock()]}
        assert await mock_singleturn_env.is_completed(messages, state)

    @pytest.mark.asyncio
    async def test_env_response_method(self, mock_singleturn_env):
        """Test the env_response method (which should never be called in practice)."""
        messages = [{"role": "user", "content": "Hello"}]
        state = {}

        response, new_state = await mock_singleturn_env.env_response(messages, state)

        # Should return minimal response (env_response returns a list of messages)
        assert len(response) == 1
        assert response[0]["role"] == "user"
        assert response[0]["content"] == ""
        assert new_state == state

    @pytest.mark.asyncio
    async def test_rollout_chat_format(self, mock_singleturn_env):
        """Test rollout with chat format."""
        prompt = [{"role": "user", "content": "What is 2+2?"}]
        answer = "4"

        completion, state = await mock_singleturn_env.rollout(
            client=mock_singleturn_env.client,
            model="test-model",
            prompt=prompt,
            answer=answer,
        )

        # Should return list format for chat
        assert isinstance(completion, list)
        assert len(completion) == 1
        assert completion[0]["role"] == "assistant"
        assert completion[0]["content"] == "This is a test response"

        # Check state structure
        assert "responses" in state
        assert len(state["responses"]) == 1
        assert state["answer"] == answer

        # Verify the client was called
        mock_singleturn_env.client.chat.completions.create.assert_called_once()

    @pytest.mark.asyncio
    async def test_rollout_completion_format(self, mock_singleturn_env_completion):
        """Test rollout with completion format."""
        prompt = "Calculate 2+2:"
        answer = "4"

        completion, state = await mock_singleturn_env_completion.rollout(
            client=mock_singleturn_env_completion.client,
            model="test-model",
            prompt=prompt,
            answer=answer,
        )

        # Should return string format for completion
        assert isinstance(completion, str)
        assert completion == "This is a test completion"

        # Check state structure
        assert "responses" in state
        assert len(state["responses"]) == 1

        # Verify the client was called
        mock_singleturn_env_completion.client.completions.create.assert_called_once()

    @pytest.mark.asyncio
    async def test_rollout_with_sampling_args(self, mock_singleturn_env):
        """Test rollout with custom sampling arguments."""
        prompt = [{"role": "user", "content": "Hello"}]
        answer = "Hi"
        sampling_args = {"temperature": 0.8, "max_tokens": 100}

        completion, state = await mock_singleturn_env.rollout(
            client=mock_singleturn_env.client,
            model="test-model",
            prompt=prompt,
            answer=answer,
            sampling_args=sampling_args,
        )

        assert isinstance(completion, list)
        assert completion[0]["content"] == "This is a test response"

        # Verify sampling args were passed
        call_args = mock_singleturn_env.client.chat.completions.create.call_args
        assert "temperature" in call_args.kwargs
        assert "max_completion_tokens" in call_args.kwargs

    @pytest.mark.asyncio
    async def test_rollout_with_task_and_info(self, mock_singleturn_env):
        """Test rollout with task and info parameters."""
        prompt = [{"role": "user", "content": "Test question"}]
        answer = "Test answer"
        task = "math"
        info = {"difficulty": "easy"}

        completion, state = await mock_singleturn_env.rollout(
            client=mock_singleturn_env.client,
            model="test-model",
            prompt=prompt,
            answer=answer,
            task=task,
            info=info,
        )

        assert isinstance(completion, list)
        # Check state contains all the information
        assert state["answer"] == answer
        assert state["task"] == task
        assert state["info"] == info

    @pytest.mark.asyncio
    async def test_rollout_error_handling(self, mock_singleturn_env):
        """Test rollout handles errors from get_model_response."""
        # Mock get_model_response to return an error
        mock_singleturn_env.client.chat.completions.create = AsyncMock(
            side_effect=Exception("API Error")
        )

        prompt = [{"role": "user", "content": "Hello"}]
        answer = "Hi"

        with pytest.raises(Exception, match="API Error"):
            await mock_singleturn_env.rollout(
                client=mock_singleturn_env.client,
                model="test-model",
                prompt=prompt,
                answer=answer,
            )

    @pytest.mark.asyncio
    async def test_rollout_state_structure(self, mock_singleturn_env):
        """Test that rollout creates proper state structure."""
        prompt = [{"role": "user", "content": "Hello"}]
        answer = "Hi"
        task = "greeting"
        info = {"context": "test"}

        completion, state = await mock_singleturn_env.rollout(
            client=mock_singleturn_env.client,
            model="test-model",
            prompt=prompt,
            answer=answer,
            task=task,
            info=info,
        )

        # Check all expected state fields
        assert state["prompt"] == prompt
        # state["completion"] is initialized to [] but not updated during rollout
        assert state["completion"] == []
        assert state["answer"] == answer
        assert state["task"] == task
        assert state["info"] == info
        assert "responses" in state
        assert isinstance(state["responses"], list)
        assert len(state["responses"]) == 1

    @pytest.mark.asyncio
    async def test_a_generate_basic(self, mock_singleturn_env):
        """Test async generation with basic inputs."""
        inputs = {
            "prompt": [
                [{"role": "user", "content": "What is 2+2?"}],
                [{"role": "user", "content": "What is 3+3?"}],
            ],
            "answer": ["4", "6"],
        }

        # Mock the rubric.score_rollouts method
        mock_singleturn_env.rubric.score_rollouts = AsyncMock(
            return_value=RolloutScores(reward=[1.0, 1.0], metrics={})
        )

        results = await mock_singleturn_env.a_generate(inputs)

        assert hasattr(results, "completion")
        assert hasattr(results, "state")
        assert hasattr(results, "reward")
        assert len(results.completion) == 2
        assert len(results.state) == 2
        assert results.reward == [1.0, 1.0]

    @pytest.mark.asyncio
    async def test_a_generate_with_dataset(
        self, mock_singleturn_env, sample_chat_dataset
    ):
        """Test async generation with Dataset input."""
        # Mock the rubric.score_rollouts method
        mock_singleturn_env.rubric.score_rollouts = AsyncMock(
            return_value=RolloutScores(reward=[1.0, 1.0], metrics={})
        )

        results = await mock_singleturn_env.a_generate(sample_chat_dataset)

        assert hasattr(results, "completion")
        assert hasattr(results, "state")
        assert hasattr(results, "reward")
        assert len(results.completion) == 2

    @pytest.mark.asyncio
    async def test_a_generate_no_scoring(self, mock_singleturn_env):
        """Test async generation without scoring rollouts."""
        inputs = {"prompt": [[{"role": "user", "content": "Hello"}]], "answer": ["Hi"]}

        results = await mock_singleturn_env.a_generate(inputs, score_rollouts=False)

        assert hasattr(results, "completion")
        assert hasattr(results, "state")
        assert hasattr(results, "reward")  # reward attribute exists but should be empty
        assert results.reward == []  # Should be empty when score_rollouts=False

    def test_generate_sync_wrapper(self, mock_singleturn_env):
        """Test the synchronous generate wrapper."""
        inputs = {
            "prompt": [[{"role": "user", "content": "Hello"}]],
            "answer": ["Hi"],
            "info": [{}],
        }

        # Mock the rubric.score_rollouts method
        mock_singleturn_env.rubric.score_rollouts = AsyncMock(
            return_value=RolloutScores(reward=[1.0], metrics={})
        )

        results = mock_singleturn_env.generate(
            inputs, client=mock_singleturn_env.client
        )

        assert hasattr(results, "completion")
        assert hasattr(results, "state")
        assert hasattr(results, "reward")

    @pytest.mark.asyncio
    async def test_different_message_types_in_same_env(
        self, mock_openai_client, sample_dataset
    ):
        """Test that environment respects its message_type setting."""
        # Chat environment
        chat_env = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            message_type="chat",
        )

        # Completion environment
        completion_dataset = Dataset.from_dict(
            {"prompt": ["Test prompt"], "answer": ["Test answer"]}
        )
        completion_env = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=completion_dataset,
            message_type="completion",
        )

        # Test chat rollout
        chat_completion, chat_state = await chat_env.rollout(
            client=mock_openai_client,
            model="test-model",
            prompt=[{"role": "user", "content": "Hello"}],
            answer="Hi",
        )
        assert isinstance(chat_completion, list)

        # Test completion rollout
        completion_result, comp_state = await completion_env.rollout(
            client=mock_openai_client,
            model="test-model",
            prompt="Complete this:",
            answer="Done",
        )
        assert isinstance(completion_result, str)

    @pytest.mark.asyncio
    async def test_singleturn_stops_after_one_response(
        self, mock_openai_client, sample_dataset
    ):
        """Test that SingleTurnEnv truly stops after one response."""
        # We'll verify this by checking the is_completed logic
        env = SingleTurnEnv(
            client=mock_openai_client, model="test-model", dataset=sample_dataset
        )

        # Before any responses
        state = {"responses": []}
        assert not await env.is_completed([], state)

        # After one response
        state = {"responses": [MagicMock()]}
        assert await env.is_completed([], state)

        # Even with multiple responses (shouldn't happen), it's still completed
        state = {"responses": [MagicMock(), MagicMock()]}
        assert await env.is_completed([], state)



================================================
FILE: tests/test_think_parser.py
================================================
"""Tests for the ThinkParser class."""

from verifiers import ThinkParser


class TestThinkParser:
    """Test cases for the ThinkParser class."""

    def test_think_parser_initialization(self, think_parser):
        """Test that ThinkParser initializes correctly."""
        assert isinstance(think_parser, ThinkParser)
        assert hasattr(think_parser, "extract_fn")

    def test_think_parser_with_custom_extractor(self, think_parser_with_extractor):
        """Test ThinkParser with custom extraction function."""
        assert isinstance(think_parser_with_extractor, ThinkParser)

    def test_parse_with_think_tags(self, think_parser):
        """Test parsing text with think tags."""
        text = """<think>
        Let me think about this problem.
        I need to consider multiple factors.
        </think>
        The final answer is 42."""

        result = think_parser.parse(text)
        assert result == "The final answer is 42."

    def test_parse_without_think_tags(self, think_parser):
        """Test parsing text without think tags."""
        text = "Just a simple answer without thinking tags."
        result = think_parser.parse(text)
        assert result == ""

    def test_parse_with_multiple_think_blocks(self, think_parser):
        """Test parsing with multiple think blocks (should use content after last one)."""
        text = """<think>First thought</think>
        Some intermediate text.
        <think>Second thought</think>
        Final answer here."""

        result = think_parser.parse(text)
        assert result == "Final answer here."

    def test_parse_with_custom_extractor(self, think_parser_with_extractor):
        """Test parsing with custom extraction function."""
        text = """<think>
        I need to solve this step by step.
        </think>
        The answer is \\boxed{42}."""

        result = think_parser_with_extractor.parse(text)
        assert result == "42"

    def test_parse_with_custom_extractor_no_boxed(self, think_parser_with_extractor):
        """Test custom extractor when no boxed answer is found."""
        text = """<think>
        Thinking about the problem.
        </think>
        Just a plain answer."""

        result = think_parser_with_extractor.parse(text)
        assert result == "Just a plain answer."

    def test_parse_empty_after_think(self, think_parser):
        """Test parsing when content after think tags is empty."""
        text = "<think>Some thinking</think>"
        result = think_parser.parse(text)
        assert result == ""

    def test_parse_whitespace_handling(self, think_parser):
        """Test that whitespace is properly stripped."""
        text = """<think>
        Thinking process here.
        </think>
        
        Answer with spaces around it.
        
        """
        result = think_parser.parse(text)
        assert result == "Answer with spaces around it."

    def test_format_reward_function_good_format(self, think_parser):
        """Test format reward function with well-formatted content."""
        reward_func = think_parser.get_format_reward_func()

        completion = [
            {"role": "assistant", "content": "<think>Let me think</think>Final answer"}
        ]
        reward = reward_func(completion)
        assert reward == 1.0

    def test_format_reward_function_bad_format(self, think_parser):
        """Test format reward function with poorly formatted content."""
        reward_func = think_parser.get_format_reward_func()

        # Missing think tags
        bad_completion1 = [
            {"role": "assistant", "content": "Just an answer without thinking"}
        ]
        reward1 = reward_func(bad_completion1)
        assert reward1 == 0.0

        # Multiple think tags
        bad_completion2 = [
            {
                "role": "assistant",
                "content": "<think>First</think><think>Second</think>Answer",
            }
        ]
        reward2 = reward_func(bad_completion2)
        assert reward2 == 0.0

        # No content after think
        bad_completion3 = [
            {"role": "assistant", "content": "<think>Only thinking</think>"}
        ]
        reward3 = reward_func(bad_completion3)
        assert reward3 == 0.0

    def test_format_reward_function_mixed_messages(self, think_parser):
        """Test format reward function with mixed good and bad messages."""
        reward_func = think_parser.get_format_reward_func()

        completion = [
            {"role": "assistant", "content": "<think>Good thinking</think>Good answer"},
            {"role": "assistant", "content": "Bad answer without thinking"},
            {
                "role": "assistant",
                "content": "<think>More thinking</think>Another good answer",
            },
        ]
        reward = reward_func(completion)
        assert reward == 2.0 / 3.0  # 2 out of 3 messages are well-formatted

    def test_format_reward_function_no_assistant_messages(self, think_parser):
        """Test format reward function with no assistant messages."""
        reward_func = think_parser.get_format_reward_func()

        completion = [{"role": "user", "content": "Question"}]
        # Should handle gracefully, though the implementation might vary
        # This tests robustness of the reward function
        try:
            reward = reward_func(completion)
            # If it doesn't raise an error, the reward should be reasonable
            assert 0.0 <= reward <= 1.0
        except (ZeroDivisionError, IndexError):
            # If it raises an error, that's also acceptable behavior
            # since there are no assistant messages to evaluate
            pass

    def test_parse_answer_integration(self, think_parser):
        """Test parse_answer method inherited from Parser."""
        completion = [
            {"role": "user", "content": "What is 2+2?"},
            {
                "role": "assistant",
                "content": "<think>Let me calculate</think>The answer is 4",
            },
        ]
        result = think_parser.parse_answer(completion)
        assert result == "The answer is 4"

    def test_parse_answer_string_integration(self, think_parser):
        """Test parse_answer with string input."""
        text = "<think>Calculating...</think>Result: 42"
        result = think_parser.parse_answer(text)
        assert result == "Result: 42"



================================================
FILE: tests/test_xml_parser.py
================================================
"""Tests for the XMLParser class."""

import pytest
from verifiers import XMLParser


class TestXMLParser:
    """Test cases for the XMLParser class."""

    def test_xml_parser_initialization(self, xml_parser):
        """Test that XMLParser initializes correctly."""
        assert isinstance(xml_parser, XMLParser)
        assert xml_parser.answer_field == "answer"

    def test_xml_parser_with_alternatives(self, xml_parser_with_alternatives):
        """Test XMLParser with alternative field names."""
        assert isinstance(xml_parser_with_alternatives, XMLParser)
        fields = xml_parser_with_alternatives.get_fields()
        assert "reasoning" in fields
        assert "code" in fields  # canonical name from ("code", "answer")

    def test_parse_simple_xml(self, xml_parser):
        """Test parsing simple XML with basic fields."""
        xml_text = """
        <reasoning>
        Let me think about this problem step by step.
        </reasoning>
        <answer>
        The final answer is 42.
        </answer>
        """
        result = xml_parser.parse(xml_text)
        assert result.reasoning == "Let me think about this problem step by step."
        assert result.answer == "The final answer is 42."

    def test_parse_xml_with_alternatives(self, xml_parser_with_alternatives):
        """Test parsing XML with alternative field names."""
        xml_text = """
        <reasoning>
        First, I need to understand the problem.
        </reasoning>
        <code>
        def solve(): return 42
        </code>
        """
        result = xml_parser_with_alternatives.parse(xml_text)
        assert result.reasoning == "First, I need to understand the problem."
        assert result.code == "def solve(): return 42"
        # Both alternatives should be accessible
        assert hasattr(result, 'answer')
        assert result.answer is None

    def test_parse_missing_fields(self, xml_parser):
        """Test parsing XML with missing fields."""
        xml_text = "<reasoning>Only reasoning here</reasoning>"
        result = xml_parser.parse(xml_text)
        assert result.reasoning == "Only reasoning here"
        assert result.answer is None

    def test_parse_empty_fields(self, xml_parser):
        """Test parsing XML with empty fields."""
        xml_text = "<reasoning></reasoning><answer></answer>"
        result = xml_parser.parse(xml_text)
        assert result.reasoning == ""
        assert result.answer == ""

    def test_parse_no_strip(self, xml_parser):
        """Test parsing without stripping whitespace."""
        # Note: The regex pattern itself removes leading/trailing whitespace
        # from the capture group, so strip=False only affects the .strip() call
        xml_text = "<answer>  spaced content  </answer>"
        result_strip = xml_parser.parse(xml_text, strip=True)
        result_no_strip = xml_parser.parse(xml_text, strip=False)
        assert result_strip.answer == "spaced content"
        assert result_no_strip.answer == "spaced content"  # regex already strips whitespace

    def test_parse_answer_from_completion(self, xml_parser):
        """Test extracting answer from completion."""
        completion = [
            {"role": "user", "content": "Solve this problem"},
            {"role": "assistant", "content": "<reasoning>Let me think</reasoning><answer>42</answer>"},
            {"role": "assistant", "content": "<reasoning>Actually, let me reconsider</reasoning><answer>43</answer>"}
        ]
        result = xml_parser.parse_answer(completion)
        assert result == "43"  # Should get the last answer
    
    def test_parse_answer_from_string_completion(self, xml_parser):
        """Test extracting answer from string completion."""
        completion = "<answer>44</answer><reasoning>Actually, that's not right either.</reasoning><answer>45</answer>"
        result = xml_parser.parse_answer(completion)
        assert result == "45" # Should return just the last answer, not a full parse() namespace

    def test_parse_answer_no_answer_field(self, xml_parser):
        """Test parse_answer when no answer field is found."""
        completion = [
            {"role": "assistant", "content": "<reasoning>Only reasoning here</reasoning>"}
        ]
        result = xml_parser.parse_answer(completion)
        assert result is None

    def test_get_format_str(self, xml_parser):
        """Test format string generation."""
        format_str = xml_parser.get_format_str()
        assert "<reasoning>" in format_str
        assert "</reasoning>" in format_str
        assert "<answer>" in format_str
        assert "</answer>" in format_str

    def test_get_format_str_with_alternatives(self, xml_parser_with_alternatives):
        """Test format string with alternatives."""
        format_str = xml_parser_with_alternatives.get_format_str()
        assert "code | answer" in format_str

    def test_format_method(self, xml_parser):
        """Test formatting keyword arguments into XML."""
        formatted = xml_parser.format(reasoning="My reasoning", answer="42")
        assert "<reasoning>\nMy reasoning\n</reasoning>" in formatted
        assert "<answer>\n42\n</answer>" in formatted

    def test_format_method_missing_field(self, xml_parser):
        """Test format method with missing required field."""
        with pytest.raises(ValueError, match="Missing value for field"):
            xml_parser.format(reasoning="Only reasoning")

    def test_format_method_with_alternatives(self, xml_parser_with_alternatives):
        """Test format method with alternative field names."""
        # Using canonical name
        formatted1 = xml_parser_with_alternatives.format(reasoning="test", code="print('hello')")
        assert "<code>\nprint('hello')\n</code>" in formatted1
        
        # Using alternative name
        formatted2 = xml_parser_with_alternatives.format(reasoning="test", answer="print('hello')")
        assert "<code>\nprint('hello')\n</code>" in formatted2  # Should use canonical tag

    def test_get_fields(self, xml_parser, xml_parser_with_alternatives):
        """Test getting field names."""
        fields1 = xml_parser.get_fields()
        assert fields1 == ["reasoning", "answer"]
        
        fields2 = xml_parser_with_alternatives.get_fields()
        assert fields2 == ["reasoning", "code"]

    def test_invalid_field_types(self):
        """Test XMLParser initialization with invalid field types."""
        with pytest.raises(TypeError):
            XMLParser([123])  # Invalid field type
        
        # Empty fields is actually allowed - it just creates a parser with no fields
        empty_parser = XMLParser([])  # This works
        assert empty_parser.get_fields() == []
        
        with pytest.raises(ValueError):
            XMLParser(["field1", "field1"])  # Duplicate fields

    def test_format_reward_function(self, xml_parser):
        """Test the format reward function."""
        reward_func = xml_parser.get_format_reward_func()
        
        # Well-formatted completion
        good_completion = [
            {"role": "assistant", "content": "<reasoning>Good reasoning</reasoning><answer>42</answer>"}
        ]
        good_reward = reward_func(good_completion)
        assert 0.0 <= good_reward <= 1.0
        
        # Poorly formatted completion - gets partial credit for proper spacing
        bad_completion = [
            {"role": "assistant", "content": "Just plain text without XML"}
        ]
        bad_reward = reward_func(bad_completion)
        assert bad_reward == 0.2  # Gets 0.2 for proper spacing (no XML tags to mess up)


================================================
FILE: verifiers/__init__.py
================================================
__version__ = "0.1.3"

import importlib
import logging
import sys
from typing import TYPE_CHECKING, Optional

from .types import *  # noqa # isort: skip
from .envs.env_group import EnvGroup
from .envs.environment import Environment
from .envs.multiturn_env import MultiTurnEnv
from .envs.singleturn_env import SingleTurnEnv
from .envs.tool_env import ToolEnv
from .parsers.parser import Parser
from .parsers.think_parser import ThinkParser
from .parsers.xml_parser import XMLParser
from .rubrics.judge_rubric import JudgeRubric
from .rubrics.rubric import Rubric
from .rubrics.rubric_group import RubricGroup
from .rubrics.tool_rubric import ToolRubric
from .utils.data_utils import (
    extract_boxed_answer,
    extract_hash_answer,
    load_example_dataset,
)
from .utils.env_utils import load_environment
from .utils.logging_utils import print_prompt_completions_sample


# Setup default logging configuration
def setup_logging(
    level: str = "INFO",
    log_format: Optional[str] = None,
    date_format: Optional[str] = None,
) -> None:
    """
    Setup basic logging configuration for the verifiers package.

    Args:
        level: The logging level to use. Defaults to "INFO".
        log_format: Custom log format string. If None, uses default format.
        date_format: Custom date format string. If None, uses default format.
    """
    if log_format is None:
        log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    if date_format is None:
        date_format = "%Y-%m-%d %H:%M:%S"

    # Create a StreamHandler that writes to stderr
    handler = logging.StreamHandler(sys.stderr)
    handler.setFormatter(logging.Formatter(fmt=log_format, datefmt=date_format))

    # Get the root logger for the verifiers package
    logger = logging.getLogger("verifiers")
    logger.setLevel(level.upper())
    logger.addHandler(handler)

    # Prevent the logger from propagating messages to the root logger
    logger.propagate = False


setup_logging()

__all__ = [
    "Parser",
    "ThinkParser",
    "XMLParser",
    "Rubric",
    "JudgeRubric",
    "RubricGroup",
    "ToolRubric",
    "Environment",
    "MultiTurnEnv",
    "SingleTurnEnv",
    "ToolEnv",
    "EnvGroup",
    "extract_boxed_answer",
    "extract_hash_answer",
    "load_example_dataset",
    "setup_logging",
    "load_environment",
    "print_prompt_completions_sample",
    "get_model",
    "get_tokenizer",
    "get_model_and_tokenizer",
    "GRPOTrainer",
    "GRPOConfig",
    "grpo_defaults",
    "lora_defaults",
]

_LAZY_IMPORTS = {
    "get_model": "verifiers.utils.model_utils:get_model",
    "get_model_and_tokenizer": "verifiers.utils.model_utils:get_model_and_tokenizer",
    "get_tokenizer": "verifiers.utils.model_utils:get_tokenizer",
    "GRPOConfig": "verifiers.trainers:GRPOConfig",
    "GRPOTrainer": "verifiers.trainers:GRPOTrainer",
    "grpo_defaults": "verifiers.trainers:grpo_defaults",
    "lora_defaults": "verifiers.trainers:lora_defaults",
}


def __getattr__(name: str):
    try:
        module, attr = _LAZY_IMPORTS[name].split(":")
        return getattr(importlib.import_module(module), attr)
    except KeyError:
        raise AttributeError(f"module 'verifiers' has no attribute '{name}'")
    except ModuleNotFoundError as e:
        # warn that accessed var needs [all] to be installed
        raise AttributeError(
            f"To use verifiers.{name}, install as `verifiers[all]`. "
        ) from e


if TYPE_CHECKING:
    from .trainers import (  # noqa: F401
        GRPOConfig,
        GRPOTrainer,
        grpo_defaults,
        lora_defaults,
    )
    from .utils.model_utils import (  # noqa: F401
        get_model,
        get_model_and_tokenizer,
        get_tokenizer,
    )



================================================
FILE: verifiers/types.py
================================================
from typing import (
    TYPE_CHECKING,
    Annotated,
    Any,
    Awaitable,
    Callable,
    Literal,
)

from openai.types.chat.chat_completion import ChatCompletion
from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam

# openai types
from openai.types.chat.chat_completion_message_tool_call import (
    ChatCompletionMessageToolCall,  # noqa: F401
)
from openai.types.chat.chat_completion_role import ChatCompletionRole  # noqa: F401
from openai.types.chat.chat_completion_tool_param import (
    ChatCompletionToolParam,  # noqa: F401
)
from openai.types.completion import Completion
from openai.types.shared_params import (  # noqa: F401
    FunctionDefinition,
    FunctionParameters,
)
from pydantic import BaseModel, Field, SkipValidation

# typing aliases
if TYPE_CHECKING:
    ChatMessage = ChatCompletionMessageParam
else:
    ChatMessage = Annotated[ChatCompletionMessageParam, SkipValidation]

MessageType = Literal["chat", "completion"]
ModelResponse = Completion | ChatCompletion | None

Message = str | ChatMessage

Messages = str | list[ChatMessage]
Info = dict[str, Any]
State = dict[str, Any]
SamplingArgs = dict[str, Any]
RewardFunc = Callable[..., float | Awaitable[float]]

# oai tools
JsonPrimitive = Literal["string", "number", "integer", "boolean", "array", "object"]


class GenerateInputs(BaseModel):
    """Pydantic model for generation inputs."""

    prompt: list[Messages]
    answer: list[str] | None = None
    info: list[dict] | None = None
    task: list[str] | None = None
    completion: list[Messages] | None = None


class GenerateOutputs(BaseModel):
    """Pydantic model for generation outputs."""

    prompt: list[Messages]
    completion: list[Messages]
    answer: list[str]
    state: list[State]
    info: list[Info]
    task: list[str]
    reward: list[float]
    metrics: dict[str, list[float]] = Field(default_factory=dict)


class RolloutScore(BaseModel):
    """Pydantic model for rollout scores."""

    reward: float
    metrics: dict[str, float] = Field(default_factory=dict)


class RolloutScores(BaseModel):
    """Pydantic model for rubric outputs."""

    reward: list[float]
    metrics: dict[str, list[float]] = Field(default_factory=dict)


class ProcessedOutputs(BaseModel):
    """Pydantic model for processed outputs."""

    prompt_ids: list[list[int]]
    prompt_mask: list[list[int]]
    completion_ids: list[list[int]]
    completion_mask: list[list[int]]
    completion_logprobs: list[list[float]]
    rewards: list[float]



================================================
FILE: verifiers/envs/__init__.py
================================================
[Empty file]


================================================
FILE: verifiers/envs/env_group.py
================================================
from datasets import concatenate_datasets
from openai import AsyncOpenAI

from verifiers import (
    ChatMessage,
    Info,
    SamplingArgs,
    State,
)
from verifiers.envs.environment import Environment
from verifiers.rubrics.rubric import Rubric
from verifiers.types import RolloutScore


class EnvGroupRubric(Rubric):
    """
    Custom rubric for EnvGroup that routes scoring to appropriate environment rubrics.
    """

    def __init__(self, env_map: dict[str, Environment]):
        super().__init__()
        self.env_map = env_map

        # Collect all unique reward function names across all environments
        all_names_set = set()
        for env in env_map.values():
            all_names_set.update(env.rubric.get_reward_func_names())
        self.all_reward_names = sorted(list(all_names_set))

        self.logger.info(
            f"EnvGroupRubric tracking {len(self.all_reward_names)} unique reward functions"
        )

    def get_reward_func_names(self) -> list[str]:
        """Return all unique reward function names across all environments."""
        return self.all_reward_names

    async def score_rollout(
        self,
        prompt: str | list[ChatMessage],
        completion: str | list[ChatMessage],
        answer: str = "",
        state: State | None = None,
        task: str = "default",
        info: dict | None = None,
        **kwargs,
    ) -> RolloutScore:
        """
        Route scoring to the appropriate environment's rubric based on task.

        Returns a RolloutScore with all reward function names, using 0.0 for functions
        not applicable to this sample's environment.
        """
        state = state or {}
        info = info or {}

        # Initialize metrics with all reward names set to 0.0
        metrics = {name: 0.0 for name in self.all_reward_names}
        reward = 0.0

        # Get the appropriate environment
        env = self.env_map.get(task)
        if env is None:
            self.logger.warning(f"No environment found for task '{task}'")
            return RolloutScore(reward=reward, metrics=metrics)

        # Score with the environment's rubric
        env_results = await env.rubric.score_rollout(
            prompt, completion, answer, state, task, info, **kwargs
        )

        # Update metrics with individual metric scores from the environment
        for reward_name, score in env_results.metrics.items():
            if reward_name in metrics:
                metrics[reward_name] = score

        # The overall reward is from the environment's rubric
        reward = env_results.reward

        return RolloutScore(reward=reward, metrics=metrics)


class EnvGroup(Environment):
    """
    Environment group that acts as a mixture of multiple environments.

    Routes operations to appropriate sub-environments based on the 'task' column.
    """

    def __init__(
        self, envs: list[Environment], env_names: list[str] | None = None, **kwargs
    ):
        """
        Initialize EnvGroup with a list of environments.

        Args:
            envs: list of Environment instances
            env_names: Optional list of names for each environment.
                      If not provided, uses "env_0", "env_1", etc.
            **kwargs: Additional arguments passed to parent Environment
        """
        if not envs:
            raise ValueError("EnvGroup requires at least one environment")

        self.envs = envs
        self.env_names = env_names or [f"env_{i}" for i in range(len(envs))]

        if len(self.env_names) != len(self.envs):
            raise ValueError("Number of env_names must match number of envs")

        # Create mapping for quick lookup
        self.env_map = {name: env for name, env in zip(self.env_names, self.envs)}

        # concatenate datasets with task labels
        datasets = []
        eval_datasets = []
        for env, name in zip(self.envs, self.env_names):

            def add_task(example):
                example["task"] = name
                return example

            env_dataset = env.get_dataset()
            if env_dataset is not None and "task" not in env_dataset.column_names:
                env_dataset = env_dataset.map(add_task)
            if env_dataset is not None:
                datasets.append(env_dataset)
            env_eval_dataset = env.get_eval_dataset()
            if (
                env_eval_dataset is not None
                and "task" not in env_eval_dataset.column_names
            ):
                env_eval_dataset = env_eval_dataset.map(add_task)
            if env_eval_dataset is not None:
                eval_datasets.append(env_eval_dataset)
        dataset = concatenate_datasets(datasets) if datasets else None
        eval_dataset = concatenate_datasets(eval_datasets) if eval_datasets else None
        # wrap rubrics
        rubric = EnvGroupRubric(self.env_map)

        # initialize parent Environment
        super().__init__(
            dataset=dataset, eval_dataset=eval_dataset, rubric=rubric, **kwargs
        )
        self.logger.info(
            f"Initialized EnvGroup with {len(envs)} environments: {self.env_names}"
        )

    async def rollout(
        self,
        client: AsyncOpenAI,
        model: str,
        prompt: str | list[ChatMessage],
        answer: str = "",
        task: str = "default",
        info: Info | None = None,
        sampling_args: SamplingArgs | None = None,
        **kwargs,
    ) -> tuple[str | list[ChatMessage], State]:
        """
        Route rollout to the appropriate sub-environment based on task.

        The task is determined from (in order of priority):
        1. kwargs['task']
        2. info['task']
        3. First environment name (default)
        """
        info = info or {}
        sampling_args = sampling_args or {}

        # Route to appropriate environment
        env = self.env_map[task]

        # Pass through all arguments
        return await env.rollout(
            client, model, prompt, answer, task, info, sampling_args, **kwargs
        )

    def get_env_for_task(self, task: str) -> Environment:
        """Get the environment instance for a given task name."""
        return self.env_map.get(task, self.envs[0])



================================================
FILE: verifiers/envs/environment.py
================================================
import asyncio
import json
import logging
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor
from copy import deepcopy
from typing import TYPE_CHECKING, Literal

from datasets import Dataset
from openai import AsyncOpenAI, OpenAI

from verifiers.parsers.parser import Parser
from verifiers.rubrics.rubric import Rubric
from verifiers.types import (
    ChatCompletion,
    ChatCompletionToolParam,
    ChatMessage,
    Completion,
    GenerateInputs,
    GenerateOutputs,
    Info,
    Messages,
    MessageType,
    ModelResponse,
    ProcessedOutputs,
    RewardFunc,
    SamplingArgs,
    State,
)
from verifiers.utils.message_utils import cleanup_messages, sanitize_tool_calls

if TYPE_CHECKING:
    from transformers.tokenization_utils_base import (  # type: ignore
        PreTrainedTokenizerBase,
    )


class Environment(ABC):
    """
    Base class for all environments.
    """

    def __init__(
        self,
        client: AsyncOpenAI | None = None,
        model: str | None = None,
        dataset: Dataset | None = None,
        eval_dataset: Dataset | None = None,
        system_prompt: str | None = None,
        few_shot: list[ChatMessage] | None = None,
        parser: Parser | None = None,
        rubric: Rubric | None = None,
        sampling_args: SamplingArgs | None = None,
        message_type: MessageType = "chat",
        oai_tools: list[ChatCompletionToolParam] | None = None,
        max_workers: int = 512,
        **kwargs,
    ):
        self.client = client
        self.model = model
        self.message_type: Literal["chat", "completion"] = message_type
        self.oai_tools: list[ChatCompletionToolParam] | None = oai_tools
        self.system_prompt = system_prompt
        self.few_shot = few_shot
        self.parser = parser or Parser()
        self.rubric = rubric or Rubric()

        if self.message_type == "chat":
            if dataset is not None:
                self.dataset = self.format_dataset(
                    dataset, self.system_prompt, self.few_shot
                )
            else:
                self.dataset = None
            if eval_dataset is not None:
                self.eval_dataset = self.format_dataset(
                    eval_dataset, self.system_prompt, self.few_shot
                )
            else:
                self.eval_dataset = None
        else:
            if self.system_prompt or self.few_shot:
                raise ValueError(
                    'The fields "system_prompt" and "few_shot" are not supported for completion tasks.'
                    'Please use message_type="chat" instead, or pre-format your dataset '
                    'to contain "prompt" and "answer" columns.'
                )
            self.dataset = dataset
            self.eval_dataset = eval_dataset

        self.sampling_args = {"n": 1, "extra_body": {}}
        if sampling_args is not None:
            # merge extra_body if provided
            self.sampling_args["extra_body"].update(sampling_args.get("extra_body", {}))
            # copy other keys
            for key, value in sampling_args.items():
                if key != "extra_body":
                    self.sampling_args[key] = value

        self.max_workers = max_workers
        self.logger = logging.getLogger(f"verifiers.envs.{self.__class__.__name__}")
        for key, value in kwargs.items():
            setattr(self, key, value)

        if self.dataset is None and self.eval_dataset is None:
            raise ValueError("Either dataset or eval_dataset must be provided")

    def format_prompt(
        self,
        prompt_str: str,
        system_prompt: str | None = None,
        few_shot: list[ChatMessage] | None = None,
    ) -> list[ChatMessage]:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        if few_shot:
            messages.extend(few_shot)
        messages.append({"role": "user", "content": prompt_str})
        return messages

    def format_dataset(
        self,
        dataset: Dataset,
        system_prompt: str | None = None,
        few_shot: list[ChatMessage] | None = None,
        question_key: str = "question",
        answer_key: str = "answer",
    ) -> Dataset:
        # skip if "prompt" already exists
        if "prompt" in dataset.column_names:
            return dataset

        # extract format_prompt as a standalone function to avoid capturing self
        def format_prompt_fn(prompt_str: str) -> list[ChatMessage]:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            if few_shot:
                messages.extend(few_shot)
            messages.append({"role": "user", "content": prompt_str})
            return messages

        if answer_key == "answer":
            return dataset.map(
                lambda x: {
                    "prompt": format_prompt_fn(x[question_key]),
                }
            )
        else:
            return dataset.map(
                lambda x: {
                    "prompt": format_prompt_fn(x[question_key]),
                    "answer": x[answer_key],
                }
            )

    def get_dataset(self, n: int = -1, seed: int | None = None) -> Dataset:
        if self.dataset is None:
            raise ValueError("dataset is not set")
        if seed is not None:
            self.dataset = self.dataset.shuffle(seed=seed)
        if n > 0:
            return self.dataset.select(range(n))
        return self.dataset

    def get_eval_dataset(self, n: int = -1, seed: int | None = None) -> Dataset | None:
        if self.eval_dataset is None:
            self.logger.warning(
                "eval_dataset is not set, falling back to train dataset"
            )
            return self.get_dataset(n, seed)
        if seed is not None:
            self.eval_dataset = self.eval_dataset.shuffle(seed=seed)
        if n > 0:
            return self.eval_dataset.select(range(n))
        return self.eval_dataset

    def get_reward_funcs(self) -> list[RewardFunc]:
        return self.rubric.get_reward_funcs()

    def get_reward_weights(self) -> list[float]:
        return self.rubric.get_reward_weights()

    async def get_model_response(
        self,
        client: AsyncOpenAI,
        model: str,
        prompt: Messages,
        oai_tools: list[ChatCompletionToolParam] | None = None,
        sampling_args: SamplingArgs | None = None,
        message_type: MessageType | None = None,
        **kwargs,
    ) -> ModelResponse:
        """
        Get model response for a given prompt (chat or completion).

        Convenience function for wrapping (chat, completion) API calls.
        Returns special error messages for context length issues.
        """
        sampling_args = sampling_args or {}
        # Resolve message type first
        if message_type is None:
            message_type = self.message_type
        # Normalize sampling args:
        # - If max_tokens is provided for chat, rename to max_completion_tokens
        # - Drop any None-valued entries to avoid sending them to the client
        if "max_tokens" in sampling_args:
            if sampling_args["max_tokens"] is None:
                sampling_args.pop("max_tokens")
            elif message_type == "chat":
                sampling_args["max_completion_tokens"] = sampling_args.pop("max_tokens")
        if (
            "max_completion_tokens" in sampling_args
            and sampling_args["max_completion_tokens"] is None
        ):
            sampling_args.pop("max_completion_tokens")
        clean_sampling_args = {k: v for k, v in sampling_args.items() if v is not None}
        try:
            if message_type == "chat":
                assert isinstance(prompt, list)
                if oai_tools:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=prompt,  # type: ignore
                        tools=oai_tools,
                        **clean_sampling_args,
                    )
                else:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=prompt,  # type: ignore
                        **clean_sampling_args,
                    )
                return response
            elif message_type == "completion":
                if oai_tools:
                    raise ValueError(
                        "oai_tools are not supported for completion tasks."
                    )
                assert isinstance(prompt, str)
                response = await client.completions.create(
                    model=model, prompt=prompt, **clean_sampling_args
                )
                return response
        except Exception as e:
            self.logger.error(f"Error getting model response: {e} \n\nExiting...")
            raise e

    @abstractmethod
    async def rollout(
        self,
        client: AsyncOpenAI,
        model: str,
        prompt: Messages,
        answer: str = "",
        task: str = "default",
        info: Info | None = None,
        sampling_args: SamplingArgs | None = None,
        **kwargs,
    ) -> tuple[Messages, State]:
        """
        Run a rollout for a given prompt.
        Returns a tuple of (completion, state).
        """
        pass

    async def run_rollout_with_semaphore(
        self,
        semaphore: asyncio.Semaphore,
        client: AsyncOpenAI,
        model: str,
        prompt: Messages,
        answer: str = "",
        task: str = "default",
        info: Info | None = None,
        sampling_args: SamplingArgs | None = None,
        **kwargs,
    ) -> tuple[Messages, State]:
        """
        Run a rollout with a semaphore.
        """
        async with semaphore:
            return await self.rollout(
                client, model, prompt, answer, task, info, sampling_args, **kwargs
            )

    async def run_rollouts(
        self,
        client: AsyncOpenAI,
        model: str,
        prompts: list[Messages],
        answers: list[str],
        tasks: list[str],
        infos: list[Info],
        sampling_args: SamplingArgs | None = None,
        max_concurrent: int = -1,
        **kwargs,
    ) -> list[tuple[Messages, State]]:
        """
        Run rollouts for a given list of prompts and return the completions.
        """
        from tqdm.asyncio import tqdm_asyncio

        if max_concurrent > 0:
            semaphore = asyncio.Semaphore(max_concurrent)
            rollout_tasks = [
                self.run_rollout_with_semaphore(
                    semaphore,
                    client,
                    model,
                    prompt,
                    answer,
                    task,
                    info,
                    sampling_args,
                    **kwargs,
                )
                for prompt, answer, task, info in zip(prompts, answers, tasks, infos)
            ]
        else:
            rollout_tasks = [
                self.rollout(
                    client, model, prompt, answer, task, info, sampling_args, **kwargs
                )
                for prompt, answer, task, info in zip(prompts, answers, tasks, infos)
            ]
        return await tqdm_asyncio.gather(
            *rollout_tasks, total=len(prompts), desc=f"Running {len(prompts)} rollouts"
        )

    async def a_generate(
        self,
        inputs: GenerateInputs | Dataset | dict,
        client: AsyncOpenAI | None = None,
        model: str | None = None,
        sampling_args: SamplingArgs | None = None,
        score_rollouts: bool = True,
        max_concurrent: int = -1,
        **kwargs,
    ) -> GenerateOutputs:
        """
        Generate completions and rewards for a given set of inputs.
        """
        if isinstance(inputs, GenerateInputs):
            inputs = inputs.model_dump()
        # use class-level client and model if not provided
        if client is None:
            assert self.client is not None
            client = self.client
        if model is None:
            assert self.model is not None
            model = self.model
        gen_sampling_args = deepcopy(self.sampling_args)
        if sampling_args is not None:
            gen_sampling_args.update(sampling_args)

        # preprocess dataset or GenerateInputs to GenerateOutputs
        results_dict = {}
        if isinstance(inputs, Dataset):
            # get prompt column
            results_dict = {}
            for col in inputs.column_names:
                if col == "info":
                    # handle info column to ensure mutable dicts
                    results_dict[col] = [dict(item) for item in inputs[col]]
                else:
                    results_dict[col] = deepcopy(inputs[col])
        else:
            results_dict = {col: deepcopy(inputs[col]) for col in inputs}
        if "prompt" not in results_dict:
            raise ValueError("prompt column not found in inputs")
        if "answer" not in results_dict and "info" not in results_dict:
            raise ValueError("answer or info column must be found in inputs")
        if "answer" not in results_dict:
            results_dict["answer"] = [""] * len(results_dict["prompt"])
        if "task" not in results_dict:
            results_dict["task"] = ["default"] * len(results_dict["prompt"])
        if "info" not in results_dict:
            results_dict["info"] = [{}] * len(results_dict["prompt"])
        for i, info in enumerate(results_dict["info"]):
            if isinstance(info, str):
                info = json.loads(info)
            if self.oai_tools and "oai_tools" not in info:
                info["oai_tools"] = self.oai_tools

        results_dict["prompt"] = [cleanup_messages(p) for p in results_dict["prompt"]]

        # prepare GenerateOutputs and run rollouts
        results = GenerateOutputs(
            prompt=results_dict["prompt"],
            answer=results_dict["answer"],
            task=results_dict["task"],
            info=results_dict["info"],
            completion=[],
            state=[],
            reward=[],
            metrics={},
        )
        rollouts = await self.run_rollouts(
            prompts=results.prompt,
            answers=results.answer,
            tasks=results.task,
            infos=results.info,
            client=client,
            model=model,
            sampling_args=gen_sampling_args,
            max_concurrent=max_concurrent,
            **kwargs,
        )
        results.completion = [rollout[0] for rollout in rollouts]
        results.state = [rollout[1] for rollout in rollouts]
        if score_rollouts:
            rollout_scores = await self.rubric.score_rollouts(
                prompts=results.prompt,
                completions=results.completion,
                answers=results.answer,
                states=results.state,
                tasks=results.task,
                infos=results.info,
                apply_weights=True,
            )
            results.reward = rollout_scores.reward
            results.metrics = rollout_scores.metrics
        return results

    def generate(
        self,
        inputs: GenerateInputs | Dataset,
        client: AsyncOpenAI | OpenAI,
        model: str | None = None,
        sampling_args: SamplingArgs | None = None,
        score_rollouts: bool = True,
        max_concurrent: int = -1,
        **kwargs,
    ) -> GenerateOutputs:
        if isinstance(client, OpenAI):
            client = AsyncOpenAI(api_key=client.api_key, base_url=client.base_url)
        coro = self.a_generate(
            inputs,
            client,
            model,
            sampling_args,
            score_rollouts,
            max_concurrent,
            **kwargs,
        )

        # check if we're in existing event loop (e.g. Jupyter)
        try:
            loop = asyncio.get_running_loop()
            import nest_asyncio  # type: ignore

            nest_asyncio.apply()
            return loop.run_until_complete(coro)
        except RuntimeError:
            pass

        # script case: create new loop and executor
        executor = ThreadPoolExecutor(max_workers=self.max_workers)
        loop = asyncio.new_event_loop()
        try:
            loop.set_default_executor(executor)
            asyncio.set_event_loop(loop)
            return loop.run_until_complete(coro)
        finally:
            loop.close()
            asyncio.set_event_loop(None)
            # shutdown the executor to prevent thread leaks
            executor.shutdown(wait=False)

    #########################################################
    # Helper functions for evaluation and dataset generation
    #########################################################

    def evaluate(
        self,
        client: AsyncOpenAI | OpenAI,
        model: str,
        sampling_args: SamplingArgs | None = None,
        num_examples: int = -1,
        rollouts_per_example: int = 1,
        score_rollouts: bool = True,
        max_concurrent: int = -1,
        **kwargs,
    ) -> GenerateOutputs:
        """
        Evaluate model on the Environment evaluation dataset.
        """
        if self.eval_dataset is None:
            self.logger.info("eval_dataset is not set, falling back to train dataset")
            assert self.dataset is not None
            inputs = self.get_dataset(n=num_examples)
        else:
            inputs = self.get_eval_dataset(n=num_examples)
        assert inputs is not None, "No dataset found"
        if rollouts_per_example > 1:
            inputs = inputs.repeat(rollouts_per_example)
        results = self.generate(
            inputs,
            client,
            model,
            sampling_args,
            score_rollouts,
            max_concurrent,
            **kwargs,
        )
        return results

    def make_dataset(
        self,
        results: GenerateOutputs,
        push_to_hub: bool = False,
        hub_name: str | None = None,
        state_columns: list[str] | None = None,
        **kwargs,
    ) -> Dataset:
        """
        Make a dataset from the evaluation results.
        """
        # TODO: enable saving of multimodal datasets
        state_columns = state_columns or []

        if push_to_hub and hub_name is None:
            raise ValueError("hub_name must be provided if push_to_hub is True")

        cols = ["prompt", "completion", "answer", "task", "reward"]

        results_dict = {
            "prompt": results.prompt,
            "completion": [],
            "answer": results.answer,
            "task": results.task,
            "reward": results.reward,
        }
        if results.info[0] != {}:
            results_dict["info"] = results.info
            cols.append("info")
        for i in range(len(results.completion)):
            results_dict["completion"].append(
                sanitize_tool_calls(results.completion[i])
            )
        results_dict.update(results.metrics)
        cols.extend(results.metrics.keys())
        if results.state[0] is not None:
            for col in state_columns:
                if col in results.state[0]:
                    results_dict[col] = [state[col] for state in results.state]
                    cols.append(col)
                else:
                    self.logger.warning(
                        f"Column {col} not found in state, skipping from dataset."
                    )
        dataset = Dataset.from_dict({col: results_dict[col] for col in cols})
        if push_to_hub:
            assert hub_name is not None
            dataset.push_to_hub(hub_name)
        return dataset

    #########################################################
    # Optional helper functions for parsing vLLM completions
    #########################################################

    def parse_chat_completion_logprobs(
        self, chat_completion: ChatCompletion
    ) -> list[float]:
        """Parses the completion logprobs from a vLLM chat completion"""
        assert len(chat_completion.choices) == 1, (
            "Response should always have one choice"
        )
        assert chat_completion.choices[0].logprobs is not None, (
            "Logprobs should not be None. Make sure to set logprobs=True in the extra body when making the request to /v1/chat/completions"
        )
        assert chat_completion.choices[0].logprobs.content is not None, (
            "Logprob content should not be None. Make sure to set logprobs=True in the extra body when making the request to /v1/chat/completions"
        )
        logprobs = [
            logprob.logprob for logprob in chat_completion.choices[0].logprobs.content
        ]
        return logprobs

    def parse_completion_logprobs(self, completion: Completion) -> list[float]:
        """Parses the completion logprobs from a vLLM chat completion"""
        assert len(completion.choices) == 1, "Response should always have one choice"
        assert completion.choices[0].logprobs is not None, (
            "Logprobs should not be None. Make sure to set logprobs=True in the extra body when making the request to /v1/completions"
        )
        assert completion.choices[0].logprobs.token_logprobs is not None, (
            "Logprob token_logprobs should not be None. Make sure to set logprobs=True in the extra body when making the request to /v1/completions"
        )
        return completion.choices[0].logprobs.token_logprobs

    def parse_chat_completion_tokens(
        self, chat_completion: ChatCompletion
    ) -> list[int]:
        """Parses the output token ids from a list of chat completions returned by vLLM OAI server."""
        assert len(chat_completion.choices) == 1, (
            "Response should always have one choice"
        )
        assert chat_completion.choices[0].logprobs is not None, (
            "Logprobs should not be None. Make sure to set logprobs=True in the extra body when making the request to /v1/chat/completions"
        )
        assert chat_completion.choices[0].logprobs.content is not None, (
            "Logprob content should not be None. Make sure to set logprobs=True in the extra body when making the request to /v1/chat/completions"
        )
        tokens = [
            # tokens are token_id:<int> because we request `return_tokens_as_token_ids` from vllm in GRPOTrainer
            int(token.token.split(":")[-1])
            for token in chat_completion.choices[0].logprobs.content
        ]
        return tokens

    def parse_completion_tokens(self, completion: Completion) -> list[int]:
        """Parses the output token ids from a list of chat completions returned by vLLM OAI server."""
        assert len(completion.choices) == 1, "Response should always have one choice"
        assert completion.choices[0].logprobs is not None, (
            "Logprobs should not be None. Make sure to set logprobs=True in the extra body when making the request to /v1/completions"
        )
        assert completion.choices[0].logprobs.tokens is not None, (
            "Logprob tokens should not be None. Make sure to set logprobs=True in the extra body when making the request to /v1/completions"
        )
        tokens = [
            # tokens are token_id:<int> because we request `return_tokens_as_token_ids` from vllm in GRPOTrainer
            int(token.split(":")[-1])
            for token in completion.choices[0].logprobs.tokens
        ]
        return tokens

    def process_chat_format_vllm(
        self,
        prompt: list[ChatMessage],
        completion: list[ChatMessage],
        state: State,
        processing_class: "PreTrainedTokenizerBase",
        mask_env_responses: bool = False,
    ) -> tuple[list[int], list[int], list[int], list[int], list[float]]:
        """
        Process chat format conversations using incremental prefixes.
        """
        responses = state["responses"]
        responses_idx = 0
        zipped = []
        for turn in completion:
            if turn["role"] == "assistant":
                zipped.append((turn, responses[responses_idx]))
                responses_idx += 1
            else:
                zipped.append((turn, None))
        assert len(responses) == responses_idx, "Responses not fully consumed"
        assert len(zipped) == len(completion), "Length mismatch"
        prompt_ids: list[int] = processing_class.apply_chat_template(
            conversation=prompt,  # type: ignore
            add_generation_prompt=True,
        )
        messages_consumed = [m for m in prompt]
        prompt_mask: list[int] = [0] * len(prompt_ids)
        completion_ids: list[int] = []
        completion_mask: list[int] = []
        completion_logprobs: list[float] = []
        i = 0
        while i < len(zipped):
            message, response = zipped[i]
            # assistant case -- use response
            if message["role"] == "assistant":
                assert response is not None, "Response should not be None"
                completion_turn_ids = self.parse_chat_completion_tokens(response)
                completion_turn_mask = [1] * len(completion_turn_ids)
                completion_turn_logprobs = self.parse_chat_completion_logprobs(response)
                completion_ids.extend(completion_turn_ids)
                completion_mask.extend(completion_turn_mask)
                completion_logprobs.extend(completion_turn_logprobs)
                messages_consumed.append(message)
                i += 1
            # user/tool case -- use message
            else:
                assert message["role"] == "user" or message["role"] == "tool"
                # Collect all consecutive non-assistant messages
                consecutive_messages = [message]
                j = i + 1
                while j < len(zipped) and zipped[j][0]["role"] != "assistant":
                    consecutive_messages.append(zipped[j][0])
                    j += 1
                token_prefix: list[int] = processing_class.apply_chat_template(
                    conversation=messages_consumed  # type: ignore
                )
                token_prefix_with_turn: list[int] = (
                    processing_class.apply_chat_template(
                        conversation=messages_consumed + consecutive_messages,  # type: ignore
                    )
                )
                assert token_prefix_with_turn[: len(token_prefix)] == token_prefix, (
                    f"Token prefix mismatch. Token prefix: {token_prefix}, token prefix with turn: {token_prefix_with_turn}"
                )
                completion_turn_ids = token_prefix_with_turn[len(token_prefix) :]
                if mask_env_responses:
                    completion_turn_mask = [0] * len(completion_turn_ids)
                else:
                    completion_turn_mask = [1] * len(completion_turn_ids)
                completion_turn_logprobs = [0.0] * len(completion_turn_ids)
                completion_ids.extend(completion_turn_ids)
                completion_mask.extend(completion_turn_mask)
                completion_logprobs.extend(completion_turn_logprobs)
                messages_consumed.extend(consecutive_messages)
                i = j
        return (
            prompt_ids,
            prompt_mask,
            completion_ids,
            completion_mask,
            completion_logprobs,
        )

    def process_completion_format_vllm(
        self,
        prompt: str,
        completion: str,
        state: State,
        processing_class: "PreTrainedTokenizerBase",
        mask_env_responses: bool = False,
    ) -> tuple[list[int], list[int], list[int], list[int], list[float]]:
        """
        Process completion format conversations using incremental prefixes.
        """
        responses: list[Completion] = state["responses"]
        responses_start_idx: list[int] = state["responses_start_idx"]
        assert len(responses) == len(responses_start_idx), (
            "Should have an index for each completion response"
        )

        idx = 0
        zipped: list[tuple[str, Completion | None]] = []
        for response, response_start_idx in zip(responses, responses_start_idx):
            if response_start_idx > idx:
                # non-model-generated section
                zipped.append((completion[idx:response_start_idx], None))
            response_text = response.choices[0].text or ""
            zipped.append((response_text, response))
            idx = response_start_idx + len(response_text)
        assert idx == len(completion), "Completion not fully consumed"

        prompt_ids: list[int] = processing_class.encode(prompt)
        rollout_consumed = prompt
        prompt_mask: list[int] = [0] * len(prompt_ids)
        completion_ids: list[int] = []
        completion_mask: list[int] = []
        completion_logprobs: list[float] = []
        i = 0
        while i < len(zipped):
            text, response = zipped[i]
            # model-generated case -- use response
            if response is not None:
                completion_turn_ids = self.parse_completion_tokens(response)
                completion_turn_mask = [1] * len(completion_turn_ids)
                completion_turn_logprobs = self.parse_completion_logprobs(response)
                completion_ids.extend(completion_turn_ids)
                completion_mask.extend(completion_turn_mask)
                completion_logprobs.extend(completion_turn_logprobs)
                rollout_consumed += text
                i += 1
            # non-model-generated (user/tool case) -- use text
            else:
                token_prefix: list[int] = processing_class.encode(rollout_consumed)
                token_prefix_with_turn: list[int] = processing_class.encode(
                    rollout_consumed + text
                )
                assert token_prefix_with_turn[: len(token_prefix)] == token_prefix, (
                    f"Token prefix mismatch. Token prefix: {token_prefix}, token prefix with turn: {token_prefix_with_turn}"
                )
                completion_turn_ids = token_prefix_with_turn[len(token_prefix) :]
                if mask_env_responses:
                    completion_turn_mask = [0] * len(completion_turn_ids)
                else:
                    completion_turn_mask = [1] * len(completion_turn_ids)
                completion_turn_logprobs = [0.0] * len(completion_turn_ids)
                completion_ids.extend(completion_turn_ids)
                completion_mask.extend(completion_turn_mask)
                completion_logprobs.extend(completion_turn_logprobs)
                rollout_consumed += text
                i += 1
        return (
            prompt_ids,
            prompt_mask,
            completion_ids,
            completion_mask,
            completion_logprobs,
        )

    def process_env_results_vllm(
        self,
        prompts: list[Messages],
        completions: list[Messages],
        states: list[State],
        rewards: list[float],
        processing_class: "PreTrainedTokenizerBase",
        max_seq_len: int = -1,
        mask_env_responses: bool = False,
        mask_truncated_completions: bool = False,
        zero_truncated_completions: bool = False,
    ) -> ProcessedOutputs:
        """
        Process results with vLLM tokens/logprobs.
        """
        is_chat_format = self.message_type == "chat"

        all_prompt_ids = []
        all_prompt_masks = []
        all_completion_ids = []
        all_completion_masks = []
        all_completion_logprobs = []
        all_rewards = []
        for i, (prompt, completion, state, reward) in enumerate(
            zip(prompts, completions, states, rewards)
        ):
            # Format-specific processing
            if is_chat_format:
                assert isinstance(prompt, list) and isinstance(completion, list)
                (
                    prompt_ids,
                    prompt_mask,
                    completion_ids,
                    completion_mask,
                    completion_logprobs,
                ) = self.process_chat_format_vllm(
                    prompt, completion, state, processing_class, mask_env_responses
                )
            else:
                assert isinstance(prompt, str) and isinstance(completion, str)
                (
                    prompt_ids,
                    prompt_mask,
                    completion_ids,
                    completion_mask,
                    completion_logprobs,
                ) = self.process_completion_format_vllm(
                    prompt, completion, state, processing_class, mask_env_responses
                )
            is_truncated = False
            if max_seq_len > 0 and len(prompt_ids) + len(completion_ids) > max_seq_len:
                if len(prompt_ids) > max_seq_len:
                    prompt_ids = prompt_ids[:max_seq_len]
                completion_ids = completion_ids[: max_seq_len - len(prompt_ids)]
                completion_mask = completion_mask[: max_seq_len - len(prompt_ids)]
                completion_logprobs = completion_logprobs[
                    : max_seq_len - len(prompt_ids)
                ]
                is_truncated = True
            if is_truncated and mask_truncated_completions:
                completion_mask = [0] * len(completion_ids)
            assert len(prompt_ids) == len(prompt_mask), (
                f"Prompt ids: {len(prompt_ids)}, prompt mask: {len(prompt_mask)}"
            )
            assert len(completion_ids) == len(completion_mask), (
                f"Completion ids: {len(completion_ids)}, completion mask: {len(completion_mask)}"
            )
            assert (
                len(completion_mask) == len(completion_ids) == len(completion_logprobs)
            ), (
                f"completion mask: {len(completion_mask)}, completion ids: {len(completion_ids)}, completion logprobs: {len(completion_logprobs)}"
            )
            all_prompt_ids.append(prompt_ids)
            all_prompt_masks.append(prompt_mask)
            all_completion_ids.append(completion_ids)
            all_completion_masks.append(completion_mask)
            all_completion_logprobs.append(completion_logprobs)
            if zero_truncated_completions and is_truncated:
                all_rewards.append(0)
            else:
                all_rewards.append(reward)
        return ProcessedOutputs(
            prompt_ids=all_prompt_ids,
            prompt_mask=all_prompt_masks,
            completion_ids=all_completion_ids,
            completion_mask=all_completion_masks,
            completion_logprobs=all_completion_logprobs,
            rewards=all_rewards,
        )

    # alias for process_env_results_vllm
    process_env_results = process_env_results_vllm



================================================
FILE: verifiers/envs/multiturn_env.py
================================================
from abc import abstractmethod

from openai import AsyncOpenAI

from verifiers.envs.environment import Environment
from verifiers.types import (
    ChatCompletion,
    ChatMessage,
    Completion,
    Info,
    Messages,
    SamplingArgs,
    State,
)
from verifiers.utils.async_utils import maybe_await


class MultiTurnEnv(Environment):
    def __init__(self, max_turns: int = 10, **kwargs):
        super().__init__(**kwargs)
        self.max_turns = max_turns

    async def setup_state(self, state: State, **kwargs) -> State:
        return state

    @abstractmethod
    async def is_completed(self, messages: Messages, state: State, **kwargs) -> bool:
        pass

    @abstractmethod
    async def env_response(
        self, messages: Messages, state: State, **kwargs
    ) -> tuple[Messages, State]:
        """
        Generate a response from the environment (messages, state).
        """
        pass

    async def rollout(
        self,
        client: AsyncOpenAI,
        model: str,
        prompt: Messages,
        answer: str = "",
        task: str = "default",
        info: Info | None = None,
        sampling_args: SamplingArgs | None = None,
        **kwargs,
    ) -> tuple[Messages, State]:
        """
        Generate a multi-turn rollout with the environment (messages, state).
        """
        info = info or {}
        is_completed = False
        state = {
            "prompt": prompt,
            "completion": [],
            "answer": answer,
            "task": task,
            "info": info,
            "responses": [],
            "turn": 0,
        }
        state = await maybe_await(self.setup_state, state, **kwargs)
        if self.message_type == "chat":
            assert isinstance(prompt, list)
            completion = []
        else:
            assert isinstance(prompt, str)
            completion = ""
            state["responses_start_idx"] = []
        rollout = list(prompt) if not isinstance(prompt, str) else prompt
        while not is_completed:
            if await maybe_await(self.is_completed, rollout, state, **kwargs):
                is_completed = True
                break
            response = await self.get_model_response(
                client=client,
                model=model,
                prompt=rollout,
                oai_tools=info.get("oai_tools", None),
                sampling_args=sampling_args,
                message_type=self.message_type,
            )
            state["responses"].append(response)
            if self.message_type == "chat":
                assert isinstance(rollout, list)
                assert isinstance(completion, list)
                assert isinstance(response, ChatCompletion)
                response_text: str = response.choices[0].message.content or ""  # type: ignore
                response_message: ChatMessage = {
                    "role": "assistant",
                    "content": response_text,
                }
                if response.choices[0].message.tool_calls:
                    response_message["tool_calls"] = response.choices[  # type: ignore
                        0
                    ].message.tool_calls
                rollout.append(response_message)
                completion.append(response_message)
            else:
                assert isinstance(rollout, str)
                assert isinstance(completion, str)
                assert isinstance(response, Completion)
                state["responses_start_idx"].append(len(completion))
                response_text: str = response.choices[0].text or ""  # type: ignore
                rollout += response_text
                completion += response_text
            state["turn"] += 1
            if (
                await maybe_await(self.is_completed, rollout, state, **kwargs)
                or state["turn"] >= self.max_turns
            ):
                is_completed = True
            else:
                env_msgs, state = await maybe_await(
                    self.env_response, rollout, state, **kwargs
                )
                if self.message_type == "chat":
                    assert isinstance(env_msgs, list)
                    assert isinstance(rollout, list)
                    assert isinstance(completion, list)
                    rollout += env_msgs
                    completion += env_msgs
                else:
                    assert isinstance(env_msgs, str)
                    assert isinstance(rollout, str)
                    assert isinstance(completion, str)
                    rollout += env_msgs
                    completion += env_msgs
        return completion, state



================================================
FILE: verifiers/envs/singleturn_env.py
================================================
from verifiers.envs.multiturn_env import MultiTurnEnv
from verifiers.types import Messages, State


class SingleTurnEnv(MultiTurnEnv):
    """
    Environment for single-turn tasks (chat or completion).
    """

    async def is_completed(self, messages: Messages, state: State, **kwargs) -> bool:
        return len(state["responses"]) > 0

    async def env_response(
        self, messages: Messages, state: State, **kwargs
    ) -> tuple[Messages, State]:
        # never called in MultiTurnEnv.rollout
        return [{"role": "user", "content": ""}], state



================================================
FILE: verifiers/envs/stateful_tool_env.py
================================================
import json
from abc import abstractmethod
from typing import Callable

from verifiers.envs.tool_env import ToolEnv
from verifiers.types import ChatCompletionMessageToolCall, Message, Messages, State
from verifiers.utils.tool_utils import convert_func_to_oai_tool


class StatefulToolEnv(ToolEnv):
    def __init__(
        self,
        tools: list[Callable] | None = None,
        max_turns: int = 10,
        error_formatter: Callable[[Exception], str] = lambda e: f"{str(e)}",
        **kwargs,
    ):
        super().__init__(
            tools=tools,
            max_turns=max_turns,
            error_formatter=error_formatter,
            **kwargs,
        )
        self.tools = tools or []
        self.max_turns = max_turns
        self.error_formatter = error_formatter
        self.oai_tools = [convert_func_to_oai_tool(tool) for tool in self.tools]
        self.tool_map = {tool.__name__: tool for tool in self.tools}

    @abstractmethod
    def update_tool_args(
        self, tool_args: dict, messages: Messages, state: State, **kwargs
    ) -> dict:
        """Update tool arguments and/or state (in-place) based on messages and state."""
        pass

    async def call_tool(
        self, tool_name: str, tool_args: dict, tool_call_id: str, **kwargs
    ) -> Message:
        """Call a tool based on JSON command."""
        try:
            tool_func = self.tool_map[tool_name]
            result = str(tool_func(**tool_args))
            return {
                "role": "tool",
                "content": str(result),
                "tool_call_id": tool_call_id,
            }
        except Exception as e:
            return {
                "role": "tool",
                "content": self.error_formatter(e),
                "tool_call_id": tool_call_id,
            }

    async def env_response(
        self, messages: Messages, state: State, **kwargs
    ) -> tuple[Messages, State]:
        assert isinstance(messages, list)
        assert "tool_calls" in messages[-1]
        tool_messages = []
        for tool_call in messages[-1]["tool_calls"]:
            assert isinstance(tool_call, ChatCompletionMessageToolCall)
            tool_name: str = tool_call.function.name
            tool_args: dict = json.loads(tool_call.function.arguments)
            tool_call_id: str = tool_call.id or ""
            tool_args = self.update_tool_args(tool_args, messages, state, **kwargs)
            tool_message: Message = await self.call_tool(
                tool_name, tool_args, tool_call_id
            )
            tool_messages.append(tool_message)
        return tool_messages, state



================================================
FILE: verifiers/envs/textarena_env.py
================================================
import random
from copy import deepcopy
from typing import Any, Callable

try:
    import nltk  # type: ignore
except ImportError:
    print("nltk is not installed. Please install it with `uv pip install nltk`.")
    exit(1)

from datasets import Dataset

# monkey-patch nltk.download to always be quiet before importing textarena
_original_nltk_download = nltk.download
nltk.download = lambda *args, **kwargs: _original_nltk_download(
    *args, **{**kwargs, "quiet": True}
)

try:
    import textarena as ta  # type: ignore
except ImportError:
    print(
        "textarena is not installed. Please install it with `uv pip install textarena`."
    )
    exit(1)

from verifiers.envs.multiturn_env import MultiTurnEnv  # noqa: E402
from verifiers.parsers.xml_parser import XMLParser  # noqa: E402
from verifiers.rubrics.rubric import Rubric  # noqa: E402
from verifiers.types import (  # noqa: E402
    Messages,
    State,
)


class TextArenaEnv(MultiTurnEnv):
    """
    Wrapper for TextArena environments.
    """

    def __init__(
        self,
        game: str = "Wordle-v0",
        num_train_examples: int = 1000,
        num_eval_examples: int = 0,
        system_prompt: str | None = None,
        parser: XMLParser | None = None,
        rubric: Rubric | None = None,
        feedback_fn: Callable[[str], str] = lambda x: x,
        seed: int = 0,
        **kwargs,
    ):
        # default parser in textarena is XMLParser
        parser = parser or XMLParser(fields=["think", "guess"], answer_field="guess")

        self.game = game
        self.ta_env = ta.make(env_id=game)
        self.num_train_examples = num_train_examples
        self.num_eval_examples = num_eval_examples
        self.seed = seed
        self.feedback_fn = feedback_fn

        nltk.download("words", quiet=True)
        nltk.download("averaged_perceptron_tagger_eng", quiet=True)
        dataset, eval_dataset = self.ta_to_hf()

        super().__init__(
            dataset=dataset,
            eval_dataset=eval_dataset,
            system_prompt=system_prompt,
            parser=parser,
            rubric=rubric,
            message_type="chat",
            **kwargs,
        )

    async def is_completed(
        self, messages: Messages, state: State, **kwargs: Any
    ) -> bool:
        if "is_finished" in state and state["is_finished"]:
            state.pop("ta_env")
            return state["is_finished"]
        self.parser
        return False

    async def env_response(
        self, messages: Messages, state: State, **kwargs: Any
    ) -> tuple[Messages, State]:
        # load env
        if "ta_env" not in state:
            ta_env = deepcopy(self.ta_env)
            ta_env.reset(num_players=1)
            ta_env.state.game_state["secret_word"] = state["answer"]
            state["ta_env"] = ta_env
        else:
            ta_env = state["ta_env"]
        # parse guess
        assert isinstance(messages[-1], dict)
        guess = self.parser.parse_answer(messages)
        # step env
        is_finished, _ = ta_env.step(str(guess))
        state["is_finished"] = is_finished
        _, observation = ta_env.get_observation()
        feedback = self.feedback_fn(observation)
        return [{"role": "user", "content": str(feedback)}], state

    def ta_to_hf(self) -> tuple[Dataset, Dataset | None]:
        dataset_rows = []
        eval_dataset_rows = []
        ta_env = ta.make(env_id=self.game)
        ta_env.reset(num_players=1)
        _, user_prompt = ta_env.get_observation()
        words = ta_env.word_list
        # set seed
        random.seed(self.seed)
        for i in range(self.num_train_examples + self.num_eval_examples):
            question = user_prompt
            answer = random.choice(words)
            if i < self.num_train_examples:
                dataset_rows.append({"question": question, "answer": answer})
            else:
                eval_dataset_rows.append({"question": question, "answer": answer})
        dataset = Dataset.from_list(dataset_rows)
        if self.num_eval_examples > 0:
            eval_dataset = Dataset.from_list(eval_dataset_rows)
        else:
            eval_dataset = None
        return dataset, eval_dataset



================================================
FILE: verifiers/envs/tool_env.py
================================================
import json
from typing import Any, Callable

from verifiers.envs.multiturn_env import MultiTurnEnv
from verifiers.types import ChatCompletionMessageToolCall, Message, Messages, State
from verifiers.utils.async_utils import maybe_await
from verifiers.utils.tool_utils import convert_func_to_oai_tool


class ToolEnv(MultiTurnEnv):
    def __init__(
        self,
        tools: list[Callable] | None = None,
        max_turns: int = 10,
        error_formatter: Callable[[Exception], str] = lambda e: f"{str(e)}",
        **kwargs,
    ):
        self.tools = tools or []
        self.max_turns = max_turns
        self.error_formatter = error_formatter
        self.oai_tools = [convert_func_to_oai_tool(tool) for tool in self.tools]
        self.tool_map = {tool.__name__: tool for tool in self.tools}
        super().__init__(oai_tools=self.oai_tools, max_turns=max_turns, **kwargs)

    async def is_completed(
        self, messages: Messages, state: State, **kwargs: Any
    ) -> bool:
        assert isinstance(messages, list)
        is_assistant_message = messages[-1]["role"] == "assistant"
        no_tool_calls = (
            "tool_calls" not in messages[-1] or messages[-1]["tool_calls"] is None
        )
        return is_assistant_message and no_tool_calls

    async def call_tool(
        self, tool_name: str, tool_args: dict, tool_call_id: str, **kwargs
    ) -> Message:
        """Call a tool based on JSON command."""
        try:
            tool_func = self.tool_map[tool_name]
            result = str(await maybe_await(tool_func, **tool_args))
            return {
                "role": "tool",
                "content": str(result),
                "tool_call_id": tool_call_id,
            }
        except Exception as e:
            return {
                "role": "tool",
                "content": self.error_formatter(e),
                "tool_call_id": tool_call_id,
            }

    async def env_response(
        self, messages: Messages, state: State, **kwargs
    ) -> tuple[Messages, State]:
        assert isinstance(messages, list)
        assert "tool_calls" in messages[-1]
        tool_messages = []
        for tool_call in messages[-1]["tool_calls"]:
            assert isinstance(tool_call, ChatCompletionMessageToolCall)
            tool_name: str = tool_call.function.name
            tool_args: dict = json.loads(tool_call.function.arguments)
            tool_call_id: str = tool_call.id or ""
            tool_message: Message = await self.call_tool(
                tool_name, tool_args, tool_call_id
            )
            tool_messages.append(tool_message)
        return tool_messages, state



================================================
FILE: verifiers/inference/__init__.py
================================================
[Empty file]


================================================
FILE: verifiers/inference/vllm_client.py
================================================
import atexit
import logging
import time

import requests
import torch
from openai import AsyncOpenAI
from requests import ConnectionError
from requests.adapters import HTTPAdapter
from trl.import_utils import is_requests_available, is_vllm_available
from vllm.distributed.device_communicators.pynccl import (
    PyNcclCommunicator,  # type: ignore
)
from vllm.distributed.utils import StatelessProcessGroup  # type: ignore

logger = logging.getLogger(__name__)


class VLLMClient(AsyncOpenAI):
    """
    A client class to interact with a vLLM server.

    This class provides methods to generate completions, initialize and manage weight update groups, and update model
    weights in a distributed setting. Before using it, start the vLLM server with `trl vllm-serve`.

    Args:
        host (`str`, *optional*, defaults to `"0.0.0.0"`):
            IP address of the vLLM server.
        server_port (`int`, *optional*, defaults to `8000`):
            Port number of the vLLM server.
        group_port (`int`, *optional*, defaults to `51216`):
            Port number for the weight update group.
        connection_timeout (`float`, *optional*, defaults to `0.0`):
            Total timeout duration in seconds to wait for the server to be up. If the server is not up after the
            timeout, a `ConnectionError` is raised.

    Examples:
        Run the vLLM server with the model `Qwen/Qwen2.5-7B`:

        ```
        $ trl vllm-serve --model Qwen/Qwen2.5-7B
        ...
        INFO:     Application startup complete.
        INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
        ```

        Use the client to generate completions and update model weights:

        ```python
        >>> from trl.extras.vllm_client import VLLMClient
        >>> client = VLLMClient()
        >>> client.generate(["Hello, AI!", "Tell me a joke"])
        [[2980, 498, 1492, 752, 448, 264, 13027, 8645, 30, 358, 2776, 4460, 311, 3270, 264, 2025],
         [911, 7988, 1251, 382, 3838, 653, 498, 1618, 4325, 879, 2581, 20027, 264, 21428, 30, 362]]

        >>> from transformers import AutoModelForCausalLM
        >>> model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B", device_map="cuda")
        >>> client.update_model_params(model)
        ```
    """

    def __init__(
        self,
        host: str = "0.0.0.0",
        port: int = 8000,
        group_port: int = 51216,
        connection_timeout: float = 0.0,
    ):
        if not is_requests_available():
            raise ImportError(
                "requests is not installed. Please install it with `pip install requests`."
            )
        if not is_vllm_available():
            raise ImportError(
                "vLLM is not installed. Please install it with `pip install vllm`."
            )

        super().__init__(base_url=f"http://{host}:{port}/v1", api_key="local")
        self.session = requests.Session()
        # Configure connection pooling to handle rapid requests better
        adapter = HTTPAdapter(
            pool_connections=10, pool_maxsize=10, max_retries=3, pool_block=False
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        self.host = host
        self.server_port = port  # Renamed from server_port to port to match super init
        self.server_url = f"http://{self.host}:{self.server_port}"

        self.group_port = group_port
        self.check_server(connection_timeout)  # check server and fail after timeout

    def check_server(self, total_timeout: float = 0.0, retry_interval: float = 2.0):
        """
        Check server availability with retries on failure, within a total timeout duration. If the server is not up
        after the total timeout duration, raise a `ConnectionError`.

        Args:
            retry_interval (`float`, *optional*, defaults to `2.0`):
                Interval in seconds between retries.
            total_timeout (`float`, *optional*, defaults to `0.0`):
                Total timeout duration in seconds.
        """
        url = f"{self.server_url}/health"
        start_time = time.time()  # Record the start time

        while True:
            try:
                response = requests.get(url)  # type: ignore
            except requests.exceptions.RequestException as exc:  # type: ignore
                # Check if the total timeout duration has passed
                elapsed_time = time.time() - start_time
                if elapsed_time >= total_timeout:
                    raise ConnectionError(  # type: ignore
                        f"The vLLM server can't be reached at {self.host}:{self.server_port} after {total_timeout} "
                        "seconds. Make sure the server is running by running `trl vllm-serve`."
                    ) from exc
            else:
                if response.status_code == 200:
                    logger.info("Server is up!")
                    return None

            # Retry logic: wait before trying again
            logger.info(
                f"Server is not up yet. Retrying in {retry_interval} seconds..."
            )
            time.sleep(retry_interval)

    def init_communicator(self):
        """
        Initializes the weight update group in a distributed setup for model synchronization.
        """

        # Get the world size from the server
        url = f"{self.server_url}/get_world_size"
        try:
            response = requests.get(url)
        except Exception as e:
            logger.error(f"Failed to get world size: {e}")
            raise

        if response.status_code == 200:
            vllm_world_size = response.json()["world_size"]
            logger.info(f"vLLM world size: {vllm_world_size}")
        else:
            raise Exception(f"Request failed: {response.status_code}, {response.text}")

        world_size = vllm_world_size + 1  # add the client to the world
        self.rank = vllm_world_size  # the client's rank is the last process
        logger.info(f"Client rank: {self.rank}, total world size: {world_size}")

        # Initialize weight update group
        url = f"{self.server_url}/init_communicator"
        # Send the actual host address for the StatelessProcessGroup connection
        try:
            response = self.session.post(
                url,
                json={
                    "host": self.host,
                    "port": self.group_port,
                    "world_size": world_size,
                },
            )
        except Exception as e:
            logger.error(f"Failed to init communicator: {e}")
            raise

        if response.status_code != 200:
            raise Exception(f"Request failed: {response.status_code}, {response.text}")

        # Brief delay to allow server initialization. While not strictly required (client socket will retry on
        # connection failure), this prevents log warnings like:
        # [W416 23:24:57.460001114 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3
        time.sleep(0.1)

        # Set up the communication group for weight broadcasting
        pg = StatelessProcessGroup.create(
            host=self.host, port=self.group_port, rank=self.rank, world_size=world_size
        )
        # Use device 0 like the old code - this seems to work better for multi-GPU setups
        device = 0
        logger.info(
            f"Initializing PyNcclCommunicator on device {device}, rank {self.rank}, world_size {world_size}"
        )
        self.pynccl_comm = PyNcclCommunicator(pg, device=device)

        # When the client object is deleted, close the weight update group
        atexit.register(self.close_communicator)

    def update_named_param(self, name: str, weights: torch.Tensor):
        """
        Updates a specific named parameter in the model and broadcasts it to other processes.

        Args:
            name (`str`):
                Name of the layer whose weights are being updated.
            weights (`torch.Tensor`):
                Tensor containing the updated weights.
        """
        dtype, shape = str(weights.dtype), tuple(weights.shape)
        url = f"{self.server_url}/update_named_param"

        # Add timeout to prevent hanging on HTTP request
        try:
            response = self.session.post(
                url, json={"name": name, "dtype": dtype, "shape": shape}, timeout=300.0
            )
        except requests.exceptions.Timeout:
            logger.error(f"Timeout waiting for server response for {name} after 300s")
            raise Exception(f"Request timeout for {name} after 300s")
        except Exception as e:
            logger.error(f"Error sending request for {name}: {e}")
            raise

        if response.status_code != 200:
            raise Exception(f"Request failed: {response.status_code}, {response.text}")

        # Broadcast the weights to the other processes
        self.pynccl_comm.broadcast(weights, src=self.rank)
        self.pynccl_comm.group.barrier()

    def reset_prefix_cache(self):
        """
        Resets the prefix cache for the model.
        """
        url = f"{self.server_url}/reset_prefix_cache"
        response = self.session.post(url)
        if response.status_code != 200:
            raise Exception(f"Request failed: {response.status_code}, {response.text}")

    def get_num_background_tasks(self):
        """
        Gets the number of background tasks.
        """
        url = f"{self.server_url}/get_num_background_tasks"
        response = self.session.post(url)
        return response.json()["num_background_tasks"]

    def close_communicator(self):
        """
        Closes the weight update group and cleans up the communication group.
        """
        url = f"http://{self.host}:{self.server_port}/close_communicator"

        try:
            response = self.session.post(url)
        except ConnectionError:
            # The server might be already down, so we don't need to close the communicator
            pass
        else:
            if response.status_code != 200:
                raise Exception(
                    f"Request failed: {response.status_code}, {response.text}"
                )



================================================
FILE: verifiers/inference/vllm_server.py
================================================
import asyncio
import os
import signal
from argparse import Namespace
from typing import Sequence

try:
    import torch  # type: ignore
except ImportError:
    print("torch is not installed. Please install it with `uv pip install torch`.")
    exit(1)

try:
    import uvloop  # type: ignore
except ImportError:
    print("uvloop is not installed. Please install it with `uv pip install uvloop`.")
    exit(1)

try:
    from fastapi import Request  # type: ignore
except ImportError:
    print("fastapi is not installed. Please install it with `uv pip install fastapi`.")
    exit(1)

try:
    from vllm.distributed.device_communicators.pynccl import (  # type: ignore
        PyNcclCommunicator,
    )
    from vllm.distributed.parallel_state import get_world_group  # type: ignore
    from vllm.distributed.utils import StatelessProcessGroup  # type: ignore
    from vllm.engine.arg_utils import AsyncEngineArgs  # type: ignore
    from vllm.engine.async_llm_engine import AsyncLLMEngine  # type: ignore
    from vllm.entrypoints.launcher import serve_http  # type: ignore
    from vllm.entrypoints.openai.api_server import (  # type: ignore
        build_app,
        create_server_socket,
        init_app_state,
    )
    from vllm.entrypoints.openai.cli_args import (  # type: ignore
        make_arg_parser,
        validate_parsed_serve_args,
    )
    from vllm.usage.usage_lib import UsageContext  # type: ignore
    from vllm.utils import FlexibleArgumentParser, set_ulimit  # type: ignore
except ImportError:
    print("vLLM is not installed. Please install it with `pip install vllm`.")
    exit(1)

os.environ["VLLM_WORKER_MULTIPROC_METHOD"] = "spawn"

# Weight update throttling
MAX_CONCURRENT_WEIGHT_UPDATES = 10
weight_update_semaphore = asyncio.Semaphore(MAX_CONCURRENT_WEIGHT_UPDATES)

# Track background tasks for cleanup
background_tasks = set()


class WeightSyncWorkerExtension:
    """
    A vLLM worker extension that enables weight synchronization between a client and multiple server workers.

    This worker uses a `StatelessProcessGroup` to establish communication and a `PyNcclCommunicator` to handle
    efficient GPU-based communication using NCCL. The primary purpose of this class is to receive updated model weights
    from a client process and distribute them to all worker processes participating in model inference.
    """

    pynccl_comm = None  # Communicator for weight updates
    client_rank = None  # Source rank for broadcasting updated weights

    def init_communicator(self, host: str, port: int, world_size: int) -> None:
        """
        Initializes the weight update communicator using a stateless process group.

        This method creates a `StatelessProcessGroup` that allows external training processes to
        communicate with vLLM workers without interfering with the global torch distributed group.

        Args:
            host (`str`):
                Hostname or IP address of the master node.
            port (`int`):
                Port number to be used for communication.
            world_size (`int`):
                Total number of participating processes in the update group.
        """
        if self.pynccl_comm is not None:
            raise RuntimeError(
                "Weight update group already initialized. Call close_communicator first."
            )

        rank = get_world_group().rank
        pg = StatelessProcessGroup.create(
            host=host, port=port, rank=rank, world_size=world_size
        )
        self.pynccl_comm = PyNcclCommunicator(pg, device=self.device)  # type: ignore
        self.client_rank = world_size - 1

    def update_named_param(self, name: str, dtype: str, shape: Sequence[int]) -> None:
        """
        Receives updated weights from the client process and updates the named parameter in the model.

        Args:
            name (`str`):
                Name of the weight tensor being updated.
            torch_dtype (`str`):
                Data type of the weight tensor (e.g., `torch.float32`).
            shape (`Sequence[int]`):
                Shape of the weight tensor.
        """
        if self.pynccl_comm is None:
            raise RuntimeError(
                "Communicator not initialized. Call `init_communicator` first."
            )

        torch_dtype = getattr(torch, dtype.split(".")[-1])
        weight = torch.empty(shape, dtype=torch_dtype, device=self.device)  # type: ignore
        self.pynccl_comm.broadcast(weight, src=self.client_rank)  # type: ignore
        self.pynccl_comm.group.barrier()
        self.model_runner.model.load_weights(weights=[(name, weight)])  # type: ignore

    def close_communicator(self) -> None:
        """
        Closes the communicator when weight synchronization is no longer needed.

        This method deletes the NCCL communicator to release associated resources.
        """

        if self.pynccl_comm is not None:
            del self.pynccl_comm
            self.pynccl_comm = None  # Ensure attribute is reset to None
            self.client_rank = None  # Ensure attribute is reset to None


async def run_server(args: Namespace):
    sock_addr = (args.host or "0.0.0.0", args.port)
    sock = create_server_socket(sock_addr)

    set_ulimit()

    def signal_handler(*_) -> None:
        raise KeyboardInterrupt

    signal.signal(signal.SIGTERM, signal_handler)

    def create_background_task(coro):
        """Create a background task and track it for cleanup"""
        task = asyncio.create_task(coro)
        background_tasks.add(task)
        task.add_done_callback(background_tasks.discard)
        return task

    engine_args = AsyncEngineArgs.from_cli_args(args)
    engine_args.worker_extension_cls = (
        "verifiers.inference.vllm_server.WeightSyncWorkerExtension"
    )
    engine = AsyncLLMEngine.from_engine_args(
        engine_args, usage_context=UsageContext.OPENAI_API_SERVER
    )
    app = build_app(args)

    @app.get("/health")
    async def health():
        """
        Health check endpoint to verify that the server is running.
        """
        return {"status": "ok"}

    @app.get("/get_world_size")
    async def get_world_size():
        """
        Retrieves the world size of the LLM engine, which is `tensor_parallel_size * data_parallel_size`.
        """
        return {"world_size": args.tensor_parallel_size * args.data_parallel_size}

    @app.post("/init_communicator")
    async def init_communicator(request: Request):
        data = await request.json()
        host = data.get("host")
        port = data.get("port")
        world_size = data.get("world_size")
        create_background_task(
            engine.collective_rpc("init_communicator", args=(host, port, world_size))
        )
        return {"status": "ok"}

    @app.post("/update_named_param")
    async def update_named_param(request: Request):
        """
        Updates the model weights with the provided tensor.

        Once this endpoint is called, the client process should broadcast the updated weights to all server workers.

        Args:
            request (`UpdateWeightsRequest`):
                - `name` (`str`): Name of the weight tensor being updated.
                - `dtype` (`str`): Data type of the weight tensor (e.g., `"torch.float32"`).
                - `shape` (list of `int`): Shape of the weight

        """
        data = await request.json()
        name = data.get("name")
        dtype = data.get("dtype")
        shape = data.get("shape")
        shape_tuple = tuple(shape)

        async def throttled_update():
            async with weight_update_semaphore:
                await engine.collective_rpc(
                    "update_named_param", args=(name, dtype, shape_tuple)
                )

        # fire and forget with throttling
        create_background_task(throttled_update())
        return {"status": "ok"}

    @app.post("/reset_prefix_cache")
    async def reset_prefix_cache(request: Request):
        # fire and forget
        create_background_task(engine.reset_prefix_cache())
        return {"status": "ok"}

    @app.post("/get_num_background_tasks")
    async def get_num_background_tasks():
        return {"num_background_tasks": len(background_tasks)}

    @app.post("/close_communicator")
    async def close_communicator(request: Request):
        # fire and forget
        await engine.collective_rpc("close_communicator")
        return {"status": "ok"}

    vllm_config = await engine.get_vllm_config()
    await init_app_state(engine, vllm_config, app.state, args)
    shutdown_task = await serve_http(
        app,
        sock,
        host=args.host,
        port=args.port,
        log_level=args.uvicorn_log_level,
        ssl_keyfile=args.ssl_keyfile,
        ssl_certfile=args.ssl_certfile,
        ssl_ca_certs=args.ssl_ca_certs,
        ssl_cert_reqs=args.ssl_cert_reqs,
    )
    await shutdown_task

    # Cancel and wait for background tasks
    for task in background_tasks:
        task.cancel()
    if background_tasks:
        await asyncio.gather(*background_tasks, return_exceptions=True)

    sock.close()


def main():
    parser = FlexibleArgumentParser(
        description="vLLM OpenAI-compatible server with weight synchronization"
    )
    parser = make_arg_parser(parser)
    args = parser.parse_args() or Namespace()
    validate_parsed_serve_args(args)
    print(args)
    uvloop.run(run_server(args))


if __name__ == "__main__":
    main()



================================================
FILE: verifiers/parsers/__init__.py
================================================
[Empty file]


================================================
FILE: verifiers/parsers/parser.py
================================================
import logging
from typing import Any, Callable

from verifiers.types import ChatMessage, Messages


class Parser:
    """
    Parser class for parsing LLM rollouts.

    Default behavior:
    - `parse` returns text as-is
    - `get_final_answer` returns the last message's content (or text if string)
    """

    def __init__(self, extract_fn: Callable[[str], str] = lambda x: x, **kwargs):
        self.logger = logging.getLogger(f"verifiers.parsers.{self.__class__.__name__}")
        self.extract_fn = extract_fn
        for key, value in kwargs.items():
            setattr(self, key, value)

    def parse(self, text: str) -> Any:
        return self.extract_fn(text)

    def get_assistant_messages(
        self, completion: list[ChatMessage]
    ) -> list[ChatMessage]:
        """Helper function to extract assistant messages from a completion."""
        return [msg for msg in completion if msg["role"] == "assistant"]

    def get_system_messages(self, completion: list[ChatMessage]) -> list[ChatMessage]:
        """Helper function to extract system messages from a completion."""
        return [msg for msg in completion if msg["role"] == "system"]

    def get_user_messages(self, completion: list[ChatMessage]) -> list[ChatMessage]:
        """Helper function to extract user messages from a completion."""
        return [msg for msg in completion if msg["role"] == "user"]

    def get_tool_messages(self, completion: list[ChatMessage]) -> list[ChatMessage]:
        """Helper function to extract tool messages from a completion."""
        return [msg for msg in completion if msg["role"] == "tool"]

    def parse_answer(self, completion: Messages) -> str | None:
        if isinstance(completion, str):
            return self.parse(completion)
        else:
            return self.parse(completion[-1]["content"])  # type: ignore

    def get_format_reward_func(self) -> Callable:
        """
        Reward function that checks if the final answer is formatted correctly.
        """

        def format_reward_func(completion: list[dict[str, str]], **kwargs) -> float:
            return 1.0

        return format_reward_func



================================================
FILE: verifiers/parsers/think_parser.py
================================================
from typing import Callable

from verifiers.parsers.parser import Parser
from verifiers.types import ChatMessage


class ThinkParser(Parser):
    def __init__(self, extract_fn: Callable[[str], str] = lambda x: x, **kwargs):
        super().__init__(**kwargs)
        self.extract_fn = extract_fn

    def parse(self, text: str) -> str:
        if "</think>" in text:
            text = text.split("</think>")[-1].strip()
        else:  # do not allow any further extraction/ parsing if no </think> is found
            text = ""
        return self.extract_fn(text.strip())

    def get_format_reward_func(self) -> Callable:
        """
        Return a reward function that checks if each message follows the format:
        <think>
        ...
        </think>
        ...
        """

        def follows_format(text: str) -> float:
            if (
                text.strip().startswith("<think>")
                and text.count("<think>") == 1
                and text.count("</think>") == 1
                and len(text.split("</think>")[-1]) > 0
            ):
                return 1.0
            return 0.0

        def format_reward_func(completion: list[ChatMessage], **kwargs) -> float:
            messages = self.get_assistant_messages(completion)
            return sum(follows_format(m["content"]) for m in messages) / len(messages)  # type: ignore

        return format_reward_func



================================================
FILE: verifiers/parsers/xml_parser.py
================================================
import re
from types import SimpleNamespace
from typing import Any, Callable

from verifiers.parsers.parser import Parser
from verifiers.types import ChatMessage, Messages


class XMLParser(Parser):
    def __init__(
        self,
        fields: list[str | tuple[str, ...]],
        answer_field: str = "answer",
    ):
        """
        Initialize the parser with field definitions.

        Each field may be:
          - a string (e.g. "reasoning"): the XML tag is fixed.
          - a tuple of alternatives (e.g. ("code", "answer")): the first element is
            the canonical name used for formatting, and all elements are allowed tags
            when parsing.

        The schema is assumed to have no duplicate names.
        """
        # list of (canonical, [alternatives])
        self._fields: list[tuple[str, list[str]]] = []

        self.answer_field = answer_field
        seen = set()
        for field in fields:
            if isinstance(field, str):
                canonical = field
                alternatives = [field]
            elif isinstance(field, tuple):
                if not field:
                    raise ValueError("Field tuple cannot be empty.")
                canonical = field[0]
                if not all(isinstance(alt, str) for alt in field):
                    raise TypeError("All alternatives in a tuple must be strings.")
                alternatives = list(field)
            else:
                raise TypeError("Each field must be a string or a tuple of strings.")
            if canonical in seen:
                raise ValueError(f"Duplicate field name: {canonical}")
            seen.add(canonical)
            self._fields.append((canonical, alternatives))

    def parse(self, text: str, strip: bool = True, last: bool = False) -> Any:
        """
        Parse the given XML string and return an object with attributes corresponding
        to all allowed tags in the schema.

        For each field defined:
          - If it is a simple field (e.g. 'reasoning'), the output object will have
            an attribute 'reasoning' set to the text content (or None if missing).
          - If it is defined with alternatives (e.g. ("code", "answer")), the output
            object will have attributes for *each* allowed tag name. For example,
            if the schema is ['reasoning', ('code', 'answer')], then both
            `result.code` and `result.answer` are always accessible. If a tag is not
            found in the XML, its corresponding attribute is set to None.
        """
        results: dict[str, str | None] = {}
        for canonical, alternatives in self._fields:
            # For each allowed alternative tag, search independently.
            for alt in alternatives:
                # Regex pattern to capture the content between the tags.
                pattern = rf"<{alt}>\s*(.*?)\s*</{alt}>"
                if last:
                    match = None
                    for match in re.finditer(pattern, text, re.DOTALL):
                        pass # iterate over matches to bind last match
                else:
                    match = re.search(pattern, text, re.DOTALL)
                if match:
                    results[alt] = match.group(1).strip() if strip else match.group(1)
                else:
                    results[alt] = None
        return SimpleNamespace(**results)

    def parse_answer(self, completion: Messages) -> str | None:
        """Extract the last answer from a completion."""
        if isinstance(completion, str):
            parsed = self.parse(completion, last=True)
            if (
                parsed
                and hasattr(parsed, self.answer_field)
                and getattr(parsed, self.answer_field) is not None
            ):
                return getattr(parsed, self.answer_field)
        else:
            for msg in reversed(self.get_assistant_messages(completion)):
                parsed = self.parse(msg["content"])
                if (
                    parsed
                    and hasattr(parsed, self.answer_field)
                    and getattr(parsed, self.answer_field) is not None
                ):
                    return getattr(parsed, self.answer_field)
        return None

    def get_format_str(self) -> str:
        """
        Return a string that describes the format of the XML.
        """
        format_str = ""
        for field in self._fields:
            if len(field[1]) > 1:
                options = " | ".join(field[1])
                format_str += f"<[ {options} ]>\n...\n</[ {options} ]>\n"
            else:
                format_str += f"<{field[0]}>\n...\n</{field[0]}>\n"
        return format_str.strip()

    def get_format_reward_func(self) -> Callable:
        """
        Return a reward function that checks if messages follow the expected format.

        The function does not make assumptions about which fields should start/end the message
        or the specific order of fields. It checks that:
        - At least one field from the schema is present in each message
        - Fields have proper content and spacing
        """

        def format_reward_func(completion: list[ChatMessage]):
            """Reward function that checks if each step follows the expected format."""
            model_messages = self.get_assistant_messages(completion)
            if not model_messages:
                return 0.0

            # Calculate format adherence for each message
            format_scores = []
            for msg in model_messages:
                content = msg["content"]
                parsed = self.parse(content)
                parsed_no_strip = self.parse(content, strip=False)

                # Check if the message has at least one valid field
                has_any_field = False
                fields_with_content = 0
                total_fields = 0

                # Keep track of which expected fields are present
                expected_field_count = len(
                    self._fields
                )  # Total number of expected field sets
                present_field_sets = (
                    set()
                )  # Which field sets have at least one alternative present

                # Check proper spacing for fields
                has_correct_spacing = True

                for i, (canonical, alternatives) in enumerate(self._fields):
                    field_set_present = False
                    for alt in alternatives:
                        if hasattr(parsed, alt) and getattr(parsed, alt) is not None:
                            has_any_field = True
                            fields_with_content += 1
                            total_fields += 1
                            field_set_present = True

                            # Check if field exists in non-stripped version too (proper spacing)
                            if not (
                                hasattr(parsed_no_strip, alt)
                                and getattr(parsed_no_strip, alt) is not None
                            ):
                                has_correct_spacing = False
                        elif (
                            content.count(f"<{alt}>") > 0
                            or content.count(f"</{alt}>") > 0
                        ):
                            # Tag exists but content wasn't properly parsed
                            total_fields += 1
                            field_set_present = True

                    # If any alternative from this field set was present, count it
                    if field_set_present:
                        present_field_sets.add(i)

                # Calculate format score components
                format_score = 0.0

                # Check if any field from the first field set starts the message
                starts_with_any_field = False
                first_field_set = self._fields[0][
                    1
                ]  # Get alternatives for first field set
                for alt in first_field_set:
                    if content.strip().startswith(f"<{alt}>"):
                        starts_with_any_field = True
                        break

                # Check if any field from the last field set ends the message
                ends_with_any_field = False
                last_field_set = self._fields[-1][
                    1
                ]  # Get alternatives for last field set
                for alt in last_field_set:
                    if content.strip().endswith(f"</{alt}>"):
                        ends_with_any_field = True
                        break

                # Weight the score based on different criteria
                if has_any_field:
                    # Calculate the proportion of expected field sets that are present
                    field_set_ratio = len(present_field_sets) / expected_field_count
                    format_score += 0.4 * field_set_ratio

                if has_correct_spacing:
                    format_score += 0.2

                if starts_with_any_field:
                    format_score += 0.2

                if ends_with_any_field:
                    format_score += 0.2

                format_scores.append(format_score)

            # Return average format adherence
            if not format_scores:
                return 0.0
            return sum(format_scores) / len(format_scores)

        return format_reward_func

    def get_fields(self) -> list[str]:
        """Return a list of the canonical field names (in order)."""
        return [canonical for canonical, _ in self._fields]

    def format(self, **kwargs) -> str:
        """
        Format the provided keyword arguments into an XML string.

        For fields with alternatives (tuple), the canonical name (the first element)
        is used as the XML tag. The method looks for a provided value using any of the
        allowed names (preferring the canonical if present).

        Example usage:
            parser = XMLParser(['reasoning', ('code', 'answer')])
            formatted_str = parser.format(reasoning="...", code="...")
        """
        parts = []
        for canonical, alternatives in self._fields:
            value = None
            # Look for a provided value using any of the acceptable keys,
            # preferring the canonical name if it exists.
            if canonical in kwargs:
                value = kwargs[canonical]
            else:
                for alt in alternatives:
                    if alt in kwargs:
                        value = kwargs[alt]
                        break
            if value is None:
                raise ValueError(
                    f"Missing value for field '{canonical}' (allowed: {alternatives})"
                )
            # Use the canonical name as the tag for formatting.
            parts.append(f"<{canonical}>\n{value}\n</{canonical}>")
        return "\n".join(parts)



================================================
FILE: verifiers/rubrics/__init__.py
================================================
[Empty file]


================================================
FILE: verifiers/rubrics/judge_rubric.py
================================================
from typing import Any

from openai import AsyncOpenAI, OpenAI

from verifiers.parsers.parser import Parser
from verifiers.rubrics.rubric import Rubric
from verifiers.types import Messages, State
from verifiers.utils.async_utils import maybe_await

DEFAULT_JUDGE_PROMPT = """Given a ground truth answer \
and a response, determine if the response is correct.

Question:
```
{question}
```

Ground truth answer:
```
{answer}
```

Response:
```
{response}
```

Respond either "yes" or "no" only."""


class JudgeRubric(Rubric):
    def __init__(
        self,
        parser: Parser | None = None,
        parallelize_scoring: bool = False,
        judge_client: OpenAI | AsyncOpenAI | None = None,
        judge_model: str = "gpt-4.1-nano",
        judge_sampling_args: dict[str, Any] | None = None,
        judge_prompt: str = DEFAULT_JUDGE_PROMPT,
        **kwargs,
    ):
        super().__init__(
            parser=parser, parallelize_scoring=parallelize_scoring, **kwargs
        )
        self.judge_client = judge_client if judge_client is not None else AsyncOpenAI()
        self.judge_model = judge_model
        self.judge_prompt = judge_prompt
        self.judge_sampling_args = judge_sampling_args or {}
        self.class_objects = {
            "parser": self.parser,
            "judge": self.judge,
            "judge_client": self.judge_client,
            "judge_model": self.judge_model,
            "judge_prompt": self.judge_prompt,
            "judge_sampling_args": self.judge_sampling_args,
        }

    async def judge(
        self,
        prompt: Messages,
        completion: Messages,
        answer: str,
        state: State,
        **kwargs,
    ) -> str:
        if isinstance(prompt, list):
            last_msg = prompt[-1]
            if isinstance(last_msg, dict) and "content" in last_msg:
                question = str(last_msg["content"])
            else:
                question = ""
        else:
            question = str(prompt)
        response = self.parser.parse_answer(completion)
        judge_prompt = self.judge_prompt.format(
            question=question, answer=answer, response=response
        )
        cached = state.get("judge_response")
        if isinstance(cached, dict) and judge_prompt in cached:
            return cached[judge_prompt]
        # Normalize judge sampling args for chat API
        judge_args = dict(self.judge_sampling_args or {})
        if "max_tokens" in judge_args:
            if judge_args["max_tokens"] is None:
                judge_args.pop("max_tokens")
            else:
                judge_args["max_completion_tokens"] = judge_args.pop("max_tokens")
        if (
            "max_completion_tokens" in judge_args
            and judge_args["max_completion_tokens"] is None
        ):
            judge_args.pop("max_completion_tokens")
        judge_args = {k: v for k, v in judge_args.items() if v is not None}
        judge_response = await maybe_await(
            self.judge_client.chat.completions.create,
            model=self.judge_model,
            messages=[{"role": "user", "content": judge_prompt}],
            **judge_args,
        )
        judge_response = str(judge_response.choices[0].message.content)
        if not isinstance(cached, dict):
            cached = {}
        cached[judge_prompt] = judge_response
        state["judge_response"] = cached
        return judge_response



================================================
FILE: verifiers/rubrics/math_rubric.py
================================================
from math_verify import parse, verify

from verifiers.parsers.parser import Parser
from verifiers.parsers.think_parser import ThinkParser
from verifiers.rubrics.rubric import Rubric
from verifiers.types import Messages, RewardFunc
from verifiers.utils.data_utils import extract_boxed_answer


class MathRubric(Rubric):
    def __init__(
        self,
        funcs: list[RewardFunc] | None = None,
        weights: list[float] | None = None,
        parser: Parser | None = None,
    ):
        parser = parser or ThinkParser(extract_fn=extract_boxed_answer)
        super().__init__(funcs=funcs, weights=weights, parser=parser)
        self.add_reward_func(self.correct_answer_reward_func)

    def correct_answer_reward_func(
        self, parser: Parser, completion: Messages, answer: str, **kwargs
    ) -> float:
        """Reward function that checks if the final answer matches the expected answer."""
        try:
            response = parser.parse_answer(completion) or ""
            if response == "":
                return 0.0
            if verify(
                parse(f"\\boxed{{{answer}}}", parsing_timeout=5),
                parse(f"\\boxed{{{response}}}", parsing_timeout=5),
                timeout_seconds=5,
            ):
                return 1.0
            else:
                return 0.0
        except BaseException:
            return 0.0



================================================
FILE: verifiers/rubrics/rubric.py
================================================
import asyncio
import inspect
import logging

from verifiers.parsers.parser import Parser
from verifiers.types import (
    Info,
    Messages,
    RewardFunc,
    RolloutScore,
    RolloutScores,
    State,
)
from verifiers.utils.async_utils import maybe_await


class Rubric:
    """
    Rubric class for reward functions.

    Each reward function takes:
    - prompt: list[dict[str, str]] | str
    - completion: list[dict[str, str]] | str
    - answer: Any (metadata for scoring)
    - task (optional): str (type of task)
    - **kwargs: additional kwargs

    Returns:
    - float | list[float] | dict[str, float]
    """

    def __init__(
        self,
        funcs: list[RewardFunc] | None = None,
        weights: list[float] | None = None,
        parser: Parser | None = None,
        parallelize_scoring: bool = True,
        **kwargs,
    ):
        self.logger = logging.getLogger(f"verifiers.rubrics.{self.__class__.__name__}")

        self.reward_funcs = funcs or []
        self.reward_weights = weights or []
        self.parser = parser or Parser()

        for key, value in kwargs.items():
            setattr(self, key, value)
        if not self.reward_weights:
            self.reward_weights = [1.0] * len(self.reward_funcs)
        self.parallelize_scoring = parallelize_scoring
        # class objects for reward functions
        self.class_objects = {}
        if self.parser:
            self.class_objects["parser"] = self.parser

    def get_reward_func_names(self) -> list[str]:
        return [func.__name__ for func in self.reward_funcs]

    def get_reward_funcs(self) -> list[RewardFunc]:
        return self.reward_funcs  # type: ignore

    def get_reward_weights(self) -> list[float]:
        return self.reward_weights  # type: ignore

    def add_reward_func(self, func: RewardFunc, weight: float = 1.0):
        self.reward_funcs.append(func)
        self.reward_weights.append(weight)

    async def call_reward_func(
        self,
        func: RewardFunc,
        prompt: Messages,
        completion: Messages,
        answer: str,
        state: State,
        task: str = "default",
        info: Info | None = None,
        **kwargs,
    ) -> float:
        """
        Invoke `func` with only the required arguments.

        Example:
        ```
        def func(completion, answer, **kwargs):
            ...
        ``
        """
        info = info or {}
        sig = inspect.signature(func)

        common = dict(
            prompt=prompt,
            completion=completion,
            answer=answer,
            state=state,
            task=task,
            info=info,
        )
        common.update(self.class_objects)
        merged = {**common, **kwargs}
        if any(p.kind == p.VAR_KEYWORD for p in sig.parameters.values()):
            try:
                ans = float(await maybe_await(func, **merged))
            except Exception as e:
                self.logger.error(f"Error calling reward function {func.__name__}: {e}")
                ans = 0.0
        else:
            allowed = {k: v for k, v in merged.items() if k in sig.parameters}
            try:
                ans = float(await maybe_await(func, **allowed))
            except Exception as e:
                self.logger.error(f"Error calling reward function {func.__name__}: {e}")
                ans = 0.0
        return ans

    async def score_rollout(
        self,
        prompt: Messages,
        completion: Messages,
        answer: str,
        state: State,
        task: str = "default",
        info: Info | None = None,
        **kwargs,
    ) -> RolloutScore:
        """
        Evaluate all reward functions asynchronously for a single rollout.
        """
        if self.parallelize_scoring:
            score_tasks = [
                self.call_reward_func(
                    func=func,
                    # **self.class_objects,
                    prompt=prompt,
                    completion=completion,
                    answer=answer,
                    state=state,
                    task=task,
                    info=info,
                    **kwargs,
                )
                for func in self.get_reward_funcs()
            ]
            reward_scores = await asyncio.gather(*score_tasks)
        else:
            reward_scores = []
            for func in self.get_reward_funcs():
                score = await self.call_reward_func(
                    func=func,
                    # **self.class_objects,
                    prompt=prompt,
                    completion=completion,
                    answer=answer,
                    state=state,
                    task=task,
                    info=info,
                    **kwargs,
                )
                reward_scores.append(score)
        rewards = RolloutScore(
            metrics={
                func.__name__: reward
                for func, reward in zip(self.get_reward_funcs(), reward_scores)
            },
            reward=sum(
                [
                    reward * weight
                    for reward, weight in zip(reward_scores, self.get_reward_weights())
                ]
            ),
        )
        return rewards

    async def score_rollouts(
        self,
        prompts: list[Messages],
        completions: list[Messages],
        answers: list[str],
        states: list[State],
        tasks: list[str],
        infos: list[Info],
        **kwargs,
    ) -> RolloutScores:
        """
        Compute reward scores for a group of rollouts.

        Default behavior:
        - evaluate each rollout asynchronously
        - return list of dictionaries of reward function names and their scores

        Potential overrides:
        - inter-group comparisons (voting, ranking, Elo, etc.)
        - scores computed using global state stored in Rubric class
        """
        from tqdm.asyncio import tqdm_asyncio

        rollout_tasks = [
            self.score_rollout(*pcasti, **kwargs)
            for pcasti in zip(prompts, completions, answers, states, tasks, infos)
        ]
        rewards = await tqdm_asyncio.gather(
            *rollout_tasks,
            total=len(prompts),
            desc=f"Evaluating {len(prompts)} rollouts",
        )

        if not rewards:
            reward_func_names = self.get_reward_func_names()
            return RolloutScores(
                reward=[],
                metrics={name: [] for name in reward_func_names},
            )

        return RolloutScores(
            reward=[reward.reward for reward in rewards],
            metrics={
                k: [item.metrics[k] for item in rewards] for k in rewards[0].metrics
            },
        )



================================================
FILE: verifiers/rubrics/rubric_group.py
================================================
from verifiers.rubrics.rubric import Rubric
from verifiers.types import Info, Messages, RewardFunc, RolloutScores, State


class RubricGroup(Rubric):
    """
    Class for aggregating multiple rubrics.
    """

    def __init__(self, rubrics: list[Rubric], **kwargs):
        if not rubrics:
            raise ValueError("RubricGroup must have at least one rubric")

        super().__init__(**kwargs)
        self.rubrics = rubrics
        self.logger.info(f"Initialized RubricGroup with {len(rubrics)} rubrics")

    def get_reward_func_names(self) -> list[str]:
        names = []
        for rubric in self.rubrics:
            names.extend(rubric.get_reward_func_names())
        return names

    def get_reward_funcs(self) -> list[RewardFunc]:
        funcs = []
        for rubric in self.rubrics:
            funcs.extend(rubric.get_reward_funcs())
        return funcs

    def get_reward_weights(self) -> list[float]:
        weights = []
        for rubric in self.rubrics:
            weights.extend(rubric.get_reward_weights())
        return weights

    def add_reward_func(self, func: RewardFunc, weight: float = 1.0):
        assert len(self.rubrics) > 0, "RubricGroup must have at least one rubric"
        self.logger.warning("Adding reward function to the first rubric in the group.")
        self.rubrics[0].add_reward_func(func, weight)

    async def score_rollouts(
        self,
        prompts: list[Messages],
        completions: list[Messages],
        answers: list[str],
        states: list[State],
        tasks: list[str],
        infos: list[Info],
        **kwargs,
    ) -> RolloutScores:
        """
        Run all rubrics sequentially and return the aggregated scores.

        Reward functions with the same name are summed up.
        """
        all_scores = RolloutScores(
            reward=[],
            metrics={},
        )
        for rubric in self.rubrics:
            rubric_scores = await rubric.score_rollouts(
                prompts, completions, answers, states, tasks, infos, **kwargs
            )
            # aggregate reward (element-wise sum across rubrics)
            if not all_scores.reward:
                all_scores.reward = rubric_scores.reward
            else:
                all_scores.reward = [
                    a + b for a, b in zip(all_scores.reward, rubric_scores.reward)
                ]
            for key, value in rubric_scores.metrics.items():
                if key in all_scores.metrics:
                    # element-wise sum
                    all_scores.metrics[key] = [
                        a + b for a, b in zip(all_scores.metrics[key], value)
                    ]
                else:
                    all_scores.metrics[key] = value
        return all_scores



================================================
FILE: verifiers/rubrics/tool_rubric.py
================================================
from typing import Callable

from verifiers.rubrics.rubric import Rubric
from verifiers.types import Messages
from verifiers.utils.tool_utils import convert_func_to_oai_tool


class ToolRubric(Rubric):
    """Simple rubric that counts tool calls in completion messages."""

    def __init__(self, tools: list[Callable] | None = None):
        self.tools = tools or []
        self.oai_tools = [convert_func_to_oai_tool(tool) for tool in self.tools]
        self.tool_names = [tool.__name__ for tool in self.tools]

        # Build initial reward functions and weights
        reward_funcs = []
        reward_funcs.append(self.total_tool_calls)
        reward_weights = [0.0]

        for tool_name in self.tool_names:
            reward_funcs.append(self.get_tool_call_count_func(tool_name))
            reward_weights.append(0.0)

        # Pass them to parent class
        super().__init__(funcs=reward_funcs, weights=reward_weights)

    async def total_tool_calls(self, completion: Messages) -> float:
        """Count the total number of tool calls across all assistant messages."""
        total = 0
        assert isinstance(completion, list)
        for msg in completion:
            if msg.get("role") == "assistant" and "tool_calls" in msg:
                tool_calls = msg.get("tool_calls", [])
                if isinstance(tool_calls, list):
                    total += len(tool_calls)
        return float(total)

    def get_tool_call_count_func(self, tool_name: str) -> Callable:
        """Create a reward function that counts calls to a specific tool."""

        async def tool_call_count_func(completion: Messages) -> float:
            """Count calls to {tool_name} tool."""
            count = 0

            # Find tool calls in assistant messages
            assert isinstance(completion, list)
            for msg in completion:
                if msg.get("role") == "assistant" and "tool_calls" in msg:
                    tool_calls = msg.get("tool_calls", [])
                    if not isinstance(tool_calls, list):
                        continue

                    for tool_call in tool_calls:
                        if hasattr(tool_call, "function"):
                            assert hasattr(getattr(tool_call, "function"), "name")
                            if getattr(tool_call, "function").name == tool_name:
                                count += 1

            return float(count)

        tool_call_count_func.__name__ = f"{tool_name}_calls"
        return tool_call_count_func



================================================
FILE: verifiers/rubrics/utils/math_utils.py
================================================
"""
Answer checker API that uses sympy to simplify expressions and check for equality.

Call grade_answer(given_answer: str, ground_truth: str).
"""

import re

import sympy
from pylatexenc import latex2text
from sympy.parsing import sympy_parser


# Dan Hendrycks' code
def mathd_normalize_answer(answer: str | None) -> str | None:
    if answer is None:
        return None
    answer = answer.strip()
    try:
        # Remove enclosing `\text{}`.
        m = re.search("^\\\\text\{(?P<text>.+?)\}$", answer)
        if m is not None:
            answer = m.group("text").strip()
        return _strip_string(answer)
    except Exception:
        return answer


def _strip_string(string):
    def _fix_fracs(string):
        substrs = string.split("\\frac")
        new_str = substrs[0]
        if len(substrs) > 1:
            substrs = substrs[1:]
            for substr in substrs:
                new_str += "\\frac"
                if substr[0] == "{":
                    new_str += substr
                else:
                    try:
                        assert len(substr) >= 2
                    except Exception:
                        return string
                    a = substr[0]
                    b = substr[1]
                    if b != "{":
                        if len(substr) > 2:
                            post_substr = substr[2:]
                            new_str += "{" + a + "}{" + b + "}" + post_substr
                        else:
                            new_str += "{" + a + "}{" + b + "}"
                    else:
                        if len(substr) > 2:
                            post_substr = substr[2:]
                            new_str += "{" + a + "}" + b + post_substr
                        else:
                            new_str += "{" + a + "}" + b
        string = new_str
        return string

    def _fix_a_slash_b(string):
        if len(string.split("/")) != 2:
            return string
        a = string.split("/")[0]
        b = string.split("/")[1]
        try:
            a = int(a)
            b = int(b)
            assert string == "{}/{}".format(a, b)
            new_string = "\\frac{" + str(a) + "}{" + str(b) + "}"
            return new_string
        except Exception:
            return string

    def _remove_right_units(string):
        # "\\text{ " only ever occurs (at least in the val set) when describing units
        if "\\text{ " in string:
            splits = string.split("\\text{ ")
            assert len(splits) == 2
            return splits[0]
        else:
            return string

    def _fix_sqrt(string):
        if "\\sqrt" not in string:
            return string
        splits = string.split("\\sqrt")
        new_string = splits[0]
        for split in splits[1:]:
            if split[0] != "{":
                a = split[0]
                new_substr = "\\sqrt{" + a + "}" + split[1:]
            else:
                new_substr = "\\sqrt" + split
            new_string += new_substr
        return new_string

    # linebreaks
    string = string.replace("\n", "")
    # print(string)

    # remove inverse spaces
    string = string.replace("\\!", "")
    # print(string)

    # replace \\ with \
    string = string.replace("\\\\", "\\")
    # print(string)

    # replace tfrac and dfrac with frac
    string = string.replace("tfrac", "frac")
    string = string.replace("dfrac", "frac")
    # print(string)

    # remove \left and \right
    string = string.replace("\\left", "")
    string = string.replace("\\right", "")
    # print(string)

    # Remove circ (degrees)
    string = string.replace("^{\\circ}", "")
    string = string.replace("^\\circ", "")

    # remove dollar signs
    string = string.replace("\\$", "")

    # remove units (on the right)
    string = _remove_right_units(string)

    # remove percentage
    string = string.replace("\\%", "")
    string = string.replace("\%", "")

    # " 0." equivalent to " ." and "{0." equivalent to "{." Alternatively, add "0" if "." is the start of the string
    string = string.replace(" .", " 0.")
    string = string.replace("{.", "{0.")
    # if empty, return empty string
    if len(string) == 0:
        return string
    if string[0] == ".":
        string = "0" + string

    # to consider: get rid of e.g. "k = " or "q = " at beginning
    if len(string.split("=")) == 2:
        if len(string.split("=")[0]) <= 2:
            string = string.split("=")[1]

    # fix sqrt3 --> sqrt{3}
    string = _fix_sqrt(string)

    # remove spaces
    string = string.replace(" ", "")

    # \frac1b or \frac12 --> \frac{1}{b} and \frac{1}{2}, etc. Even works with \frac1{72} (but not \frac{72}1). Also does a/b --> \\frac{a}{b}
    string = _fix_fracs(string)

    # manually change 0.5 --> \frac{1}{2}
    if string == "0.5":
        string = "\\frac{1}{2}"

    # NOTE: X/Y changed to \frac{X}{Y} in dataset, but in simple cases fix in case the model output is X/Y
    string = _fix_a_slash_b(string)

    return string


# sympy might hang -- we don't care about trying to be lenient in these cases
BAD_SUBSTRINGS = ["^{", "^("]
BAD_REGEXES = ["\^[0-9]+\^", "\^[0-9][0-9]+"]
TUPLE_CHARS = "()[]"


def _sympy_parse(expr: str):
    """Parses an expression with sympy."""
    py_expr = expr.replace("^", "**")
    return sympy_parser.parse_expr(
        py_expr,
        transformations=(
            sympy_parser.standard_transformations
            + (sympy_parser.implicit_multiplication_application,)
        ),
    )


def _parse_latex(expr: str) -> str:
    """Attempts to parse latex to an expression sympy can read."""
    expr = expr.replace("\\tfrac", "\\frac")
    expr = expr.replace("\\dfrac", "\\frac")
    expr = expr.replace("\\frac", " \\frac")  # Play nice with mixed numbers.
    expr = latex2text.LatexNodes2Text().latex_to_text(expr)

    # Replace the specific characters that this parser uses.
    expr = expr.replace("√", "sqrt")
    expr = expr.replace("π", "pi")
    expr = expr.replace("∞", "inf")
    expr = expr.replace("∪", "U")
    expr = expr.replace("·", "*")
    expr = expr.replace("×", "*")

    return expr.strip()


def _is_float(num: str) -> bool:
    try:
        float(num)
        return True
    except ValueError:
        return False


def _is_int(x: float) -> bool:
    try:
        return abs(x - int(round(x))) <= 1e-7
    except Exception:
        return False


def _is_frac(expr: str) -> bool:
    return bool(re.search(r"^-?[0-9]+.?/0*[1-9][0-9]*.?$", expr))


def _str_is_int(x: str) -> bool:
    try:
        x = _strip_properly_formatted_commas(x)
        x = float(x)
        return abs(x - int(round(x))) <= 1e-7
    except Exception:
        return False


def _str_to_int(x: str) -> bool:
    x = x.replace(",", "")
    x = float(x)
    return int(x)


def _inject_implicit_mixed_number(step: str):
    """
    Automatically make a mixed number evalable
    e.g. 7 3/4 => 7+3/4
    """
    p1 = re.compile("([0-9]) +([0-9])")
    step = p1.sub("\\1+\\2", step)  ## implicit mults
    return step


def _strip_properly_formatted_commas(expr: str):
    # We want to be careful because we don't want to strip tuple commas
    p1 = re.compile("(\d)(,)(\d\d\d)($|\D)")
    while True:
        next_expr = p1.sub("\\1\\3\\4", expr)
        if next_expr == expr:
            break
        expr = next_expr
    return next_expr


def _normalize(expr: str) -> str:
    """Normalize answer expressions."""
    if expr is None:
        return None

    # Remove enclosing `\text{}`.
    m = re.search("^\\\\text\{(?P<text>.+?)\}$", expr)
    if m is not None:
        expr = m.group("text")

    expr = expr.replace("\\%", "%")
    expr = expr.replace("\\$", "$")
    expr = expr.replace("$", "")
    expr = expr.replace("%", "")
    expr = expr.replace(" or ", " , ")
    expr = expr.replace(" and ", " , ")

    expr = expr.replace("million", "*10^6")
    expr = expr.replace("billion", "*10^9")
    expr = expr.replace("trillion", "*10^12")

    for unit in [
        "degree",
        "cm",
        "centimeter",
        "meter",
        "mile",
        "second",
        "minute",
        "hour",
        "day",
        "week",
        "month",
        "year",
        "foot",
        "feet",
        "inch",
        "yard",
    ]:
        expr = re.sub(f"{unit}(es)?(s)? *(\^[0-9]+)?", "", expr)
    expr = re.sub("\^ *\\\\circ", "", expr)

    if len(expr) > 0 and expr[0] == "{" and expr[-1] == "}":
        expr = expr[1:-1]

    expr = re.sub(",\\\\! *", "", expr)
    if _is_float(expr) and _is_int(float(expr)):
        expr = str(int(round(float(expr))))
    if "\\" in expr:
        try:
            expr = _parse_latex(expr)
        except Exception:
            pass

    # edge case with mixed numbers and negative signs
    expr = re.sub("- *", "-", expr)

    expr = _inject_implicit_mixed_number(expr)
    expr = expr.replace(" ", "")

    # if we somehow still have latex braces here, just drop them
    expr = expr.replace("{", "")
    expr = expr.replace("}", "")

    # don't be case sensitive for text answers
    expr = expr.lower()

    if _str_is_int(expr):
        expr = str(_str_to_int(expr))

    return expr


def count_unknown_letters_in_expr(expr: str):
    expr = expr.replace("sqrt", "")
    expr = expr.replace("frac", "")
    letters_in_expr = set([x for x in expr if x.isalpha()])
    return len(letters_in_expr)


def should_allow_eval(expr: str):
    # we don't want to try parsing unknown text or functions of more than two variables
    if count_unknown_letters_in_expr(expr) > 2:
        return False

    for bad_string in BAD_SUBSTRINGS:
        if bad_string in expr:
            return False

    for bad_regex in BAD_REGEXES:
        if re.search(bad_regex, expr) is not None:
            return False

    return True


def are_equal_under_sympy(ground_truth_normalized: str, given_normalized: str):
    are_equal = False
    try:
        expr = f"({ground_truth_normalized})-({given_normalized})"
        if should_allow_eval(expr):
            sympy_diff = _sympy_parse(expr)
            simplified = sympy.simplify(sympy_diff)
            if simplified == 0:
                are_equal = True
    except Exception:
        pass
    return are_equal


def split_tuple(expr: str):
    """
    Split the elements in a tuple/interval, while handling well-formatted commas in large numbers
    """
    expr = _strip_properly_formatted_commas(expr)
    if len(expr) == 0:
        return []
    if (
        len(expr) > 2
        and expr[0] in TUPLE_CHARS
        and expr[-1] in TUPLE_CHARS
        and all([ch not in expr[1:-1] for ch in TUPLE_CHARS])
    ):
        elems = [elem.strip() for elem in expr[1:-1].split(",")]
    else:
        elems = [expr]
    return elems


def last_boxed_only_string(string):
    idx = string.rfind("\\boxed")
    if idx < 0:
        idx = string.rfind("\\fbox")
        if idx < 0:
            return None

    i = idx
    right_brace_idx = None
    num_left_braces_open = 0
    while i < len(string):
        if string[i] == "{":
            num_left_braces_open += 1
        if string[i] == "}":
            num_left_braces_open -= 1
            if num_left_braces_open == 0:
                right_brace_idx = i
                break
        i += 1

    if right_brace_idx is None:
        retval = None
    else:
        retval = string[idx : right_brace_idx + 1]

    return retval


def remove_boxed(s):
    left = "\\boxed{"
    try:
        assert s[: len(left)] == left
        assert s[-1] == "}"
        return s[len(left) : -1]
    except Exception:
        return None


def extract_boxed_answer(solution: str) -> str:
    """Extract the answer from inside a LaTeX \\boxed{} command"""
    solution = last_boxed_only_string(solution)
    solution = remove_boxed(solution)
    return solution


def grade_answer_sympy(given_answer: str, ground_truth: str) -> bool:
    ground_truth_normalized = _normalize(ground_truth)
    given_normalized = _normalize(given_answer)

    if ground_truth_normalized is None:
        return False

    if ground_truth_normalized == given_normalized:
        return True

    if len(given_normalized) == 0:
        return False

    ground_truth_elems = split_tuple(ground_truth_normalized)
    given_elems = split_tuple(given_normalized)

    if len(ground_truth_elems) > 1 and (
        ground_truth_normalized[0] != given_normalized[0]
        or ground_truth_normalized[-1] != given_normalized[-1]
    ):
        is_correct = False
    elif len(ground_truth_elems) != len(given_elems):
        is_correct = False
    else:
        for ground_truth_elem, given_elem in zip(ground_truth_elems, given_elems):
            if _is_frac(ground_truth_elem) and _is_frac(given_elem):
                # if fractions aren't reduced, then shouldn't be marked as correct
                # so, we don't want to allow sympy.simplify in this case
                is_correct = ground_truth_elem == given_elem
            elif _str_is_int(ground_truth_elem) != _str_is_int(given_elem):
                # if the ground truth answer is an integer, we require the given answer to be a strict match (no sympy.simplify)
                is_correct = False
            else:
                is_correct = are_equal_under_sympy(ground_truth_elem, given_elem)
            if not is_correct:
                break

    return is_correct


def grade_answer_mathd(given_answer: str, ground_truth: str) -> bool:
    ground_truth_normalized_mathd = mathd_normalize_answer(ground_truth)
    given_answer_normalized_mathd = mathd_normalize_answer(given_answer)

    # be at least as lenient as mathd
    if ground_truth_normalized_mathd == given_answer_normalized_mathd:
        return True
    return False


def extract_answer(passage: str) -> str:
    if "\\boxed" in passage:
        return extract_boxed_answer(passage)
    return None


def grade_answer(solution_str: str, ground_truth: str) -> bool:
    if not ground_truth:
        return False
    if "\\boxed" in ground_truth:
        ground_truth = extract_answer(ground_truth)
    given_answer = extract_answer(solution_str)
    if given_answer is None:
        return False
    return grade_answer_mathd(given_answer, ground_truth) or grade_answer_sympy(
        given_answer, ground_truth
    )



================================================
FILE: verifiers/scripts/__init__.py
================================================
[Empty file]


================================================
FILE: verifiers/scripts/eval.py
================================================
import argparse
import importlib
import importlib.util
import json
import os
import uuid
from datetime import datetime
from pathlib import Path

import numpy as np
from datasets import Dataset
from openai import OpenAI

import verifiers as vf
from verifiers.utils.message_utils import messages_to_printable, sanitize_tool_calls


def eval_environment(
    env: str,
    env_args: dict,
    env_dir_path: str,
    endpoints_path: str,
    model: str,
    api_key_var: str,
    api_base_url: str,
    num_examples: int,
    rollouts_per_example: int,
    max_concurrent_requests: int,
    max_tokens: int | None,
    temperature: float | None,
    sampling_args: dict | None,
    verbose: bool,
    save_dataset: bool,
    save_to_hf_hub: bool,
    hf_hub_dataset_name: str,
):
    try:
        endpoints_path_obj = Path(endpoints_path)
        if endpoints_path_obj.is_dir():
            endpoints_file = endpoints_path_obj / "endpoints.py"
        else:
            endpoints_file = endpoints_path_obj

        if endpoints_file.exists():
            spec = importlib.util.spec_from_file_location("endpoints", endpoints_file)
            assert spec and spec.loader
            endpoints_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(endpoints_module)
            ENDPOINTS = endpoints_module.ENDPOINTS
        else:
            raise ImportError(f"endpoints.py not found at {endpoints_file}")
    except (ImportError, AttributeError):
        print(
            f"No local endpoint registry found at {endpoints_path}. \
Please specify the model name (-m), API host base URL (-b), and API key variable name (-k)."
        )
        ENDPOINTS = {}

    if model in ENDPOINTS:
        api_key_var = ENDPOINTS[model]["key"]
        api_base_url = ENDPOINTS[model]["url"]
        model = ENDPOINTS[model]["model"]

    client = OpenAI(api_key=os.getenv(api_key_var, "EMPTY"), base_url=api_base_url)
    vf_env = vf.load_environment(env_id=env, **env_args)
    # Merge sampling args with precedence to JSON payload over explicit flags
    merged_sampling_args: dict = {}
    if sampling_args is not None:
        merged_sampling_args.update(sampling_args)
    if "max_tokens" not in merged_sampling_args:
        merged_sampling_args["max_tokens"] = max_tokens
    if temperature is not None and "temperature" not in merged_sampling_args:
        merged_sampling_args["temperature"] = temperature
    results = vf_env.evaluate(
        client=client,
        model=model,
        sampling_args=merged_sampling_args,
        num_examples=num_examples,
        rollouts_per_example=rollouts_per_example,
        max_concurrent_requests=max_concurrent_requests,
    )
    print("--- Evaluation ---")
    print(f"Environment: {env}")
    print(f"Model: {model}")
    print(f"Provider: {api_base_url}")
    print(f"Examples: {num_examples}")
    print(f"Rollouts per example: {rollouts_per_example}")
    print("--- Example ---")
    printable_prompts = [messages_to_printable(p) for p in results.prompt]
    printable_completions = [messages_to_printable(c) for c in results.completion]
    vf.print_prompt_completions_sample(
        printable_prompts, printable_completions, results.reward, step=0
    )
    print("--- All ---")
    print("Rewards:")
    print(
        f"reward: avg - {sum(results.reward) / len(results.reward):.3f}, std - {np.std(results.reward):.3f}"
    )
    n = num_examples
    r = rollouts_per_example

    if n < 0:
        n = len(results.reward) // r
    for i in range(r):
        # rounded to 3 decimal places
        trials = [round(results.reward[(i * n) + j], 3) for j in range(n)]
        out = f"r{i + 1}: {trials}"
        print(out)
    for k in results.metrics:
        v = results.metrics[k]
        print(f"{k}: avg - {sum(v) / len(v):.3f}, std - {np.std(v):.3f}")
        for i in range(r):
            # rounded to 3 decimal places
            trials = [round(v[(i * n) + j], 3) for j in range(n)]
            out = f"r{i + 1}: {trials}"
            print(out)

    if save_dataset or save_to_hf_hub:
        ids = [i // rollouts_per_example for i in range(n * rollouts_per_example)]
        rewards = results.reward
        tasks = results.task
        data_dict = {
            "id": ids,
            "prompt": [sanitize_tool_calls(p) for p in printable_prompts],
            "completion": [sanitize_tool_calls(c) for c in printable_completions],
            "task": tasks,
        }
        if results.info[0] != {}:
            data_dict["info"] = results.info
        if results.answer[0] != "":
            data_dict["answer"] = results.answer
        data_dict["reward"] = rewards
        for k in results.metrics:
            v = results.metrics[k]
            data_dict[k] = v

        dataset = Dataset.from_dict(data_dict)
        metadata = {
            "env": env,
            "model": model,
            "num_examples": n,
            "rollouts_per_example": rollouts_per_example,
            "sampling_args": merged_sampling_args,
            "date": datetime.now().strftime("%Y-%m-%d"),
            "time": datetime.now().strftime("%H:%M:%S"),
            "avg_reward": sum(results.reward) / len(results.reward),
        }
        for k in results.metrics:
            metadata[f"avg_{k}"] = sum(results.metrics[k]) / len(results.metrics[k])

        uuid_str = str(uuid.uuid4())[:8]
        env_model_str = f"{env}--{model.replace('/', '--')}"
        if save_dataset:
            module_name = env.replace("-", "_")
            local_env_dir = Path(env_dir_path) / module_name
            if local_env_dir.exists():
                results_path = (
                    local_env_dir / "outputs" / "evals" / env_model_str / uuid_str
                )
            else:
                results_path = Path("./outputs") / "evals" / env_model_str / uuid_str
            results_path.parent.mkdir(parents=True, exist_ok=True)
            dataset.to_json(results_path / "results.jsonl")
            with open(results_path / "metadata.json", "w") as f:
                json.dump(metadata, f)

            print(f"Saved dataset to {results_path}")
        if save_to_hf_hub:
            if hf_hub_dataset_name == "":
                dataset_name = (
                    f"{env}_{model.replace('/', '-')}_n{n}_r{rollouts_per_example}"
                )
            else:
                dataset_name = hf_hub_dataset_name
            dataset.push_to_hub(dataset_name)
            print(f"Saved dataset to Hugging Face Hub: {dataset_name}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "env", type=str, default="gsm8k", help="Environment module name"
    )
    parser.add_argument(
        "--env-args",
        "-a",
        type=json.loads,
        default={},
        help='Environment module arguments as JSON object (e.g., \'{"key": "value", "num": 42}\')',
    )
    parser.add_argument(
        "--env-dir-path",
        "-p",
        type=str,
        default="./environments",
        help="Path to environments directory",
    )
    parser.add_argument(
        "--endpoints-path",
        "-e",
        type=str,
        default="./configs/endpoints.py",
        help="Path to API endpoints registry",
    )
    parser.add_argument(
        "--model",
        "-m",
        type=str,
        default="gpt-4.1-mini",
        help="Name of model to evaluate",
    )
    parser.add_argument(
        "--api-key-var",
        "-k",
        type=str,
        default="OPENAI_API_KEY",
        help="Environment variable name for API key",
    )
    parser.add_argument(
        "--api-base-url",
        "-b",
        type=str,
        default="https://api.openai.com/v1",
        help="Base URL for API",
    )
    parser.add_argument(
        "--num-examples",
        "-n",
        type=int,
        default=5,
        help="Number of examples to evaluate",
    )
    parser.add_argument(
        "--rollouts-per-example",
        "-r",
        type=int,
        default=3,
        help="Number of rollouts per example",
    )
    parser.add_argument(
        "--max-concurrent-requests",
        "-c",
        type=int,
        default=32,
        help="Maximum number of concurrent requests",
    )
    parser.add_argument(
        "--max-tokens",
        "-t",
        type=int,
        default=None,
        help="Maximum number of tokens to generate (unset to use model default)",
    )
    parser.add_argument(
        "--temperature", "-T", type=float, default=None, help="Temperature for sampling"
    )
    parser.add_argument(
        "--sampling-args",
        "-S",
        type=json.loads,
        default=None,
        help=(
            "Sampling arguments as JSON object. Keys here override --max-tokens/--temperature. "
            'Example: \'{"enable_thinking": false, "max_tokens": 256}\''
        ),
    )
    parser.add_argument(
        "--verbose", "-v", default=False, action="store_true", help="Verbose output"
    )
    parser.add_argument(
        "--save-dataset",
        "-s",
        default=False,
        action="store_true",
        help="Save dataset to disk",
    )
    parser.add_argument(
        "--save-to-hf-hub",
        "-H",
        default=False,
        action="store_true",
        help="Save dataset to Hugging Face Hub",
    )
    parser.add_argument(
        "--hf-hub-dataset-name",
        "-D",
        type=str,
        default="",
        help="Name of dataset to save to Hugging Face Hub",
    )
    args = parser.parse_args()

    eval_environment(
        env=args.env,
        env_args=args.env_args,
        env_dir_path=args.env_dir_path,
        endpoints_path=args.endpoints_path,
        model=args.model,
        api_key_var=args.api_key_var,
        api_base_url=args.api_base_url,
        num_examples=args.num_examples,
        rollouts_per_example=args.rollouts_per_example,
        max_concurrent_requests=args.max_concurrent_requests,
        max_tokens=args.max_tokens,
        temperature=args.temperature,
        sampling_args=args.sampling_args,
        verbose=args.verbose,
        save_dataset=args.save_dataset,
        save_to_hf_hub=args.save_to_hf_hub,
        hf_hub_dataset_name=args.hf_hub_dataset_name,
    )


if __name__ == "__main__":
    main()



================================================
FILE: verifiers/scripts/init.py
================================================
import argparse
from pathlib import Path

import verifiers as vf

README_TEMPLATE = """\
# {env_id_dash}

> Replace the placeholders below, then remove this callout. Keep the Evaluation Reports section at the bottom intact so reports can auto-render.

### Overview
- **Environment ID**: `{env_id_dash}`
- **Short description**: <one-sentence description>
- **Tags**: <comma-separated tags>

### Datasets
- **Primary dataset(s)**: <name(s) and brief description>
- **Source links**: <links>
- **Split sizes**: <train/eval counts>

### Task
- **Type**: <single-turn | multi-turn | tool use>
- **Parser**: <e.g., ThinkParser, XMLParser, custom>
- **Rubric overview**: <briefly list reward functions and key metrics>

### Quickstart
Run an evaluation with default settings:

```bash
uv run vf-eval {env_id_dash}
```

Configure model and sampling:

```bash
uv run vf-eval {env_id_dash} \
  -m gpt-4.1-mini \
  -n 20 -r 3 -t 1024 -T 0.7 \
  -a '{{"key": "value"}}'  # env-specific args as JSON
```

Notes:
- Use `-a` / `--env-args` to pass environment-specific configuration as a JSON object.

### Environment Arguments
Document any supported environment arguments and their meaning. Example:

| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `foo` | str | `"bar"` | What this controls |
| `max_examples` | int | `-1` | Limit on dataset size (use -1 for all) |

### Metrics
Summarize key metrics your rubric emits and how they’re interpreted.

| Metric | Meaning |
| ------ | ------- |
| `reward` | Main scalar reward (weighted sum of criteria) |
| `accuracy` | Exact match on target answer |

"""

PYPROJECT_TEMPLATE = f"""\
[project]
name = "{{env_id}}"
description = "Your environment description here"
tags = ["placeholder-tag", "train", "eval"]
version = "0.1.0"
requires-python = ">=3.11"
dependencies = [
    "verifiers>={vf.__version__}",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = ["{{env_file}}.py"]
"""

ENVIRONMENT_TEMPLATE = """\
import verifiers as vf


def load_environment(**kwargs) -> vf.Environment:
    '''
    Loads a custom environment.
    '''
    raise NotImplementedError("Implement your custom environment here.")
"""


def init_environment(
    env: str, path: str = "./environments", rewrite_readme: bool = False
) -> Path:
    """
    Initialize a new verifiers environment.

    Args:
        env: The environment id to init
        path: Path to environments directory (default: ./environments)

    Returns:
        Path to the created environment directory
    """

    env_id_dash = env.replace("_", "-")
    env_id_underscore = env_id_dash.replace("-", "_")

    # make environment parent directory if it doesn't exist
    local_dir = Path(path) / env_id_underscore
    local_dir.mkdir(parents=True, exist_ok=True)

    # create README.md if it doesn't exist (or rewrite if flag is set)
    readme_file = local_dir / "README.md"
    if rewrite_readme or not readme_file.exists():
        readme_file.write_text(
            README_TEMPLATE.format(
                env_id_dash=env_id_dash, env_id_underscore=env_id_underscore
            )
        )
    else:
        print(f"README.md already exists at {readme_file}, skipping...")

    # create pyproject.toml if it doesn't exist
    pyproject_file = local_dir / "pyproject.toml"
    if not pyproject_file.exists():
        pyproject_file.write_text(
            PYPROJECT_TEMPLATE.format(env_id=env_id_dash, env_file=env_id_underscore)
        )
    else:
        print(f"pyproject.toml already exists at {pyproject_file}, skipping...")

    # create environment file if it doesn't exist
    environment_file = local_dir / f"{env_id_underscore}.py"
    if not environment_file.exists():
        environment_file.write_text(ENVIRONMENT_TEMPLATE.format(env_id=env_id_dash))
    else:
        print(
            f"{env_id_underscore}.py already exists at {environment_file}, skipping..."
        )

    return local_dir


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "env",
        type=str,
        help=("The environment id to init"),
    )
    parser.add_argument(
        "--path",
        "-p",
        type=str,
        default="./environments",
        help="Path to environments directory (default: ./environments)",
    )
    parser.add_argument(
        "--rewrite-readme",
        action="store_true",
        default=False,
        help="Rewrite README.md even if it already exists",
    )
    args = parser.parse_args()

    init_environment(args.env, args.path, rewrite_readme=args.rewrite_readme)


if __name__ == "__main__":
    main()



================================================
FILE: verifiers/scripts/install.py
================================================
import argparse
import subprocess
from pathlib import Path

"""
Install a local environment

Usage:
    vf-install <env_id> -p <path>

Options:
    -h, --help    Show this help message and exit
    -d, --local-dir-name <local_dir_name>    The name of the local directory to install the environment into.

"""


def install_environment(env: str, path: str, from_repo: bool, branch: str):
    env_folder = env.replace("-", "_")
    env_name = env_folder.replace("_", "-")
    if from_repo:
        subprocess.run(
            [
                "uv",
                "pip",
                "install",
                f"{env_name} @ git+https://github.com/willccbb/verifiers.git@{branch}#subdirectory=environments/{env_folder}",
            ]
        )
    else:
        env_path = Path(path) / env_folder
        subprocess.run(["uv", "pip", "install", "-e", env_path])


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("env", type=str, help="The environment id to install")
    parser.add_argument(
        "-p",
        "--path",
        type=str,
        help="Path to environments directory (default: ./environments)",
        default="./environments",
    )
    parser.add_argument(
        "-r",
        "--from-repo",
        action="store_true",
        help="Install from the Verifiers repo (default: False)",
        default=False,
    )
    parser.add_argument(
        "-b",
        "--branch",
        type=str,
        help="Branch to install from if --from-repo is True (default: main)",
        default="main",
    )
    args = parser.parse_args()

    install_environment(
        env=args.env,
        path=args.path,
        from_repo=args.from_repo,
        branch=args.branch,
    )


if __name__ == "__main__":
    main()



================================================
FILE: verifiers/scripts/tui.py
================================================
#!/usr/bin/env python3
"""
Textual-based TUI for viewing verifiers eval results.
"""

import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from textual import on
from textual.app import App, ComposeResult
from textual.binding import Binding
from textual.containers import Container, Horizontal, VerticalScroll
from textual.screen import Screen
from textual.theme import Theme
from textual.widgets import Footer, Label, OptionList, Static
from textual.widgets._option_list import Option


# ----------------------------
# Discovery and data loading
# ----------------------------
@dataclass
class RunInfo:
    env_id: str
    model: str
    run_id: str
    path: Path
    metadata: Dict[str, Any]


def _iter_eval_roots(env_dir: Path, global_outputs_dir: Path) -> List[Path]:
    roots: List[Path] = []
    if env_dir.exists():
        for child in env_dir.iterdir():
            if child.is_dir():
                candidate = child / "outputs" / "evals"
                if candidate.exists():
                    roots.append(candidate)
    if (global_outputs_dir / "evals").exists():
        roots.append(global_outputs_dir / "evals")
    return roots


def _parse_env_and_model(dir_name: str) -> Optional[Tuple[str, str]]:
    if "--" not in dir_name:
        return None
    env, model_part = dir_name.split("--", 1)
    model = model_part.replace("--", "/")
    return env, model


def discover_results(
    env_dir_path: str = "./environments", outputs_dir_path: str = "./outputs"
) -> Dict[str, Dict[str, List[RunInfo]]]:
    """
    Returns mapping: env_id -> model -> list[RunInfo]
    """
    env_dir = Path(env_dir_path)
    global_outputs_dir = Path(outputs_dir_path)
    roots = _iter_eval_roots(env_dir, global_outputs_dir)

    discovered: Dict[str, Dict[str, List[RunInfo]]] = {}
    for root in roots:
        for env_model_dir in sorted(
            root.iterdir() if root.exists() else [], key=lambda p: p.name
        ):
            if not env_model_dir.is_dir():
                continue
            parsed = _parse_env_and_model(env_model_dir.name)
            if parsed is None:
                continue
            env_id, model = parsed
            for run_dir in sorted(env_model_dir.iterdir(), key=lambda p: p.name):
                if not run_dir.is_dir():
                    continue
                meta = run_dir / "metadata.json"
                results = run_dir / "results.jsonl"
                if meta.exists() and results.exists():
                    try:
                        metadata = json.loads(meta.read_text())
                    except Exception:
                        metadata = {}
                    run = RunInfo(
                        env_id=env_id,
                        model=model,
                        run_id=run_dir.name,
                        path=run_dir,
                        metadata=metadata,
                    )
                    discovered.setdefault(env_id, {}).setdefault(model, []).append(run)

    # Sort runs by time
    for env_id, models in discovered.items():
        for model, runs in models.items():
            runs.sort(
                key=lambda r: (
                    r.metadata.get("date", ""),
                    r.metadata.get("time", ""),
                    r.run_id,
                )
            )
    return discovered


def load_run_results(run: RunInfo) -> List[Dict[str, Any]]:
    """Load results.jsonl into memory."""
    data: List[Dict[str, Any]] = []
    results_path = run.path / "results.jsonl"
    with results_path.open("r") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                data.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    return data


# ----------------------------
# Formatting helpers
# ----------------------------


def format_prompt_or_completion(prompt_or_completion) -> str:
    """Format completion for display."""
    if isinstance(prompt_or_completion, list):
        lines = []
        for msg in prompt_or_completion:
            assert isinstance(msg, dict)
            role = msg.get("role", "")
            content = str(msg.get("content", ""))
            # Assistant in white, all others in grey
            if role == "assistant":
                lines.append(f"[b]{role}:[/b] {content}")
            elif role == "tool":
                lines.append(f"[dim][b]tool result:[/b] {content}[/dim]")
            else:
                lines.append(f"[dim][b]{role}:[/b] {content}[/dim]")
            if "tool_calls" in msg and msg["tool_calls"]:
                tool_calls_data = msg["tool_calls"]
                if isinstance(tool_calls_data, list) and len(tool_calls_data) > 0 and isinstance(tool_calls_data[0], str):
                    import json
                    parsed_tool_calls = []
                    for tc_str in tool_calls_data:
                        try:
                            parsed_tool_calls.append(json.loads(tc_str))
                        except (json.JSONDecodeError, TypeError):
                            parsed_tool_calls.append(tc_str)
                    tool_calls_data = parsed_tool_calls
                
                for tool_call in tool_calls_data:
                    if isinstance(tool_call, dict) and 'function' in tool_call:
                        lines.append(
                            f"[b]tool call:[/b] {tool_call['function']['name']}\n{tool_call['function']['arguments']}"
                        )
                    elif isinstance(tool_call, str):
                        lines.append(f"[b]tool call:[/b] {tool_call}")
        return "\n\n".join(lines)
    return str(prompt_or_completion)


# ----------------------------
# Custom Panel Widget
# ----------------------------
class Panel(Container):
    """A rounded panel container."""

    DEFAULT_CSS = """
    Panel {
        border: round white;
        padding: 1 2;
        margin: 1;
    }
    """


# ----------------------------
# Screens
# ----------------------------
class SelectEnvScreen(Screen):
    """Screen for selecting an environment."""

    BINDINGS = [
        Binding("q", "quit", "Quit"),
        Binding("enter", "select", "Select"),
    ]

    def __init__(self, index: Dict[str, Dict[str, List[RunInfo]]]):
        super().__init__()
        self.index = index
        self.env_ids = sorted(index.keys())

    def compose(self) -> ComposeResult:
        with Container():
            yield Panel(
                Label("Select Environment", classes="title"),
                OptionList(id="env-list"),
            )
        yield Footer()

    def on_mount(self) -> None:
        option_list = self.query_one("#env-list", OptionList)

        if not self.env_ids:
            option_list.add_option("No completed evals found")
            return

        for env_id in self.env_ids:
            models = self.index[env_id]
            total_runs = sum(len(runs) for runs in models.values())
            option_list.add_option(
                Option(
                    f"{env_id} - Models: {len(models)}, Runs: {total_runs}", id=env_id
                )
            )

        option_list.focus()

    @on(OptionList.OptionSelected, "#env-list")
    def on_option_selected(self, event: OptionList.OptionSelected) -> None:
        """Handle option selection."""
        if event.option_id and event.option_id in self.env_ids:
            self.app.push_screen(SelectModelScreen(self.index, event.option_id))

    def action_select(self) -> None:
        """Handle Enter key."""
        option_list = self.query_one("#env-list", OptionList)
        if option_list.highlighted is not None:
            option = option_list.get_option_at_index(option_list.highlighted)
            if option and option.id in self.env_ids:
                self.app.push_screen(SelectModelScreen(self.index, option.id))


class SelectModelScreen(Screen):
    """Screen for selecting a model."""

    BINDINGS = [
        Binding("q", "quit", "Quit"),
        Binding("b,backspace", "back", "Back"),
        Binding("enter", "select", "Select"),
    ]

    def __init__(self, index: Dict[str, Dict[str, List[RunInfo]]], env_id: str):
        super().__init__()
        self.index = index
        self.env_id = env_id
        self.models = sorted(index[env_id].keys())

    def compose(self) -> ComposeResult:
        with Container():
            yield Panel(
                Label(f"[b]Environment:[/b] {self.env_id}", classes="title"),
                Label("Select Model", classes="subtitle"),
                OptionList(id="model-list"),
            )
        yield Footer()

    def on_mount(self) -> None:
        option_list = self.query_one("#model-list", OptionList)

        for model in self.models:
            runs = self.index[self.env_id][model]
            option_list.add_option(Option(f"{model} - Runs: {len(runs)}", id=model))

        option_list.focus()

    def action_back(self) -> None:
        self.app.pop_screen()

    @on(OptionList.OptionSelected, "#model-list")
    def on_option_selected(self, event: OptionList.OptionSelected) -> None:
        """Handle option selection."""
        if event.option_id and event.option_id in self.models:
            self.app.push_screen(
                SelectRunScreen(self.index, self.env_id, event.option_id)
            )

    def action_select(self) -> None:
        """Handle Enter key."""
        option_list = self.query_one("#model-list", OptionList)
        if option_list.highlighted is not None:
            option = option_list.get_option_at_index(option_list.highlighted)
            if option and option.id in self.models:
                self.app.push_screen(
                    SelectRunScreen(self.index, self.env_id, option.id)
                )


class SelectRunScreen(Screen):
    """Screen for selecting a run."""

    BINDINGS = [
        Binding("q", "quit", "Quit"),
        Binding("b,backspace", "back", "Back"),
        Binding("enter", "select", "Select"),
    ]

    def __init__(
        self, index: Dict[str, Dict[str, List[RunInfo]]], env_id: str, model: str
    ):
        super().__init__()
        self.index = index
        self.env_id = env_id
        self.model = model
        self.runs = index[env_id][model]

    def compose(self) -> ComposeResult:
        with Container():
            yield Panel(
                Label(f"[b]Environment:[/b] {self.env_id}", classes="title"),
                Label(f"[b]Model:[/b] {self.model}", classes="subtitle"),
                Label("Select Run", classes="subtitle"),
                OptionList(id="run-list"),
            )
        yield Footer()

    def on_mount(self) -> None:
        option_list = self.query_one("#run-list", OptionList)

        for i, run in enumerate(self.runs):
            meta = run.metadata
            datetime_str = f"{meta.get('date', '')} {meta.get('time', '')}".strip()
            reward = meta.get("avg_reward", "")
            if isinstance(reward, (int, float)):
                reward_str = f"Reward: {reward:.3f}"
            else:
                reward_str = f"Reward: {reward}"

            option_list.add_option(
                Option(f"{run.run_id} - {datetime_str} | {reward_str}", id=str(i))
            )

        option_list.focus()

    def action_back(self) -> None:
        self.app.pop_screen()

    @on(OptionList.OptionSelected, "#run-list")
    def on_option_selected(self, event: OptionList.OptionSelected) -> None:
        """Handle option selection."""
        if event.option_id is not None:
            idx = int(event.option_id)
            if 0 <= idx < len(self.runs):
                self.app.push_screen(ViewRunScreen(self.runs[idx]))

    def action_select(self) -> None:
        """Handle Enter key."""
        option_list = self.query_one("#run-list", OptionList)
        if option_list.highlighted is not None:
            option = option_list.get_option_at_index(option_list.highlighted)
            if option and option.id is not None:
                idx = int(option.id)
                if 0 <= idx < len(self.runs):
                    self.app.push_screen(ViewRunScreen(self.runs[idx]))


class ViewRunScreen(Screen):
    """Screen for viewing run details and rollouts."""

    BINDINGS = [
        Binding("q", "quit", "Quit"),
        Binding("b,backspace", "back", "Back"),
        Binding("left,h", "prev_record", "Previous"),
        Binding("right,l", "next_record", "Next"),
    ]

    def __init__(self, run: RunInfo):
        super().__init__()
        self.run = run
        self.records = load_run_results(run)
        self.current_record_idx = 0

    def compose(self) -> ComposeResult:
        with Container():
            # Metadata section
            yield Panel(
                Static(self._get_metadata_text(), id="metadata"),
                classes="metadata-panel",
            )

            # Rollout section with two columns
            with Horizontal(classes="rollout-container"):
                with Panel(classes="column-panel"):
                    yield Label("Prompt", classes="column-header")
                    yield VerticalScroll(
                        Static("", id="prompt-content"),
                        id="prompt-scroll",
                    )

                with Panel(classes="column-panel"):
                    yield Label("Completion", classes="column-header")
                    yield VerticalScroll(
                        Static("", id="completion-content"),
                        id="completion-scroll",
                    )

            # Details section (horizontal scroll)
            yield Panel(Static("", id="details"), classes="details-panel")

        yield Footer()

    def _get_metadata_text(self) -> str:
        """Generate metadata text in columns."""
        meta = self.run.metadata
        avg_reward = meta.get("avg_reward", "")
        if isinstance(avg_reward, (int, float)):
            avg_reward_str = f"{avg_reward:.3f}"
        else:
            avg_reward_str = str(avg_reward) if avg_reward else "N/A"

        # Create three columns of information
        col1 = [
            f"[b]Environment:[/b] {self.run.env_id}",
            f"[b]Model:[/b] {self.run.model}",
            f"[b]Run ID:[/b] {self.run.run_id}",
            f"[b]Date:[/b] {meta.get('date', '')} {meta.get('time', '')}",
        ]

        col2 = [
            f"[b]Record:[/b] {self.current_record_idx + 1}/{len(self.records)}",
            f"[b]Examples:[/b] {meta.get('num_examples', '')}",
            f"[b]Rollouts/ex:[/b] {meta.get('rollouts_per_example', '')}",
            "",  # Empty for alignment
        ]

        col3 = [
            f"[b]Avg reward:[/b] {avg_reward_str}",
            f"[b]Max tokens:[/b] {meta.get('max_tokens', '')}",
            f"[b]Temperature:[/b] {meta.get('temperature', '')}",
            "",  # Empty for alignment
        ]

        # Format as columns with consistent spacing
        lines = []
        for i in range(max(len(col1), len(col2), len(col3))):
            parts = []
            if i < len(col1):
                parts.append(f"{col1[i]:<45}")
            else:
                parts.append(" " * 45)
            if i < len(col2):
                parts.append(f"{col2[i]:<35}")
            else:
                parts.append(" " * 35)
            if i < len(col3):
                parts.append(col3[i] if i < len(col3) else "")
            lines.append("".join(parts).rstrip())

        return "\n".join(lines)

    def on_mount(self) -> None:
        self.update_display()

    def update_display(self) -> None:
        """Update the display with current record."""
        if not self.records:
            return

        record = self.records[self.current_record_idx]

        # Update prompt
        prompt = record.get("prompt", "")
        prompt_widget = self.query_one("#prompt-content", Static)
        prompt_widget.update(format_prompt_or_completion(prompt))

        # Update completion
        completion = record.get("completion", "")
        completion_widget = self.query_one("#completion-content", Static)
        completion_widget.update(format_prompt_or_completion(completion))

        # Update details
        details_lines = []
        reward = record.get("reward", None)
        if reward is not None:
            if isinstance(reward, (int, float)):
                reward_str = f"{reward:.3f}"
            else:
                reward_str = str(reward)
            details_lines.append(f"[b]Reward:[/b] {reward_str}")
        answer = record.get("answer", None)
        if answer not in (None, ""):
            details_lines.append(f"[b]Answer:[/b] {answer}")
        info = record.get("info", None)
        if info not in (None, {}):
            details_lines.append(f"[b]Info:[/b] {str(info)}")
        task = record.get("task", None)
        if task not in (None, ""):
            details_lines.append(f"[b]Task:[/b] {task}")

        details_widget = self.query_one("#details", Static)
        details_widget.update(
            "\n".join(details_lines)
            if details_lines
            else "[dim]No additional details[/dim]"
        )

        # Update metadata with current record index
        metadata_widget = self.query_one("#metadata", Static)
        metadata_widget.update(self._get_metadata_text())

    def action_back(self) -> None:
        self.app.pop_screen()

    def action_prev_record(self) -> None:
        if self.records:
            self.current_record_idx = (self.current_record_idx - 1) % len(self.records)
            self.update_display()
            # Reset scroll positions
            self.query_one("#prompt-scroll").scroll_y = 0
            self.query_one("#completion-scroll").scroll_y = 0

    def action_next_record(self) -> None:
        if self.records:
            self.current_record_idx = (self.current_record_idx + 1) % len(self.records)
            self.update_display()
            # Reset scroll positions
            self.query_one("#prompt-scroll").scroll_y = 0
            self.query_one("#completion-scroll").scroll_y = 0


# ----------------------------
# Main App
# ----------------------------
class VerifiersTUI(App):
    """Textual-based TUI for viewing verifiers eval results."""

    # Custom dark theme with a modern color palette
    ENABLE_COMMAND_PALETTE = False  # Disable command palette for cleaner UI

    # Define custom dark theme
    BLACK_WARM_THEME = Theme(
        name="black-warm",
        primary="#d4a373",  # Warm tan/beige
        secondary="#808080",  # Gray
        accent="#c9ada7",  # Muted rose
        warning="#ffa500",  # Orange
        error="#ff6b6b",  # Soft red
        success="#98c379",  # Soft green
        background="#141414",
        surface="#141414",
        panel="#141414",
        foreground="#ffffff",
        dark=True,
    )

    # Define custom light theme with matching warm tones
    WHITE_WARM_THEME = Theme(
        name="white-warm",
        primary="#8b6f47",  # Darker warm brown (darker than dark theme for contrast)
        secondary="#606060",  # Medium gray
        accent="#a08b87",  # Muted warm brown-rose
        warning="#ff8c00",  # Dark orange
        error="#dc143c",  # Crimson
        success="#6b8e23",  # Olive green
        background="#f5f5f5",  # Light warm grey
        surface="#f5f5f5",  # Light warm grey
        panel="#f5f5f5",  # Light warm grey
        foreground="#1a1a1a",  # Near black
        dark=False,
    )

    BINDINGS = [
        ("q", "quit", "Quit"),
        ("d", "toggle_dark", "Toggle dark mode"),
    ]

    CSS = """
    /* Clean black theme */
    Screen {
        layout: vertical;
        background: $background;
    }
    
    Panel {
        border: round $primary;
        padding: 1 2;
        margin: 0 0 1 0;
        background: $panel;
    }
    
    Label {
        color: $text;
    }
    
    Static {
        color: $text;
    }
    
    .title {
        text-style: bold;
        color: $text;
        margin-bottom: 1;
    }
    
    .subtitle {
        color: $text-muted;
        margin-bottom: 1;
    }
    
    OptionList {
        height: auto;
        max-height: 20;
        background: $surface;
        color: $text;
    }
    
    OptionList > .option-list--option-highlighted {
        background: $primary 20%;
    }
    
    #view-container {
        layout: vertical;
        height: 100%;
    }
    
    .metadata-panel {
        height: auto;
        min-height: 6;
        max-height: 8;
    }
    
    .rollout-container {
        height: 1fr;
        layout: horizontal;
    }
    
    .column-panel {
        width: 50%;
        height: 100%;
        layout: vertical;
    }
    
    .column-header {
        height: auto;
        margin-bottom: 1;
        text-align: center;
        text-style: bold;
    }
    
    #prompt-scroll, #completion-scroll {
        height: 1fr;
        background: $surface;
        padding: 0 1;
        scrollbar-color: $secondary;
        scrollbar-background: $panel;
        scrollbar-corner-color: $panel;
    }
    
    .details-panel {
        height: auto;
        min-height: 3;
        max-height: 6;
    }
    
    Footer {
        background: $panel;
    }
    """

    def __init__(
        self, env_dir_path: str = "./environments", outputs_dir_path: str = "./outputs"
    ):
        super().__init__()
        self.env_dir_path = env_dir_path
        self.outputs_dir_path = outputs_dir_path
        self.index = discover_results(env_dir_path, outputs_dir_path)

    def on_mount(self) -> None:
        # Register both custom themes
        self.register_theme(self.BLACK_WARM_THEME)
        self.register_theme(self.WHITE_WARM_THEME)
        # Start with dark theme
        self.theme = "black-warm"
        self.push_screen(SelectEnvScreen(self.index))

    async def action_quit(self) -> None:
        """Quit the application."""
        self.exit()

    def action_toggle_dark(self) -> None:
        """Toggle between dark and light themes."""
        # Toggle between our custom dark and light themes
        if self.theme == "black-warm":
            self.theme = "white-warm"
        else:
            self.theme = "black-warm"


def main() -> None:
    # Optional args via env vars
    env_dir = os.environ.get("VF_ENV_DIR", "./environments")
    outputs_dir = os.environ.get("VF_OUTPUTS_DIR", "./outputs")
    app = VerifiersTUI(env_dir, outputs_dir)
    app.run()


if __name__ == "__main__":
    main()



================================================
FILE: verifiers/trainers/__init__.py
================================================
try:
    import torch._dynamo  # type: ignore

    torch._dynamo.config.suppress_errors = True  # type: ignore
except ImportError:
    pass

from peft import LoraConfig  # type: ignore

from .grpo_config import GRPOConfig
from .grpo_trainer import GRPOTrainer


def grpo_defaults(run_name: str) -> GRPOConfig:
    return GRPOConfig(
        output_dir=f"outputs/{run_name}",
        run_name=run_name,
        learning_rate=1e-6,
        lr_scheduler_type="constant_with_warmup",
        warmup_steps=10,
        max_steps=500,
        bf16=True,
        max_grad_norm=0.01,
        num_iterations=1,
        max_seq_len=4096,
        per_device_train_batch_size=8,
        num_generations=8,
        gradient_accumulation_steps=4,
        gradient_checkpointing=True,
        save_strategy="steps",
        save_steps=500,
        save_only_model=True,
        logging_steps=1,
        log_on_each_node=False,
        log_completions=True,
        report_to="wandb",
    )


def lora_defaults(r=8, alpha=16) -> LoraConfig:
    return LoraConfig(
        r=r,
        lora_alpha=alpha,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        task_type="CAUSAL_LM",
    )


__all__ = ["GRPOConfig", "GRPOTrainer", "grpo_defaults", "lora_defaults"]



================================================
FILE: verifiers/trainers/async_batch_generator.py
================================================
import asyncio
import logging
import queue
import threading
import time
from collections import deque
from typing import Any

from pydantic import BaseModel, Field

from verifiers import GenerateOutputs
from verifiers.types import ProcessedOutputs


class BatchRequest(BaseModel):
    """Request for batch generation"""

    batch_id: int
    env_inputs: dict[str, list[Any]]
    processing_class: Any
    mask_env_responses: bool
    max_seq_len: int
    mask_truncated_completions: bool
    zero_truncated_completions: bool
    max_concurrent: int


class BatchResult(BaseModel):
    """Result from batch generation"""

    batch_id: int
    processed_results: ProcessedOutputs
    generation_time: float = 0.0
    all_reward_dict: dict[str, list[float]] = Field(
        default_factory=dict
    )  # All reward scores
    completions: list[Any] = Field(
        default_factory=list
    )  # Store completions for logging
    prompts: list[Any] = Field(default_factory=list)  # Store prompts for logging


class AsyncBatchGenerator:
    """
    Manages asynchronous batch generation for GRPO training.

    This class runs generation in a separate thread, allowing training to continue
    while future batches are being generated. It maintains a queue of pending
    generation requests and completed results.
    """

    def __init__(
        self,
        env,
        client_config,
        model_name: str,
        sampling_args: dict[str, Any],
        num_batches_ahead: int = 1,
        max_queue_size: int | None = None,
        generation_timeout: float = 300.0,  # 5 minutes default
    ):
        self.env = env
        self.client_config = client_config
        self.client = None  # Will be created in worker thread
        self.model_name = model_name
        self.sampling_args = sampling_args
        self.num_batches_ahead = num_batches_ahead
        self.max_queue_size = max_queue_size or max(num_batches_ahead * 2, 4)
        self.generation_timeout = generation_timeout

        # Queues for communication
        self.request_queue = queue.Queue()
        self.result_queue = queue.Queue()
        self.is_generating = False

        # Tracking
        self.pending_batches = set()  # batch_ids currently being processed
        self.completed_batches = {}  # batch_id -> BatchResult
        self.next_expected_batch = 0
        self.generation_times = deque(maxlen=100)  # Track recent generation times

        # Thread management
        self.worker_thread = None
        self.stop_event = threading.Event()
        self.logger = logging.getLogger(f"AsyncBatchGenerator-{id(self)}")
        self.is_generating = False  # Track if currently generating
        self.worker_loop = None  # Will be set in worker thread
        self.started = False  # Track if generator is started

        # Synchronization
        self._lock = threading.Lock()

    def start(self):
        """Start the async generation worker thread"""
        if self.started:
            return

        self.worker_thread = threading.Thread(
            target=self._generation_worker, daemon=True, name="AsyncBatchGenerator"
        )
        self.worker_thread.start()
        self.started = True

    def stop(self):
        """Stop the async generation worker thread"""
        if not self.started:
            return

        self.stop_event.set()
        # Send poison pill
        self.request_queue.put(None)

        if self.worker_thread:
            self.worker_thread.join(timeout=10.0)

        self.started = False

    def submit_batch(self, request: BatchRequest) -> bool:
        """
        Submit a batch for async generation.

        Returns:
            bool: True if submitted successfully, False if queue is full
        """
        if not self.started:
            raise RuntimeError("AsyncBatchGenerator not started")

        with self._lock:
            if request.batch_id in self.pending_batches:
                return True  # Already submitted

            if len(self.pending_batches) >= self.max_queue_size:
                return False  # Queue full

            self.pending_batches.add(request.batch_id)

        self.request_queue.put(request)
        return True

    def get_batch(self, batch_id: int, timeout: float | None = None) -> BatchResult:
        """
        Get a completed batch result. Blocks until the batch is ready.

        Args:
            batch_id: The batch ID to retrieve
            timeout: Maximum time to wait (uses generation_timeout if None)

        Returns:
            BatchResult: The completed batch result

        Raises:
            TimeoutError: If batch doesn't complete within timeout
            RuntimeError: If generation failed
        """
        timeout = timeout or self.generation_timeout
        start_time = time.time()

        while True:
            # Check if already completed
            with self._lock:
                if batch_id in self.completed_batches:
                    return self.completed_batches.pop(batch_id)

            # Check for new results
            try:
                result = self.result_queue.get(timeout=0.1)
                with self._lock:
                    self.completed_batches[result.batch_id] = result
                    self.pending_batches.discard(result.batch_id)

                # If this is our batch, return it
                if result.batch_id == batch_id:
                    with self._lock:
                        return self.completed_batches.pop(batch_id)

            except queue.Empty:
                pass

            # Check timeout
            if time.time() - start_time > timeout:
                raise TimeoutError(
                    f"Batch {batch_id} generation timed out after {timeout}s"
                )

    def get_pending_count(self) -> int:
        """Get number of batches currently being generated"""
        with self._lock:
            return len(self.pending_batches)

    def get_completed_count(self) -> int:
        """Get number of completed batches waiting to be retrieved"""
        with self._lock:
            return len(self.completed_batches)

    def get_average_generation_time(self) -> float:
        """Get average generation time for recent batches"""
        if not self.generation_times:
            return 0.0
        return sum(self.generation_times) / len(self.generation_times)

    def should_submit_more(self) -> bool:
        """Check if we should submit more batches for generation"""
        with self._lock:
            total_pending = len(self.pending_batches) + len(self.completed_batches)
            return total_pending < self.num_batches_ahead

    def _generation_worker(self):
        """Worker thread that processes generation requests"""
        # Create event loop for this thread
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        self.worker_loop = loop  # Store the event loop reference

        # Create the AsyncOpenAI client within this event loop
        import httpx
        from openai import AsyncOpenAI

        self.client = AsyncOpenAI(
            base_url=self.client_config["base_url"],
            api_key=self.client_config["api_key"],
            http_client=httpx.AsyncClient(
                limits=httpx.Limits(
                    max_connections=self.client_config["http_client_args"]["limits"][
                        "max_connections"
                    ]
                ),
                timeout=self.client_config["http_client_args"]["timeout"],
            ),
        )

        try:
            while not self.stop_event.is_set():
                try:
                    # Get next request
                    request = self.request_queue.get(timeout=0.1)
                    if request is None:  # Poison pill
                        break

                    # Generate batch using the async method
                    start_time = time.time()
                    result = loop.run_until_complete(
                        self._generate_batch_async(request)
                    )
                    generation_time = time.time() - start_time
                    result.generation_time = generation_time
                    self.generation_times.append(generation_time)
                    self.result_queue.put(result)
                except queue.Empty:
                    continue
                except Exception as e:
                    self.logger.error(f"Error in generation worker: {e}")
                    raise e
        finally:
            # Clean up the client
            if self.client:
                loop.run_until_complete(self.client.close())
            # Clean up the event loop
            loop.close()
            asyncio.set_event_loop(None)

    async def _generate_batch_async(self, request: BatchRequest) -> BatchResult:
        """
        Generate a single batch asynchronously.
        """
        # Call environment generation
        self.is_generating = True
        env_results = await self.env.a_generate(
            request.env_inputs,
            client=self.client,
            model=self.model_name,
            sampling_args=self.sampling_args,
            score_rollouts=True,
            max_concurrent=request.max_concurrent,
        )
        self.is_generating = False

        # Extract all reward-related keys
        all_reward_dict = {
            "reward": env_results.reward,
        }
        for k in env_results.metrics:
            all_reward_dict[k] = env_results.metrics[k]

        # Process results
        processed_results = self.env.process_env_results_vllm(
            prompts=env_results.prompt,
            completions=env_results.completion,
            states=env_results.state,
            rewards=env_results.reward,
            processing_class=request.processing_class,
            max_seq_len=request.max_seq_len,
            mask_env_responses=request.mask_env_responses,
            mask_truncated_completions=request.mask_truncated_completions,
            zero_truncated_completions=request.zero_truncated_completions,
        )

        return BatchResult(
            batch_id=request.batch_id,
            processed_results=processed_results,
            all_reward_dict=all_reward_dict,
            completions=env_results.completion,
            prompts=env_results.prompt,
        )

    async def _evaluate_async(self, num_samples: int = -1) -> GenerateOutputs:
        """
        Run evaluation in the worker thread's event loop.
        """
        # Get evaluation dataset
        if self.env.eval_dataset is None:
            self.env.logger.info(
                "eval_dataset is not set, falling back to train dataset"
            )
            assert self.env.dataset is not None
            inputs = self.env.get_dataset(n=num_samples)
        else:
            inputs = self.env.get_eval_dataset(n=num_samples)
        assert inputs is not None, "No dataset found"

        # Run generation on eval dataset
        results = await self.env.a_generate(
            inputs,
            client=self.client,
            model=self.model_name,
            sampling_args=self.sampling_args,
        )
        return results

    def evaluate(self, num_samples: int = -1) -> GenerateOutputs:
        """
        Run evaluation synchronously by creating a separate thread with its own event loop.
        """
        if not self.started:
            raise RuntimeError("AsyncBatchGenerator not started")

        # Run evaluation in a separate thread to avoid event loop conflicts
        result_container = []
        exception_container = []

        def run_evaluation():
            # Create a new event loop for this thread
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            # Create a new client for this evaluation
            import httpx
            from openai import AsyncOpenAI

            eval_client = AsyncOpenAI(
                base_url=self.client_config["base_url"],
                api_key=self.client_config["api_key"],
                http_client=httpx.AsyncClient(
                    limits=httpx.Limits(
                        max_connections=self.client_config["http_client_args"][
                            "limits"
                        ]["max_connections"]
                    ),
                    timeout=self.client_config["http_client_args"]["timeout"],
                ),
            )

            async def run_eval():
                try:
                    # Get evaluation dataset
                    if self.env.eval_dataset is None:
                        self.env.logger.info(
                            "eval_dataset is not set, falling back to train dataset"
                        )
                        assert self.env.dataset is not None
                        inputs = self.env.get_dataset(n=num_samples)
                    else:
                        inputs = self.env.get_eval_dataset(n=num_samples)
                    assert inputs is not None, "No dataset found"

                    # Run generation on eval dataset
                    results = await self.env.a_generate(
                        inputs,
                        client=eval_client,
                        model=self.model_name,
                        sampling_args=self.sampling_args,
                    )
                    result_container.append(results)
                except Exception as e:
                    exception_container.append(e)
                finally:
                    await eval_client.close()

            try:
                loop.run_until_complete(run_eval())
            finally:
                loop.close()
                asyncio.set_event_loop(None)

        # Run evaluation in a separate thread
        import threading

        eval_thread = threading.Thread(target=run_evaluation)
        eval_thread.start()
        eval_thread.join(timeout=self.generation_timeout)

        if eval_thread.is_alive():
            raise TimeoutError(f"Evaluation timed out after {self.generation_timeout}s")

        if exception_container:
            raise exception_container[0]

        if not result_container:
            raise RuntimeError("Evaluation completed but no results were returned")

        return result_container[0]



================================================
FILE: verifiers/trainers/async_dataloader_wrapper.py
================================================
import threading
from collections import deque
from typing import Any

from torch.utils.data import DataLoader


class AsyncDataLoaderWrapper:
    """
    Wraps a DataLoader to provide batch prefetching capabilities for async generation.

    This wrapper maintains a buffer of upcoming batches that can be accessed
    without advancing the main iterator, allowing async generation to work
    ahead while training continues on current batches.
    """

    def __init__(self, dataloader: DataLoader, buffer_size: int = 5):
        self.dataloader = dataloader
        self.buffer_size = buffer_size
        self._buffer = deque(maxlen=buffer_size)
        self._current_iterator = None  # Iterator for current epoch
        self._next_iterator = None  # Iterator for next epoch (created when needed)
        self._lock = threading.Lock()
        self._exhausted = False
        self._current_batch = None  # Track the current batch

    def __iter__(self):
        """Reset and return iterator"""
        with self._lock:
            # If we pre-created an iterator for the next epoch, use it
            if self._next_iterator is not None:
                self._current_iterator = self._next_iterator
                self._next_iterator = None
            else:
                self._current_iterator = iter(self.dataloader)

            self._buffer.clear()
            self._exhausted = False
            self._current_batch = None
        return self

    def __next__(self):
        """Get next batch, refilling buffer as needed"""
        with self._lock:
            # If buffer is empty, try to fill it
            if not self._buffer and not self._exhausted:
                self._fill_buffer()

            if not self._buffer:
                raise StopIteration

            # Store current batch before returning
            self._current_batch = self._buffer.popleft()
            return self._current_batch

    def peek_ahead(self, n: int = 1) -> list[Any]:
        """
        Peek at the next n batches without consuming them.
        If n=0, returns the current batch (if available).
        Returns fewer batches if not enough are available.
        """
        with self._lock:
            if n == 0:
                # Return current batch if available
                return [self._current_batch] if self._current_batch is not None else []

            # Ensure buffer has enough items
            while len(self._buffer) < n and not self._exhausted:
                self._fill_buffer_single()

            # Return up to n items from buffer
            return list(self._buffer)[:n]

    def _fill_buffer(self):
        """Fill the buffer up to buffer_size"""
        while len(self._buffer) < self.buffer_size and not self._exhausted:
            self._fill_buffer_single()

    def _fill_buffer_single(self):
        """Add a single batch to the buffer"""
        # Initialize current iterator if needed
        if self._current_iterator is None:
            self._current_iterator = iter(self.dataloader)

        try:
            # Try to get batch from current iterator
            batch = next(self._current_iterator)
            self._buffer.append(batch)
        except StopIteration:
            # Current epoch exhausted - try to create iterator for next epoch
            if self._next_iterator is None:
                try:
                    self._next_iterator = iter(self.dataloader)
                except Exception:
                    # Can't create new iterator, we're done
                    self._exhausted = True
                    return

            # Try to get batch from next epoch's iterator
            try:
                batch = next(self._next_iterator)
                self._buffer.append(batch)
            except StopIteration:
                # Next iterator also exhausted, we're truly done
                self._exhausted = True

    def get_future_batches(self, start_offset: int, count: int) -> list[Any]:
        """
        Get future batches starting from start_offset positions ahead.
        This is used by async generation to get batches for future steps.

        Args:
            start_offset: How many batches ahead to start
            count: Number of batches to return

        Returns:
            list of batches (may be fewer than requested if not available)
        """
        with self._lock:
            # Ensure we have enough batches in buffer
            needed = start_offset + count
            while len(self._buffer) < needed and not self._exhausted:
                self._fill_buffer_single()

            # Extract the requested range
            result = []
            for i in range(start_offset, min(start_offset + count, len(self._buffer))):
                result.append(self._buffer[i])

            return result

    def __len__(self):
        """Return length of underlying dataloader if available"""
        return len(self.dataloader)

    @property
    def batch_size(self):
        """Return batch size of underlying dataloader"""
        return self.dataloader.batch_size

    @property
    def dataset(self):
        """Return dataset of underlying dataloader"""
        return self.dataloader.dataset



================================================
FILE: verifiers/trainers/grpo_config.py
================================================
# adapted from https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_config.py

from dataclasses import dataclass, field
from typing import Optional, Union

import transformers
from packaging import version
from transformers import TrainingArguments  # type: ignore


@dataclass
class GRPOConfig(TrainingArguments):
    r"""
    Configuration class for the [`GRPOTrainer`].

    Only the parameters specific to GRPO training are listed here. For details on other parameters, refer to the
    [`~transformers.TrainingArguments`] documentation.

    Using [`~transformers.HfArgumentParser`] we can turn this class into
    [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the
    command line.
    """

    if version.parse(transformers.__version__) <= version.parse("4.50.3"):
        from transformers.training_args import _VALID_DICT_FIELDS  # type: ignore

        _VALID_DICT_FIELDS.append("model_init_kwargs")
    else:
        _VALID_DICT_FIELDS = TrainingArguments._VALID_DICT_FIELDS + [
            "model_init_kwargs"
        ]

    # Parameters that control the model and reference model
    model_init_kwargs: Optional[Union[dict, str]] = field(
        default=None,
        metadata={
            "help": "Keyword arguments for `transformers.AutoModelForCausalLM.from_pretrained`, used when the `model` "
            "argument of the `GRPOTrainer` is provided as a string."
        },
    )

    # Parameters that control the model and reference model
    disable_dropout: bool = field(
        default=False,
        metadata={
            "help": "Whether to disable dropout in the model. This is useful for training with a reference model, as "
            "it prevents the model from generating different logprobs for the same input."
        },
    )

    # Parameters that control the data preprocessing
    # The default value remove_unused_columns is overwritten from the parent class, because in GRPO we usually rely on
    # additional columns to compute the reward
    remove_unused_columns: Optional[bool] = field(
        default=False,
        metadata={
            "help": "Whether to only keep the column 'prompt' in the dataset. If you use a custom reward function "
            "that requires any column other than 'prompts' and 'completions', you should keep this to `False`."
        },
    )
    max_prompt_length: Optional[int] = field(
        default=512,
        metadata={
            "help": "Maximum length of the prompt. If the prompt is longer than this value, it will be truncated left."
        },
    )
    num_generations: int = field(
        default=8,
        metadata={
            "help": "Number of generations to sample. The effective batch size (num_processes * per_device_batch_size "
            "* gradient_accumulation_steps) must be evenly divisible by this value."
        },
    )
    max_completion_length: Optional[int] = field(
        default=None,
        metadata={"help": "Deprecated. Use `max_seq_len` instead."},
    )
    ds3_gather_for_generation: bool = field(
        default=True,
        metadata={
            "help": "This setting applies to DeepSpeed ZeRO-3. If enabled, the policy model weights are gathered for "
            "generation, improving generation speed. However, disabling this option allows training models that "
            "exceed the VRAM capacity of a single GPU, albeit at the cost of slower generation. Disabling this option "
            "is not compatible with vLLM generation."
        },
    )
    shuffle_dataset: bool = field(
        default=True,
        metadata={"help": "Whether to shuffle the training dataset."},
    )

    # Parameters that control generation
    generation_batch_size: Optional[int] = field(
        default=None,
        metadata={
            "help": "Batch size to use for generation. If `None`, it defaults to the effective training batch size: "
            "`per_device_train_batch_size * num_processes * gradient_accumulation_steps`."
        },
    )
    steps_per_generation: Optional[int] = field(
        default=None,
        metadata={
            "help": "Number of optimization steps per generation. If `None`, it defaults to gradient_accumulation_steps."
        },
    )
    max_tokens: Optional[int] = field(
        default=None,
        metadata={"help": "Maximum number of tokens to generate per turn."},
    )
    max_seq_len: Optional[int] = field(
        default=2048,
        metadata={"help": "Maximum number of tokens to generate per turn."},
    )
    temperature: float = field(
        default=1.0,
        metadata={
            "help": "Temperature for sampling. The higher the temperature, the more random the completions."
        },
    )
    top_p: float = field(
        default=1.0,
        metadata={
            "help": "Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1]. "
            "Set to 1.0 to consider all tokens."
        },
    )
    top_k: Optional[int] = field(
        default=None,
        metadata={
            "help": "Number of highest probability vocabulary tokens to keep for top-k-filtering. If `None`, "
            "top-k-filtering is disabled."
        },
    )
    min_p: Optional[float] = field(
        default=None,
        metadata={
            "help": "Minimum token probability, which will be scaled by the probability of the most likely token. It "
            "must be a value between 0.0 and 1.0. Typical values are in the 0.01-0.2 range."
        },
    )
    repetition_penalty: float = field(
        default=1.0,
        metadata={
            "help": "Float that penalizes new tokens based on whether they appear in the prompt and the generated "
            "text so far. Values > 1.0 encourage the model to use new tokens, while values < 1.0 encourage the model "
            "to repeat tokens."
        },
    )
    presence_penalty: float = field(
        default=0.0,
        metadata={"help": "Presence penalty (default 0.0)"},
    )
    frequency_penalty: float = field(
        default=0.0,
        metadata={"help": "Frequency penalty (default 0.0)"},
    )
    max_data_workers: int = field(
        default=8,
        metadata={
            "help": "Maximum number of processes to use for filtering the dataset."
        },
    )
    max_concurrent: int = field(
        default=1024,
        metadata={"help": "Maximum number of concurrent requests to the environment."},
    )
    # Async generation parameters
    num_batches_ahead: int = field(
        default=1,
        metadata={
            "help": "Number of batches to generate ahead. Higher values can improve GPU utilization but use more memory. "
            "Set to 0 for synchronous generation (submit and wait immediately, no look-ahead)."
        },
    )
    async_generation_timeout: float = field(
        default=600.0,
        metadata={
            "help": "Timeout in seconds for async generation. If a batch doesn't complete within this time, "
            "a TimeoutError is raised."
        },
    )
    async_max_queue_size: Optional[int] = field(
        default=None,
        metadata={
            "help": "Maximum number of batches that can be queued for async generation. If None, defaults to "
            "2 * num_batches_ahead."
        },
    )

    vllm_guided_decoding_regex: Optional[str] = field(
        default=None,
        metadata={
            "help": "Regex for vLLM guided decoding. If `None` (default), guided decoding is disabled."
        },
    )

    # Parameters that control the vLLM server
    vllm_server_host: str = field(
        default="0.0.0.0",
        metadata={"help": "Host of the vLLM server to connect to."},
    )
    vllm_server_port: int = field(
        default=8000,
        metadata={"help": "Port of the vLLM server to connect to."},
    )
    vllm_server_timeout: float = field(
        default=300.0,
        metadata={
            "help": "Total timeout duration in seconds to wait for the vLLM server to be up. If the server is not up "
            "after the timeout, a `ConnectionError` is raised."
        },
    )
    # Parameters that control the training
    learning_rate: float = field(
        default=1e-6,
        metadata={
            "help": "Initial learning rate for `AdamW` optimizer. The default value replaces that of "
            "`transformers.TrainingArguments`."
        },
    )
    beta: float = field(
        default=0.001,
        metadata={
            "help": "KL coefficient. If `0.0`, the reference model is not loaded, reducing memory usage and improving "
            "training speed, but may be numerically unstable for long training runs."
        },
    )
    num_iterations: int = field(
        default=1,
        metadata={
            "help": "Number of iterations per batch (denoted as μ in the algorithm)."
        },
    )
    epsilon: float = field(
        default=0.2,
        metadata={"help": "Epsilon value for clipping."},
    )
    delta: Optional[float] = field(
        default=None,
        metadata={
            "help": "If set to a float value (e.g., 2.0), enables the upper clipping bound in two-sided GRPO loss. If None (default), the standard GRPO clipping is used. Recommended to be > 1 + epsilon when enabled."
        },
    )
    epsilon_high: Optional[float] = field(
        default=None,
        metadata={
            "help": "Upper-bound epsilon value for clipping. If not specified, it defaults to the same value as the "
            "lower-bound specified in argument `epsilon`. Paper DAPO recommends `0.28`."
        },
    )
    scale_rewards: bool = field(
        default=False,
        metadata={
            "help": "Whether to scale the rewards by dividing them by their standard deviation. If `True` (default), "
            "the rewards are normalized by the standard deviation, ensuring they have unit variance. If `False`, no "
            "scaling is applied. The Dr. GRPO paper recommends not scaling the rewards, as scaling by the standard "
            "deviation introduces a question-level difficulty bias."
        },
    )
    loss_type: str = field(
        default="dr_grpo",
        metadata={
            "help": "Specifies the loss formulation to use. Supported values are `grpo`, `bnpo`, and `dr_grpo`. "
            "`'grpo'`: Aggregates token-level losses by normalizing over sequence length. Not recommended due to "
            "length bias—this approach tends to prefer shorter completions with positive advantages and longer ones "
            "with negative advantages. "
            "`'bnpo'`: Aggregates token-level losses by normalizing number of active token in the local batch. "
            "Note that normalization is performed over the local batch only, so results may slightly vary depending "
            "on the local batch size, despite a constant effective batch size. When using "
            "`per_device_train_batch_size==1`, the loss is equivalent to the GRPO loss. "
            "`'dr_grpo'`: Aggregates token-level losses by normalizing with a global constant. This method was "
            "introduced in the Dr. GRPO paper to eliminate length bias. The value of the constant corresponds to "
            "`max_completion_length`."
        },
    )
    mask_env_responses: bool = field(
        default=True,
        metadata={
            "help": "Whether to mask the environment responses. If `True`, the environment responses are masked, "
            "preventing them from being incorrectly penalized and introducing noise during training."
        },
    )
    mask_truncated_completions: bool = field(
        default=True,
        metadata={
            "help": "When enabled, truncated completions are excluded from the loss calculation, preventing them from "
            "being incorrectly penalized and introducing noise during training. According to the DAPO paper, this is "
            "a good practice for training stability."
        },
    )
    zero_truncated_completions: bool = field(
        default=False,
        metadata={"help": "Whether to give zero reward to truncated completions."},
    )
    sync_ref_model: bool = field(
        default=True,
        metadata={
            "help": "Whether to synchronize the reference model with the active model every `ref_model_sync_steps` "
            "steps, using the `ref_model_mixup_alpha` parameter."
        },
    )
    ref_model_mixup_alpha: float = field(
        default=0.5,
        metadata={
            "help": "α parameter from the TR-DPO paper, which controls the mix between the current policy and the "
            "previous reference policy during updates. The reference policy is updated according to the equation: "
            "`π_ref = α * π_θ + (1 - α) * π_ref_prev`. To use this parameter, you must set `sync_ref_model=True`."
        },
    )
    ref_model_sync_steps: int = field(
        default=100,
        metadata={
            "help": "τ parameter from the TR-DPO paper, which determines how frequently the current policy is "
            "synchronized with the reference policy. To use this parameter, you must set `sync_ref_model=True`."
        },
    )
    # Parameters that control the logging
    log_completions: bool = field(
        default=False,
        metadata={
            "help": "Whether to log a sample of (prompt, completion) pairs every `logging_steps` steps. If `rich` is "
            "installed, it prints the sample. If `wandb` logging is enabled, it logs it to `wandb`."
        },
    )
    num_completions_to_print: Optional[int] = field(
        default=None,
        metadata={
            "help": "Number of completions to print with `rich`. If `None`, all completions are logged."
        },
    )
    wandb_log_unique_prompts: Optional[bool] = field(
        default=False,
        metadata={
            "help": "Whether to log unique prompts in wandb. If `True`, only unique prompts are logged. If `False`, "
            "all prompts are logged."
        },
    )

    def __post_init__(self):
        super().__post_init__()

        num_processes = self.world_size
        # The current default effective batch size
        if (
            self.generation_batch_size is not None
            and self.steps_per_generation is not None
        ):
            raise ValueError(
                "'generation_batch_size' and 'steps_per_generation' can not be both configured at the same time"
            )

        if self.steps_per_generation is None:
            self.steps_per_generation = self.gradient_accumulation_steps

        if self.generation_batch_size is None:
            self.generation_batch_size = (
                self.per_device_train_batch_size
                * num_processes
                * self.steps_per_generation
            )

        if (
            self.generation_batch_size
            % self.per_device_train_batch_size
            * num_processes
            != 0
        ):
            raise ValueError(
                f"generation_batch_size ({self.generation_batch_size}) must be divisible by the global batch size "
                f"({self.per_device_train_batch_size * num_processes})."
            )

        self.steps_per_generation = self.generation_batch_size // (
            self.per_device_train_batch_size * num_processes
        )

        # Check if the effective batch size can be divided by the number of generations
        if self.num_generations < 2:
            raise ValueError(
                "GRPO requires at least 2 generations per prompt to calculate the advantages. You provided "
                f"{self.num_generations}, which is less than the minimum required."
            )
        possible_values = [
            n_gen
            for n_gen in range(2, self.generation_batch_size + 1)
            if (self.generation_batch_size) % n_gen == 0
        ]

        if self.num_generations not in possible_values:
            raise ValueError(
                f"The effective train batch size ({num_processes} x {self.per_device_train_batch_size} x "
                f"{self.steps_per_generation}) must be evenly divisible by the number of generations per "
                f"prompt ({self.num_generations}). Given the current effective train batch size, the valid values for "
                f"the number of generations are: {possible_values}."
            )
        if self.eval_strategy != "no":
            global_eval_batch_size = self.per_device_eval_batch_size * num_processes
            possible_values = [
                n_gen
                for n_gen in range(2, global_eval_batch_size + 1)
                if (global_eval_batch_size) % n_gen == 0
            ]
            if self.num_generations not in possible_values:
                raise ValueError(
                    f"The global eval batch size ({num_processes} x {self.per_device_eval_batch_size}) must be "
                    f"evenly divisible by the number of generations per prompt ({self.num_generations}). Given the "
                    "current global eval batch size, the valid values for the number of generations are: "
                    f"{possible_values}."
                )



================================================
FILE: verifiers/utils/__init__.py
================================================
[Empty file]


================================================
FILE: verifiers/utils/async_utils.py
================================================
import inspect
from typing import Callable


async def maybe_await(func: Callable, *args, **kwargs):
    result = func(*args, **kwargs)
    if inspect.isawaitable(result):
        return await result
    return result



================================================
FILE: verifiers/utils/data_utils.py
================================================
# NOTE: Helper functions for example datasets. Not intended for core functionality.

import random
from typing import Any, Callable

from datasets import Dataset, concatenate_datasets, load_dataset  # type: ignore

### PROMPTS ###

THINK_BOXED_SYSTEM_PROMPT = "Think step-by-step inside <think>...</think> tags. \
    Then, give your final answer inside \\boxed{}."

### https://github.com/huggingface/lighteval/blob/ecef2c662b9418866b6447d33b5e7d5dedd74af8/src/lighteval/tasks/default_prompts.py#L1474
BOXED_SYSTEM_PROMPT = (
    "Please reason step by step, and put your final answer within \\boxed{}."
)
###############


def extract_boxed_answer(text: str) -> str:
    def find_matching_brace(s: str, start: int) -> int:
        count = 1
        i = start
        while i < len(s) and count > 0:
            if s[i] == "{":
                count += 1
            elif s[i] == "}":
                count -= 1
            i += 1
        return i - 1 if count == 0 else -1

    # Find \boxed{
    boxed_start = text.find("\\boxed{")
    if boxed_start == -1:
        return text
    # Find the content between the braces
    content_start = boxed_start + 7  # len('\\boxed{')
    closing_brace = find_matching_brace(text, content_start)

    if closing_brace == -1:
        return text

    return text[content_start:closing_brace]


def strip_non_numeric(text: str) -> str:
    return "".join(c for c in text if c.isdigit() or c == ".")


def extract_hash_answer(text: str) -> str:
    if "####" not in text:
        return text
    return text.split("####")[1].strip()


def get_preprocess_fn(name: str) -> Callable[[dict], dict]:
    if name == "aime2024":

        def preprocess_aime2024(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["problem"],
                "answer": str(int(x["answer"])),
            }

        return preprocess_aime2024
    elif name == "aime2025":

        def preprocess_aime2025(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["question"],
                "answer": strip_non_numeric(x["answer"]),
            }

        return preprocess_aime2025
    elif name == "amc2023":

        def preprocess_amc2023(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["problem"],
                "answer": x["answer"],
            }

        return preprocess_amc2023
    elif name in ["gpqa_diamond", "gpqa_main"]:

        def preprocess_gpqa(x: dict[str, Any]) -> dict[str, Any]:
            q = x["Question"]
            letters = ["A", "B", "C", "D"]
            random.shuffle(letters)
            itos = {k: v for k, v in enumerate(letters)}
            ans = {
                itos[0]: x["Correct Answer"],
                itos[1]: x["Incorrect Answer 1"],
                itos[2]: x["Incorrect Answer 2"],
                itos[3]: x["Incorrect Answer 3"],
            }
            question = f"Question: {q}\n\n"
            question += f"A: {ans['A']}\n"
            question += f"B: {ans['B']}\n"
            question += f"C: {ans['C']}\n"
            question += f"D: {ans['D']}"

            return {
                "question": question,
                "answer": itos[0],
            }

        return preprocess_gpqa
    elif name == "gsm8k":

        def preprocess_gsm8k(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["question"],
                "answer": extract_hash_answer(x["answer"]),
            }

        return preprocess_gsm8k
    elif name == "math":

        def preprocess_math(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["problem"],
                "answer": extract_boxed_answer(x["solution"]),
            }

        return preprocess_math
    elif name == "math500":

        def preprocess_math500(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["problem"],
                "answer": x["answer"],
            }

        return preprocess_math500
    elif name == "mmlu":
        mmlu_map = ["A", "B", "C", "D"]

        def preprocess_mmlu(x: dict[str, Any]) -> dict[str, Any]:
            options = x["choices"]
            answer = x["answer"]
            question = f"Question: {x['question']}\n"
            for i, option in enumerate(options):
                question += f"\n{mmlu_map[i]}: {option}"
            return {
                "question": question,
                "temp_answer": mmlu_map[answer],
            }

        return preprocess_mmlu
    elif name == "mmlu_pro":
        mmlu_map = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J"]

        def preprocess_mmlu(x: dict[str, Any]) -> dict[str, Any]:
            options = x["options"]
            answer = x["answer"]
            question = f"Question: {x['question']}\n"
            for i, option in enumerate(options):
                question += f"\n{mmlu_map[i]}: {option}"
            return {
                "question": question,
                "answer": answer,
            }

        return preprocess_mmlu
    elif name == "openbookqa":

        def preprocess_openbookqa(x: dict[str, Any]) -> dict[str, Any]:
            choices_texts = x["choices"]["text"]
            choices_labels = x["choices"]["label"]

            formatted_choices = []
            for i in range(len(choices_labels)):
                formatted_choices.append(f"{choices_labels[i]}. {choices_texts[i]}")

            question = f"Question: {x['question_stem']}\n\nChoices:\n" + "\n".join(
                formatted_choices
            )
            return {
                "question": question,
                "answer": x["answerKey"],
            }

        return preprocess_openbookqa
    elif name in ["openrs", "openrs_easy", "openrs_hard"]:

        def preprocess_openrs(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["problem"],
                "answer": x["answer"],
            }

        return preprocess_openrs
    elif name == "prime_code":

        def preprocess_prime_code(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["prompt"],
                "answer": x["verification_info"],
            }

        return preprocess_prime_code
    else:
        raise ValueError(f"Dataset {name} not supported for preprocess_dataset.")


def load_example_dataset(
    name: str = "gsm8k", split: str | None = None, n: int | None = None, seed: int = 0
) -> Dataset:
    if name == "aime2024":
        if split is None:
            split = "train"
        dataset = load_dataset("HuggingFaceH4/aime_2024")[split]  # type: ignore
    elif name == "aime2025":
        if split is None:
            split = "test"
        aime_i = load_dataset("opencompass/AIME2025", "AIME2025-I")[split]  # type: ignore
        aime_ii = load_dataset("opencompass/AIME2025", "AIME2025-II")[split]  # type: ignore
        dataset = concatenate_datasets([aime_i, aime_ii])  # type: ignore
    elif name == "amc2023":
        if split is None:
            split = "train"
        dataset = load_dataset("knoveleng/AMC-23")[split]  # type: ignore
    elif name == "gpqa_diamond":
        if split is None:
            split = "train"
        dataset = load_dataset("Idavidrein/gpqa", "gpqa_diamond")[split]  # type: ignore
    elif name == "gpqa_main":
        if split is None:
            split = "train"
        dataset = load_dataset("Idavidrein/gpqa", "gpqa_main")[split]  # type: ignore
    elif name == "gsm8k":
        if split is None:
            split = "test"
        dataset: Dataset = load_dataset("openai/gsm8k", "main")[split]  # type: ignore
    elif name == "math":
        if split is None:
            split = "train"
        dataset: Dataset = load_dataset("chiayewken/competition_math")[split]  # type: ignore
    elif name == "math500":
        if split is None:
            split = "test"
        dataset: Dataset = load_dataset("HuggingFaceH4/MATH-500")[split]  # type: ignore
    elif name == "mmlu":
        if split is None:
            split = "dev"
        dataset = load_dataset("cais/mmlu", "all")[split]  # type: ignore
    elif name == "mmlu_pro":
        if split is None:
            split = "validation"
        dataset = load_dataset("TIGER-Lab/MMLU-Pro")[split]  # type: ignore
    elif name == "openbookqa":
        if split is None:
            split = "train"
        dataset: Dataset = load_dataset("allenai/openbookqa", "main")[split]  # type: ignore
    elif name == "openrs":
        if split is None:
            split = "train"
        dataset: Dataset = load_dataset("knoveleng/open-rs")[split]  # type: ignore
    elif name == "openrs_easy":
        if split is None:
            split = "train"
        dataset: Dataset = load_dataset("knoveleng/open-rs")[split]  # type: ignore
        dataset = dataset.filter(lambda x: x["level"] == "Easy")  # type: ignore
    elif name == "openrs_hard":
        if split is None:
            split = "train"
        dataset: Dataset = load_dataset("knoveleng/open-rs")[split]  # type: ignore
        dataset = dataset.filter(lambda x: x["level"] == "Hard")  # type: ignore
    elif name == "prime_code":
        if split is None:
            split = "train"
        dataset: Dataset = load_dataset("PrimeIntellect/verifiable-coding-problems")[
            split
        ]  # type: ignore
        dataset = dataset.filter(
            lambda x: x["prompt"].startswith(
                "Solve the following coding problem using the programming language python:"
            )
        )  # type: ignore
    else:
        raise ValueError(
            f"Dataset {name} not supported for preprocess_dataset. \
Please ensure that the dataset is formatted with 'prompt' (str) and 'answer' (str) keys."
        )

    preprocess_fn = get_preprocess_fn(name)
    if n is not None and n > 0:
        dataset = dataset.shuffle(seed=seed).select(range(n))  # type: ignore
    dataset = dataset.map(
        preprocess_fn, num_proc=10, remove_columns=dataset.column_names
    )  # type: ignore
    if "temp_answer" in dataset.column_names:
        dataset = dataset.rename_column("temp_answer", "answer")
    return dataset



================================================
FILE: verifiers/utils/env_utils.py
================================================
import importlib

from verifiers.envs.environment import Environment


def load_environment(env_id: str, **env_args) -> Environment:
    module_name = env_id.replace("-", "_")

    # check if installed already
    try:
        module = importlib.import_module(module_name)

        if not hasattr(module, "load_environment"):
            raise AttributeError(
                f"Module '{module_name}' does not have a 'load_environment' function"
            )

        return module.load_environment(**env_args)

    except ImportError as e:
        raise ValueError(
            f"Could not import '{env_id}' environment. Ensure the package for the '{env_id}' environment is installed."
        ) from e
    except Exception as e:
        raise RuntimeError(f"Failed to load environment '{env_id}': {str(e)}") from e



================================================
FILE: verifiers/utils/logging_utils.py
================================================
import json
import logging
import sys

from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

from verifiers.types import Messages


def setup_logging(
    level: str = "INFO",
    log_format: str | None = None,
    date_format: str | None = None,
) -> None:
    """
    Setup basic logging configuration for the verifiers package.

    Args:
        level: The logging level to use. Defaults to "INFO".
        log_format: Custom log format string. If None, uses default format.
        date_format: Custom date format string. If None, uses default format.
    """
    if log_format is None:
        log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    if date_format is None:
        date_format = "%Y-%m-%d %H:%M:%S"

    handler = logging.StreamHandler(sys.stderr)
    handler.setFormatter(logging.Formatter(fmt=log_format, datefmt=date_format))

    logger = logging.getLogger("verifiers")
    logger.setLevel(level.upper())
    logger.addHandler(handler)

    # Prevent the logger from propagating messages to the root logger
    logger.propagate = False


def print_prompt_completions_sample(
    prompts: list[Messages],
    completions: list[Messages],
    rewards: list[float],
    step: int,
    num_samples: int = 1,
) -> None:
    def _format_messages(messages) -> Text:
        if isinstance(messages, str):
            return Text(messages)
        out = Text()
        for idx, msg in enumerate(list(messages)):
            if idx > 0:
                out.append("\n\n")
            assert isinstance(msg, dict)
            role = msg.get("role", "")
            content = msg.get("content", "")
            style = "bright_cyan" if role == "assistant" else "bright_magenta"
            out.append(f"{role}: ", style="bold")
            out.append(content, style=style)
            if "tool_calls" in msg:
                for tool_call in msg["tool_calls"]:
                    name = getattr(tool_call.function, "name", "")
                    args = getattr(tool_call.function, "arguments", {})
                    tool_call_str = json.dumps({"name": name, "args": args}, indent=2)
                    out.append(f"\n\n[tool call]\n{tool_call_str}", style=style)
        return out

    console = Console()
    table = Table(show_header=True, header_style="bold white", expand=True)

    table.add_column("Prompt", style="bright_yellow")
    table.add_column("Completion", style="bright_green")
    table.add_column("Reward", style="bold cyan", justify="right")

    reward_values = rewards
    if len(reward_values) < len(prompts):
        reward_values = reward_values + [0.0] * (len(prompts) - len(reward_values))

    samples_to_show = min(num_samples, len(prompts))
    for i in range(samples_to_show):
        prompt = list(prompts)[i]
        completion = list(completions)[i]
        reward = reward_values[i]

        formatted_prompt = _format_messages(prompt)
        formatted_completion = _format_messages(completion)

        table.add_row(formatted_prompt, formatted_completion, Text(f"{reward:.2f}"))
        if i < samples_to_show - 1:
            table.add_section()

    panel = Panel(table, expand=False, title=f"Step {step}", border_style="bold white")
    console.print(panel)



================================================
FILE: verifiers/utils/message_utils.py
================================================
import json
from typing import cast

from verifiers.types import ChatMessage, Messages


def message_to_printable(message: ChatMessage) -> ChatMessage:
    """
    Removes image_url objects from message content.
    """
    new_message = {}
    new_message["role"] = message["role"]
    new_message["content"] = []
    if "tool_calls" in message:
        new_message["tool_calls"] = message["tool_calls"]
    content = message.get("content")
    if content is None:
        return cast(ChatMessage, new_message)
    if isinstance(content, str):
        new_message["content"].append(content)
    else:
        for c in content:
            if isinstance(c, str):
                new_message["content"].append(c)
            else:
                c_dict = dict(c)
                if c_dict["type"] == "text":
                    new_message["content"].append(c_dict["text"])
                elif c_dict["type"] == "image_url":
                    new_message["content"].append("[image]")
    new_message["content"] = "\n\n".join(new_message["content"])
    return cast(ChatMessage, new_message)


def messages_to_printable(messages: Messages) -> Messages:
    """
    Removes image_url objects from messages.
    """
    if isinstance(messages, str):
        return messages
    return [message_to_printable(m) for m in messages]


def cleanup_message(message: ChatMessage) -> ChatMessage:
    new_message = {}
    new_message["role"] = message["role"]
    if "tool_calls" in message:
        new_message["tool_calls"] = message["tool_calls"]
    new_message["content"] = []
    content = message.get("content")
    if content is None:
        return cast(ChatMessage, new_message)
    if isinstance(content, str):
        new_message["content"] = content
    else:
        for c in content:
            new_c = c.copy()
            c_dict = dict(c)
            if "image_url" in c_dict and "type" in c_dict and c_dict["type"] == "text":
                new_c.pop("image_url")
                new_message["content"].append(new_c)
            elif (
                "image_url" in c_dict
                and "type" in c_dict
                and c_dict["type"] == "image_url"
            ):
                new_c.pop("text")
                new_message["content"].append(new_c)
            else:
                new_message["content"].append(new_c)
    return cast(ChatMessage, new_message)


def cleanup_messages(messages: Messages) -> Messages:
    if isinstance(messages, str):
        return messages
    new_messages = []
    for m in messages:
        new_messages.append(cleanup_message(m))
    return new_messages


def sanitize_tool_calls(messages: Messages):
    """
    Sanitize tool calls from messages.
    """
    if not isinstance(messages, list):
        return messages
    sanitized_messages = []
    for m in messages:
        if "tool_calls" in m:
            new_m = {
                "role": m["role"],
                "content": m.get("content", ""),
                "tool_calls": [
                    json.dumps(tc.model_dump())  # type: ignore
                    for tc in m.get("tool_calls", [])
                ],
            }
            sanitized_messages.append(new_m)
        else:
            sanitized_messages.append(m)
    return sanitized_messages



================================================
FILE: verifiers/utils/model_utils.py
================================================
from importlib.util import find_spec
from typing import Any, Callable

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer  # type: ignore


class _ForwardRedirection:
    """Implements the `forward-redirection`.

    Taken from Pytorch-lightning: https://github.com/Lightning-AI/pytorch-lightning/blob/02311d03fb982560246eead7c08104481fac9579/src/lightning/pytorch/strategies/strategy.py#L602

    A method call to a wrapped module gets rerouted through the wrapper's `forward` method instead.

    """

    def __call__(
        self,
        wrapper_module: nn.Module,
        original_module: nn.Module,
        method: Callable,
        *args: Any,
        **kwargs: Any,
    ) -> Any:
        """Reroutes a method call through the `wrapper_module`'s `forward` method.

        Args:
            wrapper_module: The module that has `original_module` wrapped.
            original_module: The module that was wrapped inside `wrapper_module`.
            method_name: The name of the method that should be called on the `original_module` after inputs get
                redirected through the `wrapper_module`'s `forward` method.
            *args: The positional arguments to the method `method_name`. They will get passed to a patched
                `forward` method instead.
            **kwargs: The keyword arguments to the method `method_name`. They will get passed to a patched
                `forward` method instead.

        """
        original_forward = original_module.forward

        def wrapped_forward(*_args: Any, **_kwargs: Any) -> Any:
            # Unpatch ourselves immediately before calling the method `method_name`
            # because itself may want to call the real `forward`
            original_module.forward = original_forward  # type: ignore[method-assign]
            # Call the actual method e.g. `.training_step(...)`
            out = method(*_args, **_kwargs)
            self.on_after_inner_forward(wrapper_module, original_module)
            return out

        # Patch the original_module's forward so we can redirect the arguments back to the real method
        original_module.forward = wrapped_forward  # type: ignore[method-assign]

        wrapper_output = wrapper_module(*args, **kwargs)
        self.on_after_outer_forward(wrapper_module, original_module)
        return wrapper_output

    def on_after_inner_forward(
        self, wrapper_module: nn.Module, original_module: nn.Module
    ) -> None:
        pass

    def on_after_outer_forward(
        self, wrapper_module: nn.Module, original_module: nn.Module
    ) -> None:
        pass


def is_liger_available() -> bool:
    return find_spec("liger_kernel") is not None


def get_model(
    model_name: str,
    use_liger: bool = True,
    model_kwargs: dict[str, Any] | None = None,
) -> Any:
    if model_kwargs is None:
        model_kwargs = dict(
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
            use_cache=False,
        )
    if is_liger_available() and use_liger:
        print("Using Liger kernel")
        from liger_kernel.transformers import AutoLigerKernelForCausalLM  # type: ignore

        return AutoLigerKernelForCausalLM.from_pretrained(model_name, **model_kwargs)
    else:
        return AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)


def get_tokenizer(model_name: str) -> Any:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if not hasattr(tokenizer, "chat_template"):
        raise ValueError(
            f"Tokenizer for model {model_name} does not have chat_template attribute, \
                            and could not find a tokenizer with the same name as the model with suffix \
                            '-Instruct'. Please provide a tokenizer with the chat_template attribute."
        )
    return tokenizer


def get_model_and_tokenizer(
    model_name: str, use_liger: bool = True, model_kwargs: dict[str, Any] | None = None
) -> tuple[Any, Any]:
    model = get_model(model_name, use_liger, model_kwargs)
    tokenizer = get_tokenizer(model_name)
    return model, tokenizer



================================================
FILE: verifiers/utils/report_utils.py
================================================
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from hashlib import sha1
from importlib import metadata as importlib_metadata
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np
from jinja2 import BaseLoader, Environment, StrictUndefined, select_autoescape

from verifiers.types import GenerateOutputs, Messages

# Hard cap on the number of fully rendered examples in the HTML report.
DETAILED_EXAMPLES_CAP: int = 50


@dataclass
class ReportMeta:
    env_id: str
    env_version: str
    model: str
    num_examples: int
    rollouts_per_example: int
    api_base_url: str
    sampling_args: Dict[str, Any]
    env_args: Dict[str, Any]


def get_env_version(module_name: str) -> str:
    """Return installed package version for the environment module.

    Falls back to "0.0.0" if not installed as a package.
    """
    try:
        return importlib_metadata.version(module_name)
    except importlib_metadata.PackageNotFoundError:
        return "0.0.0"


def _safe_last_assistant_text(messages: Messages) -> str:
    """Extract the last assistant message content if present; otherwise a short placeholder.

    Messages may be a list of dicts in chat format. Return a trimmed snippet.
    """
    try:
        if isinstance(messages, list) and messages:
            # iterate backwards to find last assistant
            for msg in reversed(messages):
                if isinstance(msg, dict) and msg.get("role") == "assistant":
                    content = msg.get("content", "") or ""
                    return _trim_snippet(str(content))
        return ""
    except Exception:
        return ""


def _trim_snippet(text: str, max_chars: int = 300) -> str:
    if len(text) <= max_chars:
        return text
    return text[: max_chars - 1] + "…"


def _compute_basic_stats(values: List[float]) -> Dict[str, float]:
    arr = np.array(values, dtype=float)
    if arr.size == 0:
        return {"mean": float("nan"), "std": float("nan"), "n": 0}
    return {
        "mean": float(np.mean(arr)),
        "std": float(np.std(arr)),
        "n": int(arr.size),
    }


def _compute_percentiles(
    values: List[float], percentiles: Tuple[int, ...] = (5, 25, 50, 75, 95)
) -> Dict[str, float]:
    arr = np.array(values, dtype=float)
    if arr.size == 0:
        return {f"p{p}": float("nan") for p in percentiles}
    qs = np.percentile(arr, percentiles)
    return {f"p{p}": float(q) for p, q in zip(percentiles, qs)}


def compute_summary(results: GenerateOutputs) -> Dict[str, Any]:
    """Compute aggregated statistics from GenerateOutputs in a format usable by templates.

    This function intentionally does not change layout based on dataset size.
    """
    summary: Dict[str, Any] = {}

    reward_stats = _compute_basic_stats(results.reward)
    reward_percentiles = _compute_percentiles(results.reward)
    summary["reward"] = {**reward_stats, **reward_percentiles}

    metric_summaries: Dict[str, Dict[str, float]] = {}
    for metric_name, metric_values in results.metrics.items():
        metric_summaries[metric_name] = {
            **_compute_basic_stats(metric_values),
            **_compute_percentiles(metric_values),
        }
    summary["metrics"] = metric_summaries

    return summary


def build_examples(
    results: GenerateOutputs, cap: int = DETAILED_EXAMPLES_CAP
) -> List[Dict[str, Any]]:
    """Prepare a capped list of example rows for rendering.

    Each row contains: index, reward, optional first metric column, and a completion snippet.
    """
    num = min(len(results.reward), cap)
    metric_first_name = next(iter(results.metrics.keys()), None)
    rows: List[Dict[str, Any]] = []
    for i in range(num):
        completion_snippet = _safe_last_assistant_text(results.completion[i])
        row = {
            "index": i,
            "reward": float(results.reward[i]),
            "metric_name": metric_first_name,
            "metric_value": float(results.metrics[metric_first_name][i])
            if metric_first_name
            else None,
            "completion": completion_snippet,
        }
        rows.append(row)
    return rows


def _hash_env_args(env_args: Dict[str, Any]) -> str:
    if not env_args:
        return "noargs"
    # Stable JSON-like representation for hashing
    try:
        import json

        normalized = json.dumps(env_args, sort_keys=True, separators=(",", ":"))
    except Exception:
        normalized = str(sorted(env_args.items()))
    return sha1(normalized.encode("utf-8")).hexdigest()[:8]


def build_report_filename(meta: ReportMeta) -> str:
    """Construct a deterministic report file name without timestamps.

    Pattern: {env_id}--v{env_version}--model={model}--n={n}--r={r}--args={hash}.html
    """
    args_hash = _hash_env_args(meta.env_args)
    safe_model = meta.model.replace("/", "--")
    return (
        f"{meta.env_id}--v{meta.env_version}--model={safe_model}"
        f"--n={meta.num_examples}--r={meta.rollouts_per_example}--args={args_hash}.html"
    )


_TEMPLATE = """<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <style>
      body { font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, Helvetica, Arial, sans-serif; margin: 24px; }
      h1, h2, h3 { margin: 0.4em 0; }
      table { border-collapse: collapse; width: 100%; margin: 8px 0 16px; }
      th, td { border: 1px solid #ddd; padding: 8px; font-size: 14px; text-align: left; }
      th { background: #f6f8fa; }
      .meta { margin-bottom: 16px; color: #444; }
      .code { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; background: #f6f8fa; padding: 2px 6px; border-radius: 4px; }
      .examples pre { white-space: pre-wrap; }
      .muted { color: #777; }
    </style>
  </head>
  <body>
    <h3>{{ env_id }}: {{ model }} (n={{ num_examples }}, r={{ rollouts_per_example }})</h3>
    <div class="meta">
      <div><b>Environment</b>: {{ env_id }} (v{{ env_version }})</div>
      <div><b>Model</b>: <span class="code">{{ model }}</span></div>
      <div><b>Provider</b>: {{ api_base_url }}</div>
      <div><b>Samples</b>: n={{ num_examples }}, r={{ rollouts_per_example }}</div>
      <div><b>Date</b>: {{ date }}</div>
      <div><b>Time</b>: {{ time }}</div>
      <div><b>Sampling</b>: max_tokens={{ sampling_args.max_tokens }}, temperature={{ sampling_args.temperature }}</div>
    </div>

    <h2>Reward</h2>
    <table>
      <tr><th>mean</th><th>std</th><th>n</th><th>p5</th><th>p25</th><th>p50</th><th>p75</th><th>p95</th></tr>
      <tr>
        <td>{{ reward.mean | round(4) }}</td>
        <td>{{ reward.std | round(4) }}</td>
        <td>{{ reward.n }}</td>
        <td>{{ reward.p5 | round(4) }}</td>
        <td>{{ reward.p25 | round(4) }}</td>
        <td>{{ reward.p50 | round(4) }}</td>
        <td>{{ reward.p75 | round(4) }}</td>
        <td>{{ reward.p95 | round(4) }}</td>
      </tr>
    </table>

    {% if metrics %}
    <h2>Metrics</h2>
    <table>
      <tr>
        <th>metric</th><th>mean</th><th>std</th><th>n</th><th>p5</th><th>p25</th><th>p50</th><th>p75</th><th>p95</th>
      </tr>
      {% for name, m in metrics.items() %}
      <tr>
        <td>{{ name }}</td>
        <td>{{ m.mean | round(4) }}</td>
        <td>{{ m.std | round(4) }}</td>
        <td>{{ m.n }}</td>
        <td>{{ m.p5 | round(4) }}</td>
        <td>{{ m.p25 | round(4) }}</td>
        <td>{{ m.p50 | round(4) }}</td>
        <td>{{ m.p75 | round(4) }}</td>
        <td>{{ m.p95 | round(4) }}</td>
      </tr>
      {% endfor %}
    </table>
    {% endif %}

    <h2>Examples <span class="muted">(showing up to {{ examples|length }} of {{ total_examples }})</span></h2>
    <div class="examples">
      <table>
        <tr><th>#</th><th>reward</th><th>{% if examples and examples[0].metric_name %}{{ examples[0].metric_name }}{% else %}metric{% endif %}</th><th>completion</th></tr>
        {% for ex in examples %}
        <tr>
          <td>{{ ex.index }}</td>
          <td>{{ ex.reward | round(4) }}</td>
          <td>{% if ex.metric_value is not none %}{{ ex.metric_value | round(4) }}{% else %}-{% endif %}</td>
          <td><pre>{{ ex.completion }}</pre></td>
        </tr>
        {% endfor %}
      </table>
    </div>
  </body>
 </html>
"""


_env = Environment(
    loader=BaseLoader(),
    undefined=StrictUndefined,
    autoescape=select_autoescape(["html", "xml"]),
)


def render_html(
    meta: ReportMeta,
    summary: Dict[str, Any],
    examples: List[Dict[str, Any]],
    total_examples: int,
) -> str:
    template = _env.from_string(_TEMPLATE)
    return template.render(
        env_id=meta.env_id,
        env_version=meta.env_version,
        model=meta.model,
        api_base_url=meta.api_base_url,
        num_examples=meta.num_examples,
        rollouts_per_example=meta.rollouts_per_example,
        date=datetime.now().strftime("%Y-%m-%d"),
        time=datetime.now().strftime("%H:%M:%S"),
        sampling_args={
            "max_tokens": meta.sampling_args.get("max_tokens"),
            "temperature": meta.sampling_args.get("temperature"),
        },
        reward=summary.get("reward", {}),
        metrics=summary.get("metrics", {}),
        examples=examples,
        total_examples=total_examples,
    )


def write_html_report(
    report_dir: Path,
    meta: ReportMeta,
    results: GenerateOutputs,
) -> Path:
    """Render and write the HTML report next to the environment under `reports/`.

    Returns the path to the written HTML file.
    """
    report_dir.mkdir(parents=True, exist_ok=True)

    summary = compute_summary(results)
    examples = build_examples(results, cap=DETAILED_EXAMPLES_CAP)
    html = render_html(
        meta=meta,
        summary=summary,
        examples=examples,
        total_examples=len(results.reward),
    )
    filename = build_report_filename(meta)
    out_path = report_dir / filename
    out_path.write_text(html, encoding="utf-8")
    return out_path



================================================
FILE: verifiers/utils/tool_utils.py
================================================
from __future__ import annotations

import inspect
import re
from typing import Any, Literal, Union, get_args, get_origin

from verifiers.types import (
    ChatCompletionToolParam,
    FunctionParameters,
    JsonPrimitive,
)

_JSON_PRIMITIVE_MAP: dict[type, JsonPrimitive] = {
    str: "string",
    int: "integer",
    float: "number",
    bool: "boolean",
    list: "array",
    dict: "object",
}


def _get_json_type(annotation: Any) -> tuple[JsonPrimitive, list[Any] | None]:
    """Return the JSON Schema type name and optional enum values for *annotation*.

    The second element is a list of literal values if *annotation* is a typing.Literal.
    """
    origin = get_origin(annotation)

    if origin is Literal:
        # Treat Literal values as strings/numbers depending on their Python type.
        literal_values = list(get_args(annotation))
        if not literal_values:
            return "string", None
        first_value = literal_values[0]
        json_type = _JSON_PRIMITIVE_MAP.get(type(first_value), "string")
        return json_type, literal_values

    if origin is Union:
        # If Optional[T] or Union[T, None], return type of T.
        args = [a for a in get_args(annotation) if a is not type(None)]  # noqa: E721, we really need NoneType
        if len(args) == 1:
            return _get_json_type(args[0])

    # Normal (non-parameterised) annotation
    json_type = _JSON_PRIMITIVE_MAP.get(annotation, "string")
    return json_type, None


_PARAM_RE = re.compile(r"^\s*(\w+)\s*\(([^)]*)\):\s*(.*)$")


def _parse_docstring(func: Any) -> tuple[str, dict[str, str]]:
    """Extract the short description and parameter descriptions from *func*'s docstring.

    Returns
    -------
    (summary, param_descriptions)
    """
    doc = inspect.getdoc(func) or ""
    if not doc:
        return "", {}

    lines = doc.splitlines()
    # First non-empty line is the summary.
    summary = next((line.strip() for line in lines if line.strip()), "")

    param_descs: dict[str, str] = {}
    # Try to locate an "Args:" or "Parameters:" block.
    try:
        block_idx = next(
            i
            for i, line in enumerate(lines)
            if line.strip().lower() in {"args:", "arguments:", "parameters:"}
        )
    except StopIteration:
        return summary, param_descs

    for raw in lines[block_idx + 1 :]:
        if not raw.strip():
            # Stop once we hit a blank line, assuming end of args block.
            break
        match = _PARAM_RE.match(raw)
        if match:
            name, _type, desc = match.groups()
            param_descs[name] = desc.strip()
        else:
            # Continuation lines – append to last parameter if any.
            if param_descs and raw.startswith(" " * 4):
                last_key = next(reversed(param_descs))
                param_descs[last_key] += " " + raw.strip()
            else:
                break  # End of recognised param section
    return summary, param_descs


def _is_required(annotation: Any) -> bool:
    """True if *annotation* is not Optional/Union[..., None]."""
    origin = get_origin(annotation)
    if origin is Union:
        return type(None) not in get_args(annotation)
    return True


def convert_func_to_oai_tool(func: Any) -> ChatCompletionToolParam:
    """Convert *func* to an OpenAI function-calling tool schema.

    The returned mapping matches the structure expected in the `tools` list
    of the OpenAI ChatCompletion API.
    """
    if not callable(func):
        raise TypeError("Expected a callable object")

    signature = inspect.signature(func)
    summary, param_descs = _parse_docstring(func)

    if not summary:
        summary = f"Auto-generated description for `{func.__name__}`."  # basic fallback

    # Resolve postponed annotations so we properly interpret Literal and others
    try:
        resolved_hints = inspect.get_annotations(func, eval_str=True)
    except AttributeError:  # Fallback for older Python versions
        from typing import get_type_hints

        resolved_hints = get_type_hints(func)

    properties: dict[str, Any] = {}
    required: list[str] = []

    for name, param in signature.parameters.items():
        if name == "self":
            continue  # Ignore instance methods' self parameter

        annotation = resolved_hints.get(
            name,
            param.annotation
            if param.annotation is not inspect.Parameter.empty
            else str,
        )
        json_type, enum_vals = _get_json_type(annotation)

        prop_schema: dict[str, Any] = {
            "type": json_type,
        }
        if enum_vals is not None:
            prop_schema["enum"] = enum_vals

        # Description: prefer docstring info, else fallback to generic text.
        if name in param_descs:
            prop_schema["description"] = param_descs[name]
        else:
            prop_schema.setdefault(
                "description",
                f"Parameter `{name}` of type {json_type}.",
            )

        properties[name] = prop_schema

        # Consider parameter required if no default or default is inspect._empty
        if param.default is inspect.Parameter.empty and _is_required(annotation):
            required.append(name)

    parameters_schema: FunctionParameters = {
        "type": "object",
        "properties": properties,
    }
    if required:
        parameters_schema["required"] = required

    return {
        "type": "function",
        "function": {
            "name": func.__name__,
            "description": summary,
            "parameters": parameters_schema,
        },
    }



================================================
FILE: verifiers/utils/tools.py
================================================
# tool examples, not actively maintained or intended for extension
# includes: ask, search, calculator, python

import ast
import os
import subprocess
import textwrap


def _get_url_markdown(url: str) -> str:
    """Get contents of URL as nicely formatted markdown."""
    import requests

    try:
        from markdownify import markdownify as md  # type: ignore

        response = requests.get(url, timeout=30)
        response.raise_for_status()
        return md(response.text)
    except Exception as e:
        return f"Error: {str(e)}"


def ask(question: str, url: str) -> str:
    """Ask a question about a web page returned from search results.

    Args:
        question: The question to be answered (by an LLM who will be given the web page contents)
        url: The URL of the web page to query

    Returns:
        A LLM-generated answer to the question based on the web page contents.

    Examples:
        {"question": "What is the capital of France?", "url": "https://en.wikipedia.org/wiki/France"} -> "The capital of France is Paris."
        {"question": "How many people live in the United States?", "url": "https://en.wikipedia.org/wiki/United_States"} -> "The population of the United States is approximately 340 million people."
    """
    BASE_URL = "https://api.deepinfra.com/v1/openai"
    API_KEY = os.getenv("DEEPINFRA_API_KEY")
    MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"

    contents = _get_url_markdown(url)[:50000]

    if contents.startswith("Error:"):
        return "Error: Failed to fetch URL contents."

    from openai import OpenAI

    client = OpenAI(base_url=BASE_URL, api_key=API_KEY)

    prompt = f"""Answer the following question based on the provided web page contents:

    Question: {question}

    Page: {url}

    Page contents:
    {contents}
    """

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=4000,
        )
        return response.choices[0].message.content or "Error: No response from model."
    except Exception as e:
        return f"Error: {str(e)}"


def calculator(expression: str) -> str:
    """Evaluates a single line of Python math expression. No imports or variables allowed.

    Args:
        expression (str): A mathematical expression using only numbers and basic operators (+,-,*,/,**,())

    Returns:
        The result of the calculation or an error message

    Examples:
        "2 + 2" -> "4"
        "3 * (17 + 4)" -> "63"
        "100 / 5" -> "20.0"
    """
    allowed = set("0123456789+-*/.() ")
    if not all(c in allowed for c in expression):
        return "Error: Invalid characters in expression"

    try:
        result = eval(expression, {"__builtins__": {}}, {})
        return str(result)
    except Exception as e:
        return f"Error: {str(e)}"


def _jupyterize(src: str) -> str:
    src = textwrap.dedent(src)
    tree = ast.parse(src, mode="exec")

    if tree.body and isinstance(tree.body[-1], ast.Expr):
        # Extract the last expression
        last = tree.body.pop()
        body_code = ast.unparse(ast.Module(tree.body, []))
        expr_code = ast.unparse(last.value)  # type: ignore
        # Display the last expression value like Jupyter does
        return f"{body_code}\n_ = {expr_code}\nif _ is not None: print(_)"
    else:
        return src


def python(code: str) -> str:
    """Evaluates Python code like a Jupyter notebook cell, returning output and/or the last expression value.

    Args:
        code (str): A block of Python code

    Returns:
        The output (stdout + last expression if not None) or error message

    Examples:
        {"code": "import numpy as np\nnp.array([1, 2, 3]) + np.array([4, 5, 6])"} -> "[5 7 9]"
        {"code": "x = 5\ny = 10\nx + y"} -> "15"
        {"code": "a, b = 3, 4\na, b"} -> "(3, 4)"
    """

    try:
        # Run the code block in subprocess with 10-second timeout
        result = subprocess.run(
            ["python", "-c", _jupyterize(code)],
            timeout=10,
            text=True,
            capture_output=True,
        )

        output = result.stdout.strip()[:1000]
        error = result.stderr.strip()[:1000]
        if error:
            return error
        return output
    except subprocess.TimeoutExpired:
        return "Error: Code execution timed out after 10 seconds"


def search_ddg(query: str, num_results: int = 5) -> str:
    """Searches DuckDuckGo and returns concise summaries of top results.

    Args:
        query (str): The search query string
        num_results (int): Number of results to return (default: 5, max: 10)

    Returns:
        Formatted string with bullet points of top results, each with title and brief summary

    Examples:
        {"query": "who invented the lightbulb", "num_results": 3}
    """

    try:
        from duckduckgo_search import DDGS  # type: ignore

        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=min(num_results, 10)))
            if not results:
                return "No results found"

            summaries = []
            for r in results:
                title = r["title"]
                snippet = r["body"][:200].rsplit(".", 1)[0] + "."
                summaries.append(f"• {title}\n  {snippet}")

            return "\n\n".join(summaries)
    except Exception as e:
        return f"Error: {str(e)}"


def search(query: str) -> str:
    """Searches the web and returns summaries of top results.

    Args:
        query: The search query string

    Returns:
        Formatted string with bullet points of top 3 results, each with title, source, url, and brief summary

    Examples:
        {"query": "who invented the lightbulb"} -> ["Thomas Edison (1847-1931) - Inventor of the lightbulb", ...]
        {"query": "what is the capital of France"} -> ["Paris is the capital of France", ...]
        {"query": "when was the Declaration of Independence signed"} -> ["The Declaration of Independence was signed on July 4, 1776", ...]
    """
    try:
        from brave import Brave  # type: ignore

        brave = Brave()
        results = brave.search(q=query, count=10, raw=True)  # type: ignore
        web_results = results.get("web", {}).get("results", [])  # type: ignore

        if not web_results:
            return "No results found"

        summaries = []
        for r in web_results:
            if "profile" not in r:
                continue
            header = f"{r['profile']['name']} ({r['profile']['long_name']})"
            title = r["title"]
            snippet = r["description"][:300] + " ..."
            url = r["url"]
            summaries.append(f"•  {header}\n   {title}\n   {snippet}\n   {url}")

        return "\n\n".join(summaries[:3])
    except Exception as e:
        return f"Error: {str(e)}"



================================================
FILE: .github/pull_request_template.md
================================================
## Description
<!-- Provide a brief description of the changes in this PR -->

## Type of Change
<!-- Mark the relevant option with an "x" -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] Documentation update
- [ ] Test improvement

## Testing
<!-- Describe the tests you ran to verify your changes -->
- [ ] All existing tests pass
- [ ] New tests have been added to cover the changes
- [ ] Tests have been run locally with `python -m pytest tests/`

### Test Coverage
<!-- If applicable, mention the test coverage for new code -->
- Current coverage: ___%
- Coverage after changes: ___%

## Checklist
- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] Any dependent changes have been merged and published

## Additional Notes
<!-- Add any additional notes, screenshots, or context about the PR here -->


================================================
FILE: .github/workflows/README.md
================================================
# GitHub Actions Workflows

This directory contains automated workflows for the verifiers project.

## Workflows

### 1. Style (`style.yaml`)
**Purpose**: Code style checking using ruff.

**Triggers**:
- Pull requests (opened, synchronized, reopened)
- Pushes to `main` branch

**What it does**:
- Runs ruff for linting and formatting checks
- Uses configuration from `pyproject.toml`

### 2. Test (`test.yml`)
**Purpose**: Comprehensive testing with coverage reports.

**Triggers**:
- Pull requests affecting Python files, dependencies, or workflow files
- Pushes to `main`, `master`, or `develop` branches with the same file changes

**What it does**:
- Runs tests on multiple Python versions (3.11, 3.12)
- Generates coverage reports (XML, HTML, and terminal output)
- Uploads coverage to Codecov (requires `CODECOV_TOKEN` secret)
- Uploads HTML coverage reports as artifacts
- Comments on PRs with test results

## Setting Up

### Branch Protection
It's recommended to set up branch protection rules for your main branch:
1. Go to Settings → Branches
2. Add a rule for your main branch
3. Enable "Require status checks to pass before merging"
4. Select the CI jobs you want to require

## Running Tests Locally

To run tests locally the same way they run in CI:

```bash
# Install dependencies
pip install -e .
pip install pytest pytest-asyncio pytest-cov

# Run tests
python -m pytest tests/ -v

# Run tests with coverage
python -m pytest tests/ -v --cov=verifiers --cov-report=html
```

## Customization

### Adding New Python Versions
Edit the `matrix.python-version` in the workflow files to test on additional Python versions.

### Changing Trigger Conditions
Modify the `on:` section in the workflow files to change when workflows run.

### Adding More Checks
You can extend the workflows to include:
- Type checking with mypy
- Security scanning
- Documentation building
- Package building and publishing


================================================
FILE: .github/workflows/badge.md
================================================
# Status Badges

Add these badges to your main README.md to show the build status:

## For GitHub Actions workflows:

```markdown
![CI](https://github.com/YOUR_USERNAME/YOUR_REPO/workflows/CI/badge.svg)
![Tests](https://github.com/YOUR_USERNAME/YOUR_REPO/workflows/Run%20Tests/badge.svg)
```

## With links to the workflows:

```markdown
[![CI](https://github.com/YOUR_USERNAME/YOUR_REPO/workflows/CI/badge.svg)](https://github.com/YOUR_USERNAME/YOUR_REPO/actions/workflows/ci.yml)
[![Tests](https://github.com/YOUR_USERNAME/YOUR_REPO/workflows/Run%20Tests/badge.svg)](https://github.com/YOUR_USERNAME/YOUR_REPO/actions/workflows/test.yml)
```

## For specific branches:

```markdown
![CI](https://github.com/YOUR_USERNAME/YOUR_REPO/workflows/CI/badge.svg?branch=main)
```

## Combined status badge:

```markdown
[![CI Status](https://github.com/YOUR_USERNAME/YOUR_REPO/actions/workflows/ci.yml/badge.svg)](https://github.com/YOUR_USERNAME/YOUR_REPO/actions/workflows/ci.yml)
[![Coverage Status](https://codecov.io/gh/YOUR_USERNAME/YOUR_REPO/branch/main/graph/badge.svg)](https://codecov.io/gh/YOUR_USERNAME/YOUR_REPO)
```

Remember to replace `YOUR_USERNAME` and `YOUR_REPO` with your actual GitHub username and repository name.


================================================
FILE: .github/workflows/style.yaml
================================================
name: Style

on:
  push:
    branches:
      - main
  pull_request:
    # This will trigger the workflow for pull requests to any branch
    types: [opened, synchronize, reopened]

jobs:
  ruff:
    name: Ruff
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: true
      - name: Run ruff
        uses: astral-sh/ruff-action@v3


================================================
FILE: .github/workflows/test.yml
================================================
name: Test

on:
  pull_request:
    branches: [ main, master ]
    paths:
      - '**.py'
      - 'pyproject.toml'
      - '.github/workflows/test.yml'
  push:
    branches: [ main, master ]
    paths:
      - '**.py'
      - 'pyproject.toml'
      - '.github/workflows/test.yml'

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.11", "3.12"]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip packages
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-asyncio pytest-cov nest-asyncio
    
    - name: Run tests
      run: |
        python -m pytest tests/ -v --cov=verifiers --cov-report=xml --cov-report=term

