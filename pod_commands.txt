# Shop-R1 Testing Commands for Prime Intellect Pod
# Copy and run these commands after SSH'ing into the pod

# SSH into pod first:
# ssh -p 1234 root@62.169.159.61

# ============================================
# PART 1: Setup and Installation
# ============================================

cd /ephemeral || cd /workspace || cd ~
rm -rf shop-r1-test
git clone https://github.com/christianchartier/shop-r1.git shop-r1-test
cd shop-r1-test

# Create Python environment (try 3.11 first, fallback to 3.10/3.9)
python3.11 -m venv .venv311 || python3.10 -m venv .venv311 || python3.9 -m venv .venv311
source .venv311/bin/activate
python --version

# Install core dependencies
python -m pip install -U pip setuptools wheel
python -m pip uninstall -y torchvision
python -m pip install "transformers>=4.55,<5" "trl==0.21.0" "vllm==0.10.1.1"
python -m pip install accelerate>=0.30 peft>=0.11 datasets>=2.19 requests openai
python -m pip install 'verifiers @ git+https://github.com/willccbb/verifiers@main'

# Install shop-r1
python -m pip install -e .
vf-install shop-r1

# ============================================
# PART 2: Verify Installation
# ============================================

# Test that environment loads
python -c "import verifiers as vf; env = vf.load_environment('shop-r1'); print('âœ“ Environment loaded')"

# Check GPUs
nvidia-smi -L

# ============================================
# PART 3: Create Test Data
# ============================================

mkdir -p data
cat > data/test.jsonl << 'EOF'
{"prompt": [{"role": "user", "content": "Search for laptop"}], "answer": {"type": "type_and_submit", "name": "search", "text": "laptop"}, "rationale": "Looking for a laptop"}
{"prompt": [{"role": "user", "content": "Click add to cart"}], "answer": {"type": "click", "name": "add_to_cart"}, "rationale": "Adding to cart"}
{"prompt": [{"role": "user", "content": "Done shopping"}], "answer": {"type": "terminate"}, "rationale": "Finished"}
EOF

# ============================================
# PART 4: Test SFT Training (Quick)
# ============================================

python scripts/sft_train.py \
  --dataset data/test.jsonl \
  --model Qwen/Qwen2.5-0.5B-Instruct \
  --output_dir checkpoints/test_sft \
  --epochs 1 \
  --save_steps 2 \
  --logging_steps 1 \
  --per_device_batch_size 1 \
  --grad_accum 1 \
  --max_seq_len 1024 \
  --lr 2e-5

# Check if it worked
ls -la checkpoints/test_sft/

# ============================================
# PART 5: Test GRPO (If 2+ GPUs)
# ============================================

# Check GPU count
NUM_GPUS=$(nvidia-smi -L | wc -l)
echo "Found $NUM_GPUS GPUs"

# If you have 2+ GPUs, run these in separate terminals:

# Terminal 1 (tmux or screen):
tmux new -s vllm
# Inside tmux:
cd /ephemeral/shop-r1-test && source .venv311/bin/activate
CUDA_VISIBLE_DEVICES=0 python -m trl.scripts.vllm_serve \
  --model Qwen/Qwen2.5-0.5B-Instruct \
  --host 0.0.0.0 --port 8000 \
  --max-model-len 1024 \
  --gpu-memory-utilization 0.20

# Detach from tmux: Ctrl+B, then D

# Terminal 2 (main):
# Wait 15 seconds for server to start
sleep 15

# Test endpoints
curl -L -s -o /dev/null -w "World Size: %{http_code}\n" http://localhost:8000/get_world_size/
curl -L -s -o /dev/null -w "Init Comm: %{http_code}\n" http://localhost:8000/init_communicator/

# Run GRPO test
CUDA_VISIBLE_DEVICES=1 python scripts/rl_train_grpo.py \
  --model Qwen/Qwen2.5-0.5B-Instruct \
  --dataset data/test.jsonl \
  --output_dir checkpoints/test_rl \
  --max_steps 2 \
  --save_steps 10 \
  --eval_steps 10 \
  --per_device_batch_size 1 \
  --num_generations 1 \
  --grad_accum 1 \
  --max_seq_len 1024 \
  --learning_rate 1e-7

# Kill the vLLM server
tmux kill-session -t vllm

# ============================================
# PART 6: Test Evaluation
# ============================================

# Start evaluation server
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-0.5B-Instruct \
  --host 0.0.0.0 --port 8001 \
  --dtype auto \
  --max-model-len 2048 \
  --gpu-memory-utilization 0.90 &

sleep 15

# Run evaluation
export OPENAI_API_KEY=EMPTY
export OPENAI_BASE_URL=http://localhost:8001/v1
vf-eval shop-r1 -m Qwen/Qwen2.5-0.5B-Instruct -s -n 5

# Kill server
pkill -f "vllm.entrypoints"

# ============================================
# PART 7: Debug Any Errors
# ============================================

# If you get errors, check:

# 1. Python version
python --version  # Should be 3.9+

# 2. Verifiers version
python -c "import verifiers; print(verifiers.__version__)"

# 3. GPU memory
nvidia-smi

# 4. Test reward computation manually
python -c "
import json
import verifiers as vf

env = vf.load_environment('shop-r1', debug_rewards=True)
completion = '{\"rationale\": \"test\", \"action\": {\"type\": \"click\", \"name\": \"button\"}}'
answer = {'type': 'click', 'name': 'button'}
prompt = [{'role': 'user', 'content': 'test'}]

for i, (func, weight) in enumerate(zip(env.rubric.funcs, env.rubric.weights)):
    try:
        r = func(completion, answer, prompt=prompt, info=answer)
        print(f'Reward {i}: {r:.3f} (weight={weight})')
    except Exception as e:
        print(f'Error in reward {i}: {e}')
"