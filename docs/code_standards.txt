# prime-environments PR guidelines

thanks for your work on the PR thus far! some notes below on code quality + what we’ll be looking for before merging, extending what we shared in the PI discord earlier (see #environment-hub-beta pinned channel). 

TLDR: 

- code needs to be high-quality + functional + up-to-date with verifiers>=0.1.3 + documented
    - run `ruff check --fix .` on your code
- small-scale evals need to be included + pass the smell test
- let us know when you’ve read the above + are ready for review (if you haven’t already) by tagging Will or Johannes in the PR and setting the PR to Open
    - if your PR was previously Open and is now set to Draft, this is us flagging that you have some TODOs remaining here

## code quality guidelines

we shouldn’t have to tell you this, but PRs cannot not be “vibe-coded slop”. by all means, use AI tooling to speed up implementation and debug, but do not rely on it for making consequential design choices without checking them. models are prone to silliness for these kinds of tasks, and this can ruin the integrity of an eval. we ask that you take responsibility for understanding the original source in depth, and are prepared to answer questions about anything that looks funny in your code (for which “idk Claude did it” is not an acceptable answer).

- if we judge your submission to be clearly a bad-faith attempt at submitting a correct + high-quality implementation, we reserve the right to close your PR and release the lock on your task.
- if you have multiple bounties in progress, this can extend to your others as well.

especially for prominent evals (e.g. ones reported in flagship model releases), the goal here is a **fully correct implementation** which is faithful to the original source.  

the main exceptions to this rule are for versions of evals which allow custom scaffolds, where you can take some reasonable creative liberties if there isn’t an obvious default (e.g. choice of web search tools). the reward components still need to match the original graders.

for train-only or more open-ended tasks, we can be more lenient on straying from the original source implementation, but we’ll also want you to share more context about the choices you made in your README/PR message.

as a general rule, whenever it is possible to do so, you should install original source libraries as dependencies (pinned commits/forks are fine if necessary) and import functionality from them. reimplementing major components from scratch is a recipe for brittleness, and introduces many more opportunities for subtle failures of correctness. 

## review process: TODOs

**to help expedite your review:**

- write a clear PR note that explains your implementation, and any key design choices
- update your environment to use **verifiers>=0.1.3** and fix any breaks
- update your README to remove the reports sections
- include proper tags, description, and dependencies in your pyproject.toml
- **include a link to your source implementation** (e.g. your prime-environments fork) at the top of the readme, along with your socials if you choose -- we'll be maintaining many of the environments in prime-environments going forward, and want to ensure you/us can periodically sync if desired. you're also encouraged to include your GH/socials for credit in the readme as well.
- choose a model -- either API or self-hosted vLLM -- and ensure that reward scores pass the smell test, particularly for any evals with publicly reported benchmarks. **run with vf-eval -s** and include the outputs in the PR. doesn't have to be 100% exact match, but close enough; we'll run more evals ourselves after PRs are merged to catch minor discrepancies.
- Qwen3 30B-A3B instruct or thinking is your best choice for self-hosting; it's really smart, really fast, and can fit on <$1/hr setups in FP8 (rtx 6000 ada, 2x 4090). testing with RL training is not needed for the vast majority of envs. let us know if you need more compute for testing, happy to help.

we'll be prioritizing reviews in order of those which **follow the above guidelines**

- if you want it to be merged fast + get your bounty, this will help a lot.

**when your PR is merged**

- you can either accept the cash bounty by filling out this form: https://forms.gle/LDjco1S8ZL8AJiKk9
- or the higher compute bounties by letting us know directly.

## testing + evals

TLDR: 

- we want to ensure everyone can appropriately test without paying out of pocket, and have given you all some GPU credits for testing (let us know if this is not the case, or if you’re running out and need more)
- you don’t need to test with huge models or at large volumes
- total costs for testing at small scales should not be terribly high in most cases
- best options for free-tier APIs:
    - OpenRouter
    - Gemini 2.5 Flash
    - Cloudflare Workers API ([gist](https://gist.github.com/mchenco/4b288e7123e235407dfc9fc6a3832c01))
- non-free:
    - deepseek-chat / deepseek-reasoner (V3.1)
    - gpt-4.1 series
    - claude 3.5 haiku, gemini 2.5 flash / flash-lite
- best models to self-host for testing:
    - Qwen3-30B-A3B (Instruct, Coder, Thinking) 2507
    - Qwen3-4B (Instruct, Thinking) 2507

The main criteria for deciding models to test in your environment are:

- Whether you want to self-host vs. use an API
- How hard your task is, and whether reasoning is required/helpful
- How high-throughput you need for inference

### Self-hosting with vLLM

Self-hosting makes most sense when you can host across one or more GPUs (depending on model size and want high total throughput in terms of parallel requests but are less sensitive to tok/s on a per-request basis, or are testing with a small model which isn’t easily available via API a high-throughput API — perfect for large evals on small models.

For evals, you’ll want much more headroom than the bare minimum. For example, you *could* self-host a 32B dense model on a 48GB GPU in FP8, but will likely find that this is pretty slow in terms of total throughput for many parallel requests.

We recommend using the [Qwen3](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f) series via vLLM for most self-hosted testing. Qwen has some excellent docs for getting started with self-hosting [here](https://qwen.readthedocs.io/en/latest/deployment/vllm.html).

The 30B-A3B series of models also offers a great sweet spot for powerful models with small footprints and very fast inference (due to having only 3B active parameters).

Models to consider

- non-thinking:
    - Qwen3-30B-A3B-Instruct-2507
    - Qwen3-Coder-30B-A3B-Instruct
    - Qwen3-4B-Instruct-2507
- thinking:
    - Qwen3-30B-A3B-Thinking-2507
    - Qwen3-4B-Thinking-2507
- earlier series (mostly useful for smallest models + RL training experiments):
    - Qwen3-8B (thinking)
    - Qwen3-1.7B (thinking)
    - Qwen3-0.6B (thinking)

notes on Qwen3 series:

- all Qwen3 models offer FP8 versions. if you’re using an Ada/Hopper/Blackwell GPU this is likely what you’ll want to use for improved speed (and negligible performance differences).
- the 235B model series is quite strong, but is also widely available from [API providers](https://openrouter.ai/qwen/qwen3-235b-a22b-2507) at low costs ($0.13/$0.6 per M toks in/out), and is generally not economical for self-hosting unless you’re doing high-throughput synthetic data generation with a highly optimized hosting configuration.
- for RL training, you’ll want to use a modified chat template which avoids stripping <think> sections when applied to messages; a collection with pre-modified templates is available [here](https://huggingface.co/collections/willcb/qwen3-68434f4883925bfdb4570ee5).

## OpenAI-compatible API models

DeepSeek API

- non-thinking: deepseek-chat (V3.1 non-thinking)
- thinking: deepseek-reasoner (V3.1 thinking)

OpenAI API

- non-thinking: gpt-4.1 (-mini, -nano)
- thinking: o4-mini, gpt-5 (-mini, -nano)
    - note: thinking summary not exposed for chatcompletions

Anthropic API

- non-thinking: Haiku 3.5, Sonnet 4
- thinking: Sonnet 4 (with `extra_body={"thinking": { "type": "enabled", "budget_tokens": 2000 }}`  in `sampling_args`)

Gemini API

- thinking: gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite
    - with `reasoning_effort = "low" / "medium" / "high`" in `sampling_args`
- non-thinking: gemini-2.5-flash, gemini-2.5-flash-lite
    - with `reasoning_effort = "none"` in `sampling_args`

OpenRouter API:

- has most popular open-weights models at low prices from many providers (along with entrypoints for all major proprietary models), pick your favorites